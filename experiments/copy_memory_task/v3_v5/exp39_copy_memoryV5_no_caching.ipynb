{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from snn_delays.snn_refactored import SNN\n",
    "from snn_delays.utils.dataset_loader import DatasetLoader\n",
    "from snn_delays.utils.train_utils_refact_minimal import train, get_device, propagate_batch_simple, to_plot\n",
    "from snn_delays.utils.test_behavior import tb_addtask_refact\n",
    "# from snn_delays.utils.visualization_utils import plot_taus\n",
    "import numpy as np\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V5: noisier and more realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\datasets\\sequential_datasets.py:279: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3641.)\n",
      "  label[:,0] = seq[start_time:start_time + mem_length, 0].T.clone().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50, 3])\n",
      "torch.Size([128, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "time_window = 50\n",
    "batch_size = 128 # 128: anil kag\n",
    "\n",
    "ckpt_dir = 'copymemory_04_2025_tests'\n",
    "\n",
    "dataset = 'copymemory_episodic'\n",
    "#dataset = 'addtask' \n",
    "\n",
    "num_epochs = 3000 # important: epochs here refer to iteration steps\n",
    "\n",
    "DL = DatasetLoader(dataset=dataset, caching='', \n",
    "                   dataset_size = batch_size,\n",
    "                   num_workers=0, batch_size=batch_size, \n",
    "                   total_time=time_window)\n",
    "\n",
    "train_loader, test_loader, dataset_dict = DL.get_dataloaders()\n",
    "dataset_dict[\"time_ms\"] = 2e3\n",
    "\n",
    "for img, lbl in train_loader:\n",
    "    print(img.shape)\n",
    "    print(lbl.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1ac59ff4bc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8000, 0.9000, 0.6000, 0.7000, 0.5000],\n",
      "        [0.8000, 0.9000, 0.6000, 0.7000, 0.5000],\n",
      "        [0.8000, 0.9000, 0.6000, 0.7000, 0.5000],\n",
      "        [0.8000, 0.9000, 0.6000, 0.7000, 0.5000],\n",
      "        [0.8000, 0.9000, 0.6000, 0.7000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(lbl[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training copy memory task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n",
      "Delta t: 40.0 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (criterion): MSELoss()\n",
       "  (layers): ModuleList(\n",
       "    (0): FeedforwardSNNLayer(\n",
       "      (linear): Linear(in_features=3, out_features=128, bias=False)\n",
       "    )\n",
       "    (1): FeedforwardSNNLayer(\n",
       "      (linear): Linear(in_features=5120, out_features=128, bias=False)\n",
       "    )\n",
       "    (2): FeedforwardSNNLayer(\n",
       "      (linear): Linear(in_features=128, out_features=5, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_h = 128\n",
    "\n",
    "extra_kwargs = {'multifeedforward':3}\n",
    "snn_f = SNN(dataset_dict, structure=(num_h, 2, 'mf'), win=time_window,\n",
    "             loss_fn='mem_prediction', batch_size=batch_size, device=device, debug=False, **extra_kwargs)\n",
    "\n",
    "snn_f.set_layers()\n",
    "#snn_f.num_train_samples = batch_size\n",
    "snn_f.to(device)\n",
    "\n",
    "snn_rnn = SNN(dataset_dict, structure=(num_h, 2, 'r'), win=time_window,\n",
    "               loss_fn='mem_prediction', batch_size=batch_size, device=device, debug=False)\n",
    "\n",
    "snn_rnn.set_layers()\n",
    "#snn_rnn.num_train_samples = batch_size\n",
    "snn_rnn.to(device)\n",
    "\n",
    "extra_kwargs = {'delay_range':(40, 1),\n",
    "                'pruned_delays': 3}\n",
    "\n",
    "snn_rd = SNN(dataset_dict, structure=(num_h, 2, 'd'), win=time_window,\n",
    "               loss_fn='mem_prediction', batch_size=batch_size, device=device, debug=False, **extra_kwargs)\n",
    "\n",
    "snn_rd.set_layers()\n",
    "#snn_rd.num_train_samples = batch_size\n",
    "snn_rd.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error: 0.0004953516181558371% \n",
      "--------------------------\n",
      "Epoch [1/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.33603\n",
      "128\n",
      "Time elasped: 0.850513219833374\n",
      "Epoch [2/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 10.35419\n",
      "128\n",
      "Time elasped: 0.24068045616149902\n",
      "Epoch [3/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.69544\n",
      "128\n",
      "Time elasped: 0.21925997734069824\n",
      "Epoch [4/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.33632\n",
      "128\n",
      "Time elasped: 0.18001770973205566\n",
      "Epoch [5/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.17958\n",
      "128\n",
      "Time elasped: 0.21988201141357422\n",
      "Mean Error: 0.0010273103835061193% \n",
      "--------------------------\n",
      "Epoch [6/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.13241\n",
      "128\n",
      "Time elasped: 0.18160367012023926\n",
      "Epoch [7/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10896\n",
      "128\n",
      "Time elasped: 0.22149014472961426\n",
      "Epoch [8/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09990\n",
      "128\n",
      "Time elasped: 0.1833817958831787\n",
      "Epoch [9/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09497\n",
      "128\n",
      "Time elasped: 0.1822795867919922\n",
      "Epoch [10/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10327\n",
      "128\n",
      "Time elasped: 0.21758008003234863\n",
      "Mean Error: 0.0008031204342842102% \n",
      "--------------------------\n",
      "Epoch [11/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10268\n",
      "128\n",
      "Time elasped: 0.23773550987243652\n",
      "Epoch [12/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09874\n",
      "128\n",
      "Time elasped: 0.19277715682983398\n",
      "Epoch [13/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09931\n",
      "128\n",
      "Time elasped: 0.21498346328735352\n",
      "Epoch [14/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10525\n",
      "128\n",
      "Time elasped: 0.20529532432556152\n",
      "Epoch [15/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.11035\n",
      "128\n",
      "Time elasped: 0.20749902725219727\n",
      "Mean Error: 0.0008369736024178565% \n",
      "--------------------------\n",
      "Epoch [16/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.11073\n",
      "128\n",
      "Time elasped: 0.21014809608459473\n",
      "Epoch [17/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10485\n",
      "128\n",
      "Time elasped: 0.20010137557983398\n",
      "Epoch [18/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10636\n",
      "128\n",
      "Time elasped: 0.1915132999420166\n",
      "Epoch [19/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09979\n",
      "128\n",
      "Time elasped: 0.18735527992248535\n",
      "Epoch [20/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09542\n",
      "128\n",
      "Time elasped: 0.2011268138885498\n",
      "Mean Error: 0.0007184536661952734% \n",
      "--------------------------\n",
      "Epoch [21/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.10252\n",
      "128\n",
      "Time elasped: 0.1953737735748291\n",
      "Epoch [22/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09437\n",
      "128\n",
      "Time elasped: 0.212296724319458\n",
      "Epoch [23/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09578\n",
      "128\n",
      "Time elasped: 0.18776273727416992\n",
      "Epoch [24/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09138\n",
      "128\n",
      "Time elasped: 0.18816876411437988\n",
      "Epoch [25/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09520\n",
      "128\n",
      "Time elasped: 0.21790051460266113\n",
      "Mean Error: 0.0007004451472312212% \n",
      "--------------------------\n",
      "Epoch [26/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09207\n",
      "128\n",
      "Time elasped: 0.1957688331604004\n",
      "Epoch [27/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09469\n",
      "128\n",
      "Time elasped: 0.18622827529907227\n",
      "Epoch [28/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08862\n",
      "128\n",
      "Time elasped: 0.21737384796142578\n",
      "Epoch [29/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09789\n",
      "128\n",
      "Time elasped: 0.18932771682739258\n",
      "Epoch [30/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09480\n",
      "128\n",
      "Time elasped: 0.1934034824371338\n",
      "Mean Error: 0.0007007124368101358% \n",
      "--------------------------\n",
      "Epoch [31/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09474\n",
      "128\n",
      "Time elasped: 0.23273277282714844\n",
      "Epoch [32/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08444\n",
      "128\n",
      "Time elasped: 0.19035100936889648\n",
      "Epoch [33/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09186\n",
      "128\n",
      "Time elasped: 0.2261490821838379\n",
      "Epoch [34/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.09049\n",
      "128\n",
      "Time elasped: 0.20804476737976074\n",
      "Epoch [35/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08518\n",
      "128\n",
      "Time elasped: 0.26189208030700684\n",
      "Mean Error: 0.0006362023996189237% \n",
      "--------------------------\n",
      "Epoch [36/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08167\n",
      "128\n",
      "Time elasped: 0.21212363243103027\n",
      "Epoch [37/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08415\n",
      "128\n",
      "Time elasped: 0.1992795467376709\n",
      "Epoch [38/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08366\n",
      "128\n",
      "Time elasped: 0.21990227699279785\n",
      "Epoch [39/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08330\n",
      "128\n",
      "Time elasped: 0.1947193145751953\n",
      "Epoch [40/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08024\n",
      "128\n",
      "Time elasped: 0.21762919425964355\n",
      "Mean Error: 0.0005716668674722314% \n",
      "--------------------------\n",
      "Epoch [41/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08677\n",
      "128\n",
      "Time elasped: 0.18471837043762207\n",
      "Epoch [42/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07348\n",
      "128\n",
      "Time elasped: 0.20508193969726562\n",
      "Epoch [43/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07991\n",
      "128\n",
      "Time elasped: 0.21410155296325684\n",
      "Epoch [44/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07886\n",
      "128\n",
      "Time elasped: 0.2311418056488037\n",
      "Epoch [45/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.08126\n",
      "128\n",
      "Time elasped: 0.18757843971252441\n",
      "Mean Error: 0.0005859983502887189% \n",
      "--------------------------\n",
      "Epoch [46/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07385\n",
      "128\n",
      "Time elasped: 0.26001977920532227\n",
      "Epoch [47/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07957\n",
      "128\n",
      "Time elasped: 0.22856998443603516\n",
      "Epoch [48/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07282\n",
      "128\n",
      "Time elasped: 0.200408935546875\n",
      "Epoch [49/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07855\n",
      "128\n",
      "Time elasped: 0.24949431419372559\n",
      "Epoch [50/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07121\n",
      "128\n",
      "Time elasped: 0.21455168724060059\n",
      "Mean Error: 0.0006027144845575094% \n",
      "--------------------------\n",
      "Epoch [51/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07252\n",
      "128\n",
      "Time elasped: 0.1899113655090332\n",
      "Epoch [52/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07507\n",
      "128\n",
      "Time elasped: 0.19881629943847656\n",
      "Epoch [53/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07549\n",
      "128\n",
      "Time elasped: 0.23511719703674316\n",
      "Epoch [54/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07465\n",
      "128\n",
      "Time elasped: 0.21356654167175293\n",
      "Epoch [55/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07245\n",
      "128\n",
      "Time elasped: 0.20264554023742676\n",
      "Mean Error: 0.000565873458981514% \n",
      "--------------------------\n",
      "Epoch [56/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07434\n",
      "128\n",
      "Time elasped: 0.21986627578735352\n",
      "Epoch [57/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07379\n",
      "128\n",
      "Time elasped: 0.19989705085754395\n",
      "Epoch [58/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07178\n",
      "128\n",
      "Time elasped: 0.20644235610961914\n",
      "Epoch [59/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07386\n",
      "128\n",
      "Time elasped: 0.2192840576171875\n",
      "Epoch [60/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07040\n",
      "128\n",
      "Time elasped: 0.2341909408569336\n",
      "Mean Error: 0.0005238935118541121% \n",
      "--------------------------\n",
      "Epoch [61/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07471\n",
      "128\n",
      "Time elasped: 0.2201077938079834\n",
      "Epoch [62/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06935\n",
      "128\n",
      "Time elasped: 0.19986200332641602\n",
      "Epoch [63/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06941\n",
      "128\n",
      "Time elasped: 0.21610355377197266\n",
      "Epoch [64/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07546\n",
      "128\n",
      "Time elasped: 0.2138984203338623\n",
      "Epoch [65/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07166\n",
      "128\n",
      "Time elasped: 0.20525217056274414\n",
      "Mean Error: 0.0005344440578483045% \n",
      "--------------------------\n",
      "Epoch [66/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07047\n",
      "128\n",
      "Time elasped: 0.2578623294830322\n",
      "Epoch [67/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07350\n",
      "128\n",
      "Time elasped: 0.2181377410888672\n",
      "Epoch [68/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06791\n",
      "128\n",
      "Time elasped: 0.22873568534851074\n",
      "Epoch [69/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07048\n",
      "128\n",
      "Time elasped: 0.2115492820739746\n",
      "Epoch [70/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07049\n",
      "128\n",
      "Time elasped: 0.20523548126220703\n",
      "Mean Error: 0.0005411844467744231% \n",
      "--------------------------\n",
      "Epoch [71/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06466\n",
      "128\n",
      "Time elasped: 0.22968840599060059\n",
      "Epoch [72/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06902\n",
      "128\n",
      "Time elasped: 0.21166253089904785\n",
      "Epoch [73/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07018\n",
      "128\n",
      "Time elasped: 0.254408597946167\n",
      "Epoch [74/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06895\n",
      "128\n",
      "Time elasped: 0.1870419979095459\n",
      "Epoch [75/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06519\n",
      "128\n",
      "Time elasped: 0.20499467849731445\n",
      "Mean Error: 0.0005307022947818041% \n",
      "--------------------------\n",
      "Epoch [76/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06856\n",
      "128\n",
      "Time elasped: 0.24999022483825684\n",
      "Epoch [77/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06737\n",
      "128\n",
      "Time elasped: 0.21134662628173828\n",
      "Epoch [78/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06704\n",
      "128\n",
      "Time elasped: 0.23845505714416504\n",
      "Epoch [79/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06469\n",
      "128\n",
      "Time elasped: 0.21146750450134277\n",
      "Epoch [80/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06745\n",
      "128\n",
      "Time elasped: 0.22377443313598633\n",
      "Mean Error: 0.0005102174472995102% \n",
      "--------------------------\n",
      "Epoch [81/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06666\n",
      "128\n",
      "Time elasped: 0.21208763122558594\n",
      "Epoch [82/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06225\n",
      "128\n",
      "Time elasped: 0.2521834373474121\n",
      "Epoch [83/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06317\n",
      "128\n",
      "Time elasped: 0.23372101783752441\n",
      "Epoch [84/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06823\n",
      "128\n",
      "Time elasped: 0.1921396255493164\n",
      "Epoch [85/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06324\n",
      "128\n",
      "Time elasped: 0.20760059356689453\n",
      "Mean Error: 0.0005057948874309659% \n",
      "--------------------------\n",
      "Epoch [86/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.05989\n",
      "128\n",
      "Time elasped: 0.23978447914123535\n",
      "Epoch [87/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06777\n",
      "128\n",
      "Time elasped: 0.23034214973449707\n",
      "Epoch [88/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06862\n",
      "128\n",
      "Time elasped: 0.21979141235351562\n",
      "Epoch [89/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06534\n",
      "128\n",
      "Time elasped: 0.24062204360961914\n",
      "Epoch [90/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06571\n",
      "128\n",
      "Time elasped: 0.20961499214172363\n",
      "Mean Error: 0.0005450620083138347% \n",
      "--------------------------\n",
      "Epoch [91/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06649\n",
      "128\n",
      "Time elasped: 0.22431230545043945\n",
      "Epoch [92/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06497\n",
      "128\n",
      "Time elasped: 0.20117473602294922\n",
      "Epoch [93/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07135\n",
      "128\n",
      "Time elasped: 0.22880315780639648\n",
      "Epoch [94/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06629\n",
      "128\n",
      "Time elasped: 0.2149369716644287\n",
      "Epoch [95/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07044\n",
      "128\n",
      "Time elasped: 0.21826386451721191\n",
      "Mean Error: 0.0005374239408411086% \n",
      "--------------------------\n",
      "Epoch [96/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06679\n",
      "128\n",
      "Time elasped: 0.2566530704498291\n",
      "Epoch [97/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06680\n",
      "128\n",
      "Time elasped: 0.22608065605163574\n",
      "Epoch [98/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06887\n",
      "128\n",
      "Time elasped: 0.21397900581359863\n",
      "Epoch [99/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.07181\n",
      "128\n",
      "Time elasped: 0.2821640968322754\n",
      "Epoch [100/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06904\n",
      "128\n",
      "Time elasped: 0.19129252433776855\n",
      "Mean Error: 0.000514639017637819% \n",
      "--------------------------\n",
      "Epoch [101/3000], learning_rates 0.001000, 1.000000\n",
      "Step [1/1], Loss: 0.06911\n",
      "128\n",
      "Time elasped: 0.25663113594055176\n",
      "Epoch [102/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06460\n",
      "128\n",
      "Time elasped: 0.24466729164123535\n",
      "Epoch [103/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06203\n",
      "128\n",
      "Time elasped: 0.24991774559020996\n",
      "Epoch [104/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06458\n",
      "128\n",
      "Time elasped: 0.26947999000549316\n",
      "Epoch [105/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06422\n",
      "128\n",
      "Time elasped: 0.2424910068511963\n",
      "Mean Error: 0.0005430445889942348% \n",
      "--------------------------\n",
      "Epoch [106/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06900\n",
      "128\n",
      "Time elasped: 0.25160980224609375\n",
      "Epoch [107/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06442\n",
      "128\n",
      "Time elasped: 0.28871917724609375\n",
      "Epoch [108/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06426\n",
      "128\n",
      "Time elasped: 0.2079610824584961\n",
      "Epoch [109/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06589\n",
      "128\n",
      "Time elasped: 0.24231481552124023\n",
      "Epoch [110/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06852\n",
      "128\n",
      "Time elasped: 0.20423054695129395\n",
      "Mean Error: 0.0005435183411464095% \n",
      "--------------------------\n",
      "Epoch [111/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06834\n",
      "128\n",
      "Time elasped: 0.22126221656799316\n",
      "Epoch [112/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06814\n",
      "128\n",
      "Time elasped: 0.2464156150817871\n",
      "Epoch [113/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06789\n",
      "128\n",
      "Time elasped: 0.27277708053588867\n",
      "Epoch [114/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06553\n",
      "128\n",
      "Time elasped: 0.3212313652038574\n",
      "Epoch [115/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06676\n",
      "128\n",
      "Time elasped: 0.2148151397705078\n",
      "Mean Error: 0.0005039215902797878% \n",
      "--------------------------\n",
      "Epoch [116/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06436\n",
      "128\n",
      "Time elasped: 0.218339204788208\n",
      "Epoch [117/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06855\n",
      "128\n",
      "Time elasped: 0.24817919731140137\n",
      "Epoch [118/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06331\n",
      "128\n",
      "Time elasped: 0.22689080238342285\n",
      "Epoch [119/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06658\n",
      "128\n",
      "Time elasped: 0.25449609756469727\n",
      "Epoch [120/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06572\n",
      "128\n",
      "Time elasped: 0.25559020042419434\n",
      "Mean Error: 0.0005495372461155057% \n",
      "--------------------------\n",
      "Epoch [121/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06799\n",
      "128\n",
      "Time elasped: 0.22059273719787598\n",
      "Epoch [122/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06739\n",
      "128\n",
      "Time elasped: 0.1958324909210205\n",
      "Epoch [123/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06699\n",
      "128\n",
      "Time elasped: 0.23365283012390137\n",
      "Epoch [124/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06667\n",
      "128\n",
      "Time elasped: 0.23351144790649414\n",
      "Epoch [125/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06689\n",
      "128\n",
      "Time elasped: 0.2164323329925537\n",
      "Mean Error: 0.0005223678308539093% \n",
      "--------------------------\n",
      "Epoch [126/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06619\n",
      "128\n",
      "Time elasped: 0.21037507057189941\n",
      "Epoch [127/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06474\n",
      "128\n",
      "Time elasped: 0.2156364917755127\n",
      "Epoch [128/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06353\n",
      "128\n",
      "Time elasped: 0.22552967071533203\n",
      "Epoch [129/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06817\n",
      "128\n",
      "Time elasped: 0.2200617790222168\n",
      "Epoch [130/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06703\n",
      "128\n",
      "Time elasped: 0.2659761905670166\n",
      "Mean Error: 0.0005224518827162683% \n",
      "--------------------------\n",
      "Epoch [131/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06522\n",
      "128\n",
      "Time elasped: 0.21661663055419922\n",
      "Epoch [132/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06620\n",
      "128\n",
      "Time elasped: 0.23627018928527832\n",
      "Epoch [133/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06536\n",
      "128\n",
      "Time elasped: 0.21109533309936523\n",
      "Epoch [134/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.07051\n",
      "128\n",
      "Time elasped: 0.22341251373291016\n",
      "Epoch [135/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06694\n",
      "128\n",
      "Time elasped: 0.21593642234802246\n",
      "Mean Error: 0.0005195258418098092% \n",
      "--------------------------\n",
      "Epoch [136/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06543\n",
      "128\n",
      "Time elasped: 0.25679755210876465\n",
      "Epoch [137/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06488\n",
      "128\n",
      "Time elasped: 0.22489619255065918\n",
      "Epoch [138/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06547\n",
      "128\n",
      "Time elasped: 0.27501440048217773\n",
      "Epoch [139/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06755\n",
      "128\n",
      "Time elasped: 0.20492196083068848\n",
      "Epoch [140/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06599\n",
      "128\n",
      "Time elasped: 0.21329712867736816\n",
      "Mean Error: 0.0004722564190160483% \n",
      "--------------------------\n",
      "Epoch [141/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06194\n",
      "128\n",
      "Time elasped: 0.21951031684875488\n",
      "Epoch [142/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06320\n",
      "128\n",
      "Time elasped: 0.2362051010131836\n",
      "Epoch [143/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06194\n",
      "128\n",
      "Time elasped: 0.20217013359069824\n",
      "Epoch [144/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06691\n",
      "128\n",
      "Time elasped: 0.1878201961517334\n",
      "Epoch [145/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06617\n",
      "128\n",
      "Time elasped: 0.23982000350952148\n",
      "Mean Error: 0.0005000632954761386% \n",
      "--------------------------\n",
      "Epoch [146/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06770\n",
      "128\n",
      "Time elasped: 0.25774192810058594\n",
      "Epoch [147/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06668\n",
      "128\n",
      "Time elasped: 0.23985075950622559\n",
      "Epoch [148/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06180\n",
      "128\n",
      "Time elasped: 0.28264474868774414\n",
      "Epoch [149/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06586\n",
      "128\n",
      "Time elasped: 0.22574853897094727\n",
      "Epoch [150/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06310\n",
      "128\n",
      "Time elasped: 0.23792004585266113\n",
      "Mean Error: 0.0005049154860898852% \n",
      "--------------------------\n",
      "Epoch [151/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06906\n",
      "128\n",
      "Time elasped: 0.21109652519226074\n",
      "Epoch [152/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.07007\n",
      "128\n",
      "Time elasped: 0.21141338348388672\n",
      "Epoch [153/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06984\n",
      "128\n",
      "Time elasped: 0.2142937183380127\n",
      "Epoch [154/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06370\n",
      "128\n",
      "Time elasped: 0.26991963386535645\n",
      "Epoch [155/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06315\n",
      "128\n",
      "Time elasped: 0.2510955333709717\n",
      "Mean Error: 0.0005119710112921894% \n",
      "--------------------------\n",
      "Epoch [156/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06690\n",
      "128\n",
      "Time elasped: 0.28020668029785156\n",
      "Epoch [157/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06351\n",
      "128\n",
      "Time elasped: 0.2921478748321533\n",
      "Epoch [158/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06840\n",
      "128\n",
      "Time elasped: 0.22956466674804688\n",
      "Epoch [159/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.05768\n",
      "128\n",
      "Time elasped: 0.22430133819580078\n",
      "Epoch [160/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06861\n",
      "128\n",
      "Time elasped: 0.25786900520324707\n",
      "Mean Error: 0.0005041734548285604% \n",
      "--------------------------\n",
      "Epoch [161/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06361\n",
      "128\n",
      "Time elasped: 0.22010350227355957\n",
      "Epoch [162/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06466\n",
      "128\n",
      "Time elasped: 0.25580430030822754\n",
      "Epoch [163/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06422\n",
      "128\n",
      "Time elasped: 0.2199873924255371\n",
      "Epoch [164/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06358\n",
      "128\n",
      "Time elasped: 0.2829296588897705\n",
      "Epoch [165/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06438\n",
      "128\n",
      "Time elasped: 0.20616745948791504\n",
      "Mean Error: 0.0004971636808477342% \n",
      "--------------------------\n",
      "Epoch [166/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06288\n",
      "128\n",
      "Time elasped: 0.2088150978088379\n",
      "Epoch [167/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06339\n",
      "128\n",
      "Time elasped: 0.22719097137451172\n",
      "Epoch [168/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06416\n",
      "128\n",
      "Time elasped: 0.2264392375946045\n",
      "Epoch [169/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06593\n",
      "128\n",
      "Time elasped: 0.23711872100830078\n",
      "Epoch [170/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06266\n",
      "128\n",
      "Time elasped: 0.25220513343811035\n",
      "Mean Error: 0.00050339539302513% \n",
      "--------------------------\n",
      "Epoch [171/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06397\n",
      "128\n",
      "Time elasped: 0.23276901245117188\n",
      "Epoch [172/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06480\n",
      "128\n",
      "Time elasped: 0.22586417198181152\n",
      "Epoch [173/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06163\n",
      "128\n",
      "Time elasped: 0.22314906120300293\n",
      "Epoch [174/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06342\n",
      "128\n",
      "Time elasped: 0.22888541221618652\n",
      "Epoch [175/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06317\n",
      "128\n",
      "Time elasped: 0.2986276149749756\n",
      "Mean Error: 0.00048542788135819137% \n",
      "--------------------------\n",
      "Epoch [176/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06527\n",
      "128\n",
      "Time elasped: 0.2132282257080078\n",
      "Epoch [177/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06893\n",
      "128\n",
      "Time elasped: 0.23156309127807617\n",
      "Epoch [178/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06545\n",
      "128\n",
      "Time elasped: 0.21367359161376953\n",
      "Epoch [179/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06499\n",
      "128\n",
      "Time elasped: 0.20355653762817383\n",
      "Epoch [180/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.05909\n",
      "128\n",
      "Time elasped: 0.2240152359008789\n",
      "Mean Error: 0.000507707183714956% \n",
      "--------------------------\n",
      "Epoch [181/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06313\n",
      "128\n",
      "Time elasped: 0.24454116821289062\n",
      "Epoch [182/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06566\n",
      "128\n",
      "Time elasped: 0.19988441467285156\n",
      "Epoch [183/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06462\n",
      "128\n",
      "Time elasped: 0.2228684425354004\n",
      "Epoch [184/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06543\n",
      "128\n",
      "Time elasped: 0.21983075141906738\n",
      "Epoch [185/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06544\n",
      "128\n",
      "Time elasped: 0.24927854537963867\n",
      "Mean Error: 0.0005071365158073604% \n",
      "--------------------------\n",
      "Epoch [186/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06602\n",
      "128\n",
      "Time elasped: 0.21803045272827148\n",
      "Epoch [187/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06825\n",
      "128\n",
      "Time elasped: 0.22494053840637207\n",
      "Epoch [188/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.05926\n",
      "128\n",
      "Time elasped: 0.21586346626281738\n",
      "Epoch [189/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06157\n",
      "128\n",
      "Time elasped: 0.2220454216003418\n",
      "Epoch [190/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06884\n",
      "128\n",
      "Time elasped: 0.22736334800720215\n",
      "Mean Error: 0.0005021516117267311% \n",
      "--------------------------\n",
      "Epoch [191/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06531\n",
      "128\n",
      "Time elasped: 0.21104788780212402\n",
      "Epoch [192/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06167\n",
      "128\n",
      "Time elasped: 0.2612001895904541\n",
      "Epoch [193/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06053\n",
      "128\n",
      "Time elasped: 0.2529134750366211\n",
      "Epoch [194/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06104\n",
      "128\n",
      "Time elasped: 0.1968858242034912\n",
      "Epoch [195/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06323\n",
      "128\n",
      "Time elasped: 0.24026823043823242\n",
      "Mean Error: 0.00047664603334851563% \n",
      "--------------------------\n",
      "Epoch [196/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06343\n",
      "128\n",
      "Time elasped: 0.1919538974761963\n",
      "Epoch [197/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.05888\n",
      "128\n",
      "Time elasped: 0.2642703056335449\n",
      "Epoch [198/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.05737\n",
      "128\n",
      "Time elasped: 0.25418710708618164\n",
      "Epoch [199/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06678\n",
      "128\n",
      "Time elasped: 0.2183065414428711\n",
      "Epoch [200/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06621\n",
      "128\n",
      "Time elasped: 0.2225642204284668\n",
      "Mean Error: 0.0005197192076593637% \n",
      "--------------------------\n",
      "Epoch [201/3000], learning_rates 0.000950, 0.950000\n",
      "Step [1/1], Loss: 0.06546\n",
      "128\n",
      "Time elasped: 0.25867700576782227\n",
      "Epoch [202/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06507\n",
      "128\n",
      "Time elasped: 0.19641566276550293\n",
      "Epoch [203/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06318\n",
      "128\n",
      "Time elasped: 0.1970539093017578\n",
      "Epoch [204/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06869\n",
      "128\n",
      "Time elasped: 0.19762253761291504\n",
      "Epoch [205/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06291\n",
      "128\n",
      "Time elasped: 0.22289466857910156\n",
      "Mean Error: 0.0005176453851163387% \n",
      "--------------------------\n",
      "Epoch [206/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05905\n",
      "128\n",
      "Time elasped: 0.3113436698913574\n",
      "Epoch [207/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06336\n",
      "128\n",
      "Time elasped: 0.24393510818481445\n",
      "Epoch [208/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06680\n",
      "128\n",
      "Time elasped: 0.2061748504638672\n",
      "Epoch [209/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05867\n",
      "128\n",
      "Time elasped: 0.24463963508605957\n",
      "Epoch [210/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06346\n",
      "128\n",
      "Time elasped: 0.2153770923614502\n",
      "Mean Error: 0.0005038275849074125% \n",
      "--------------------------\n",
      "Epoch [211/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05994\n",
      "128\n",
      "Time elasped: 0.22527289390563965\n",
      "Epoch [212/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06348\n",
      "128\n",
      "Time elasped: 0.20308494567871094\n",
      "Epoch [213/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06697\n",
      "128\n",
      "Time elasped: 0.19144892692565918\n",
      "Epoch [214/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06418\n",
      "128\n",
      "Time elasped: 0.22988295555114746\n",
      "Epoch [215/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05950\n",
      "128\n",
      "Time elasped: 0.1927783489227295\n",
      "Mean Error: 0.0004927885602228343% \n",
      "--------------------------\n",
      "Epoch [216/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05927\n",
      "128\n",
      "Time elasped: 0.22705602645874023\n",
      "Epoch [217/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06457\n",
      "128\n",
      "Time elasped: 0.19222426414489746\n",
      "Epoch [218/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06317\n",
      "128\n",
      "Time elasped: 0.1999223232269287\n",
      "Epoch [219/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06414\n",
      "128\n",
      "Time elasped: 0.23457002639770508\n",
      "Epoch [220/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06345\n",
      "128\n",
      "Time elasped: 0.19933724403381348\n",
      "Mean Error: 0.000509857025463134% \n",
      "--------------------------\n",
      "Epoch [221/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06663\n",
      "128\n",
      "Time elasped: 0.21723222732543945\n",
      "Epoch [222/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06717\n",
      "128\n",
      "Time elasped: 0.2601923942565918\n",
      "Epoch [223/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06222\n",
      "128\n",
      "Time elasped: 0.21220088005065918\n",
      "Epoch [224/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06409\n",
      "128\n",
      "Time elasped: 0.24761247634887695\n",
      "Epoch [225/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06064\n",
      "128\n",
      "Time elasped: 0.23119640350341797\n",
      "Mean Error: 0.0005282351048663259% \n",
      "--------------------------\n",
      "Epoch [226/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06619\n",
      "128\n",
      "Time elasped: 0.20052123069763184\n",
      "Epoch [227/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06444\n",
      "128\n",
      "Time elasped: 0.24763917922973633\n",
      "Epoch [228/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06296\n",
      "128\n",
      "Time elasped: 0.21397995948791504\n",
      "Epoch [229/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06325\n",
      "128\n",
      "Time elasped: 0.2053232192993164\n",
      "Epoch [230/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06511\n",
      "128\n",
      "Time elasped: 0.24649381637573242\n",
      "Mean Error: 0.00044614943908527493% \n",
      "--------------------------\n",
      "Epoch [231/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06302\n",
      "128\n",
      "Time elasped: 0.2648777961730957\n",
      "Epoch [232/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06274\n",
      "128\n",
      "Time elasped: 0.285968542098999\n",
      "Epoch [233/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06160\n",
      "128\n",
      "Time elasped: 0.24914789199829102\n",
      "Epoch [234/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06316\n",
      "128\n",
      "Time elasped: 0.22691726684570312\n",
      "Epoch [235/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06102\n",
      "128\n",
      "Time elasped: 0.19806909561157227\n",
      "Mean Error: 0.00048563937889412045% \n",
      "--------------------------\n",
      "Epoch [236/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06326\n",
      "128\n",
      "Time elasped: 0.20995187759399414\n",
      "Epoch [237/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05964\n",
      "128\n",
      "Time elasped: 0.20540714263916016\n",
      "Epoch [238/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06086\n",
      "128\n",
      "Time elasped: 0.24307942390441895\n",
      "Epoch [239/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06777\n",
      "128\n",
      "Time elasped: 0.24829435348510742\n",
      "Epoch [240/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06566\n",
      "128\n",
      "Time elasped: 0.24300241470336914\n",
      "Mean Error: 0.00048437275108881295% \n",
      "--------------------------\n",
      "Epoch [241/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05956\n",
      "128\n",
      "Time elasped: 0.21257781982421875\n",
      "Epoch [242/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06622\n",
      "128\n",
      "Time elasped: 0.2622673511505127\n",
      "Epoch [243/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06399\n",
      "128\n",
      "Time elasped: 0.22894573211669922\n",
      "Epoch [244/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06679\n",
      "128\n",
      "Time elasped: 0.20962190628051758\n",
      "Epoch [245/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06390\n",
      "128\n",
      "Time elasped: 0.20022988319396973\n",
      "Mean Error: 0.0005170479998923838% \n",
      "--------------------------\n",
      "Epoch [246/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05887\n",
      "128\n",
      "Time elasped: 0.2103433609008789\n",
      "Epoch [247/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06229\n",
      "128\n",
      "Time elasped: 0.21024584770202637\n",
      "Epoch [248/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06107\n",
      "128\n",
      "Time elasped: 0.20001006126403809\n",
      "Epoch [249/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05865\n",
      "128\n",
      "Time elasped: 0.23774123191833496\n",
      "Epoch [250/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06350\n",
      "128\n",
      "Time elasped: 0.22719407081604004\n",
      "Mean Error: 0.0004896065802313387% \n",
      "--------------------------\n",
      "Epoch [251/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06085\n",
      "128\n",
      "Time elasped: 0.2188270092010498\n",
      "Epoch [252/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06384\n",
      "128\n",
      "Time elasped: 0.2534811496734619\n",
      "Epoch [253/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05953\n",
      "128\n",
      "Time elasped: 0.20669221878051758\n",
      "Epoch [254/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05649\n",
      "128\n",
      "Time elasped: 0.2849612236022949\n",
      "Epoch [255/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06772\n",
      "128\n",
      "Time elasped: 0.23239970207214355\n",
      "Mean Error: 0.0004769326187670231% \n",
      "--------------------------\n",
      "Epoch [256/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06084\n",
      "128\n",
      "Time elasped: 0.27133631706237793\n",
      "Epoch [257/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05979\n",
      "128\n",
      "Time elasped: 0.22869348526000977\n",
      "Epoch [258/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06675\n",
      "128\n",
      "Time elasped: 0.2070300579071045\n",
      "Epoch [259/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06638\n",
      "128\n",
      "Time elasped: 0.23794937133789062\n",
      "Epoch [260/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06318\n",
      "128\n",
      "Time elasped: 0.1950821876525879\n",
      "Mean Error: 0.0004820845788344741% \n",
      "--------------------------\n",
      "Epoch [261/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06350\n",
      "128\n",
      "Time elasped: 0.20045232772827148\n",
      "Epoch [262/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06638\n",
      "128\n",
      "Time elasped: 0.2735331058502197\n",
      "Epoch [263/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06722\n",
      "128\n",
      "Time elasped: 0.27659058570861816\n",
      "Epoch [264/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06564\n",
      "128\n",
      "Time elasped: 0.25326013565063477\n",
      "Epoch [265/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06301\n",
      "128\n",
      "Time elasped: 0.20033955574035645\n",
      "Mean Error: 0.0004947340348735452% \n",
      "--------------------------\n",
      "Epoch [266/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06076\n",
      "128\n",
      "Time elasped: 0.20729827880859375\n",
      "Epoch [267/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06182\n",
      "128\n",
      "Time elasped: 0.2256624698638916\n",
      "Epoch [268/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06090\n",
      "128\n",
      "Time elasped: 0.20458078384399414\n",
      "Epoch [269/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06560\n",
      "128\n",
      "Time elasped: 0.2230069637298584\n",
      "Epoch [270/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06110\n",
      "128\n",
      "Time elasped: 0.23924612998962402\n",
      "Mean Error: 0.0005122310831211507% \n",
      "--------------------------\n",
      "Epoch [271/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06374\n",
      "128\n",
      "Time elasped: 0.20560693740844727\n",
      "Epoch [272/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06014\n",
      "128\n",
      "Time elasped: 0.2101893424987793\n",
      "Epoch [273/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06097\n",
      "128\n",
      "Time elasped: 0.20972704887390137\n",
      "Epoch [274/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05656\n",
      "128\n",
      "Time elasped: 0.21019697189331055\n",
      "Epoch [275/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06418\n",
      "128\n",
      "Time elasped: 0.20011305809020996\n",
      "Mean Error: 0.0004934663884341717% \n",
      "--------------------------\n",
      "Epoch [276/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06209\n",
      "128\n",
      "Time elasped: 0.21491456031799316\n",
      "Epoch [277/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06268\n",
      "128\n",
      "Time elasped: 0.23608136177062988\n",
      "Epoch [278/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06179\n",
      "128\n",
      "Time elasped: 0.21185660362243652\n",
      "Epoch [279/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06211\n",
      "128\n",
      "Time elasped: 0.2280881404876709\n",
      "Epoch [280/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06459\n",
      "128\n",
      "Time elasped: 0.20304369926452637\n",
      "Mean Error: 0.000509711098857224% \n",
      "--------------------------\n",
      "Epoch [281/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05716\n",
      "128\n",
      "Time elasped: 0.2241802215576172\n",
      "Epoch [282/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06086\n",
      "128\n",
      "Time elasped: 0.2322850227355957\n",
      "Epoch [283/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06206\n",
      "128\n",
      "Time elasped: 0.2863497734069824\n",
      "Epoch [284/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05898\n",
      "128\n",
      "Time elasped: 0.19677996635437012\n",
      "Epoch [285/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05994\n",
      "128\n",
      "Time elasped: 0.2229607105255127\n",
      "Mean Error: 0.0005157153937034309% \n",
      "--------------------------\n",
      "Epoch [286/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06052\n",
      "128\n",
      "Time elasped: 0.21301674842834473\n",
      "Epoch [287/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06206\n",
      "128\n",
      "Time elasped: 0.251753568649292\n",
      "Epoch [288/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06820\n",
      "128\n",
      "Time elasped: 0.2180953025817871\n",
      "Epoch [289/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06420\n",
      "128\n",
      "Time elasped: 0.21765398979187012\n",
      "Epoch [290/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05948\n",
      "128\n",
      "Time elasped: 0.21308302879333496\n",
      "Mean Error: 0.0004881995846517384% \n",
      "--------------------------\n",
      "Epoch [291/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06040\n",
      "128\n",
      "Time elasped: 0.22615814208984375\n",
      "Epoch [292/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06316\n",
      "128\n",
      "Time elasped: 0.2040112018585205\n",
      "Epoch [293/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06546\n",
      "128\n",
      "Time elasped: 0.24567484855651855\n",
      "Epoch [294/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05977\n",
      "128\n",
      "Time elasped: 0.22005701065063477\n",
      "Epoch [295/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06207\n",
      "128\n",
      "Time elasped: 0.21565937995910645\n",
      "Mean Error: 0.0004492254520300776% \n",
      "--------------------------\n",
      "Epoch [296/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06283\n",
      "128\n",
      "Time elasped: 0.24718832969665527\n",
      "Epoch [297/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06213\n",
      "128\n",
      "Time elasped: 0.2311997413635254\n",
      "Epoch [298/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06196\n",
      "128\n",
      "Time elasped: 0.24893450736999512\n",
      "Epoch [299/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06461\n",
      "128\n",
      "Time elasped: 0.2524302005767822\n",
      "Epoch [300/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.05807\n",
      "128\n",
      "Time elasped: 0.22258377075195312\n",
      "Mean Error: 0.00048674605204723775% \n",
      "--------------------------\n",
      "Epoch [301/3000], learning_rates 0.000902, 0.902500\n",
      "Step [1/1], Loss: 0.06185\n",
      "128\n",
      "Time elasped: 0.20183491706848145\n",
      "Epoch [302/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06372\n",
      "128\n",
      "Time elasped: 0.22165322303771973\n",
      "Epoch [303/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06256\n",
      "128\n",
      "Time elasped: 0.2626643180847168\n",
      "Epoch [304/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06261\n",
      "128\n",
      "Time elasped: 0.22754311561584473\n",
      "Epoch [305/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06034\n",
      "128\n",
      "Time elasped: 0.26334261894226074\n",
      "Mean Error: 0.0005138514679856598% \n",
      "--------------------------\n",
      "Epoch [306/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06374\n",
      "128\n",
      "Time elasped: 0.26139307022094727\n",
      "Epoch [307/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06389\n",
      "128\n",
      "Time elasped: 0.2761249542236328\n",
      "Epoch [308/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05811\n",
      "128\n",
      "Time elasped: 0.26038384437561035\n",
      "Epoch [309/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06280\n",
      "128\n",
      "Time elasped: 0.25159788131713867\n",
      "Epoch [310/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06150\n",
      "128\n",
      "Time elasped: 0.2452559471130371\n",
      "Mean Error: 0.000493766157887876% \n",
      "--------------------------\n",
      "Epoch [311/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05962\n",
      "128\n",
      "Time elasped: 0.20884323120117188\n",
      "Epoch [312/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06298\n",
      "128\n",
      "Time elasped: 0.2050163745880127\n",
      "Epoch [313/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06139\n",
      "128\n",
      "Time elasped: 0.22078514099121094\n",
      "Epoch [314/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06047\n",
      "128\n",
      "Time elasped: 0.2615230083465576\n",
      "Epoch [315/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06549\n",
      "128\n",
      "Time elasped: 0.24358344078063965\n",
      "Mean Error: 0.00047106636338867247% \n",
      "--------------------------\n",
      "Epoch [316/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06148\n",
      "128\n",
      "Time elasped: 0.25075721740722656\n",
      "Epoch [317/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06011\n",
      "128\n",
      "Time elasped: 0.23734545707702637\n",
      "Epoch [318/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05745\n",
      "128\n",
      "Time elasped: 0.2086338996887207\n",
      "Epoch [319/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06111\n",
      "128\n",
      "Time elasped: 0.2110152244567871\n",
      "Epoch [320/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05974\n",
      "128\n",
      "Time elasped: 0.23164629936218262\n",
      "Mean Error: 0.0004969312576577067% \n",
      "--------------------------\n",
      "Epoch [321/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06357\n",
      "128\n",
      "Time elasped: 0.22589683532714844\n",
      "Epoch [322/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06018\n",
      "128\n",
      "Time elasped: 0.24322223663330078\n",
      "Epoch [323/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06417\n",
      "128\n",
      "Time elasped: 0.24803400039672852\n",
      "Epoch [324/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06047\n",
      "128\n",
      "Time elasped: 0.2227308750152588\n",
      "Epoch [325/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06488\n",
      "128\n",
      "Time elasped: 0.20029902458190918\n",
      "Mean Error: 0.0004715351387858391% \n",
      "--------------------------\n",
      "Epoch [326/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06271\n",
      "128\n",
      "Time elasped: 0.2317218780517578\n",
      "Epoch [327/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06003\n",
      "128\n",
      "Time elasped: 0.22964882850646973\n",
      "Epoch [328/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06113\n",
      "128\n",
      "Time elasped: 0.22075676918029785\n",
      "Epoch [329/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06051\n",
      "128\n",
      "Time elasped: 0.220977783203125\n",
      "Epoch [330/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06021\n",
      "128\n",
      "Time elasped: 0.2013692855834961\n",
      "Mean Error: 0.0004460265045054257% \n",
      "--------------------------\n",
      "Epoch [331/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06521\n",
      "128\n",
      "Time elasped: 0.23834824562072754\n",
      "Epoch [332/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05863\n",
      "128\n",
      "Time elasped: 0.22050738334655762\n",
      "Epoch [333/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06326\n",
      "128\n",
      "Time elasped: 0.19958710670471191\n",
      "Epoch [334/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06043\n",
      "128\n",
      "Time elasped: 0.2090294361114502\n",
      "Epoch [335/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06104\n",
      "128\n",
      "Time elasped: 0.2481374740600586\n",
      "Mean Error: 0.00047919241478666663% \n",
      "--------------------------\n",
      "Epoch [336/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06113\n",
      "128\n",
      "Time elasped: 0.21046042442321777\n",
      "Epoch [337/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06334\n",
      "128\n",
      "Time elasped: 0.28034186363220215\n",
      "Epoch [338/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06063\n",
      "128\n",
      "Time elasped: 0.2152557373046875\n",
      "Epoch [339/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06474\n",
      "128\n",
      "Time elasped: 0.21989011764526367\n",
      "Epoch [340/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06137\n",
      "128\n",
      "Time elasped: 0.24469566345214844\n",
      "Mean Error: 0.00046713813208043575% \n",
      "--------------------------\n",
      "Epoch [341/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06190\n",
      "128\n",
      "Time elasped: 0.27223730087280273\n",
      "Epoch [342/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06109\n",
      "128\n",
      "Time elasped: 0.2294924259185791\n",
      "Epoch [343/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05770\n",
      "128\n",
      "Time elasped: 0.22411298751831055\n",
      "Epoch [344/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06247\n",
      "128\n",
      "Time elasped: 0.2683563232421875\n",
      "Epoch [345/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06489\n",
      "128\n",
      "Time elasped: 0.23788022994995117\n",
      "Mean Error: 0.00047458935296162963% \n",
      "--------------------------\n",
      "Epoch [346/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05875\n",
      "128\n",
      "Time elasped: 0.23230957984924316\n",
      "Epoch [347/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06108\n",
      "128\n",
      "Time elasped: 0.21657419204711914\n",
      "Epoch [348/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05900\n",
      "128\n",
      "Time elasped: 0.23494863510131836\n",
      "Epoch [349/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06714\n",
      "128\n",
      "Time elasped: 0.32388901710510254\n",
      "Epoch [350/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06419\n",
      "128\n",
      "Time elasped: 0.2811775207519531\n",
      "Mean Error: 0.0004877060418948531% \n",
      "--------------------------\n",
      "Epoch [351/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05963\n",
      "128\n",
      "Time elasped: 0.2763087749481201\n",
      "Epoch [352/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05980\n",
      "128\n",
      "Time elasped: 0.28495121002197266\n",
      "Epoch [353/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06145\n",
      "128\n",
      "Time elasped: 0.2578465938568115\n",
      "Epoch [354/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05845\n",
      "128\n",
      "Time elasped: 0.27702999114990234\n",
      "Epoch [355/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06107\n",
      "128\n",
      "Time elasped: 0.29291677474975586\n",
      "Mean Error: 0.00048872921615839% \n",
      "--------------------------\n",
      "Epoch [356/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05855\n",
      "128\n",
      "Time elasped: 0.2993466854095459\n",
      "Epoch [357/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05641\n",
      "128\n",
      "Time elasped: 0.348712682723999\n",
      "Epoch [358/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06180\n",
      "128\n",
      "Time elasped: 0.3387753963470459\n",
      "Epoch [359/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06099\n",
      "128\n",
      "Time elasped: 0.23633098602294922\n",
      "Epoch [360/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06198\n",
      "128\n",
      "Time elasped: 0.2549855709075928\n",
      "Mean Error: 0.0004643186694011092% \n",
      "--------------------------\n",
      "Epoch [361/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05740\n",
      "128\n",
      "Time elasped: 0.2553977966308594\n",
      "Epoch [362/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06122\n",
      "128\n",
      "Time elasped: 0.22976970672607422\n",
      "Epoch [363/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05899\n",
      "128\n",
      "Time elasped: 0.24982595443725586\n",
      "Epoch [364/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05872\n",
      "128\n",
      "Time elasped: 0.25923609733581543\n",
      "Epoch [365/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06244\n",
      "128\n",
      "Time elasped: 0.24088454246520996\n",
      "Mean Error: 0.0004609428287949413% \n",
      "--------------------------\n",
      "Epoch [366/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05962\n",
      "128\n",
      "Time elasped: 0.22660040855407715\n",
      "Epoch [367/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05903\n",
      "128\n",
      "Time elasped: 0.21505355834960938\n",
      "Epoch [368/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06388\n",
      "128\n",
      "Time elasped: 0.2440342903137207\n",
      "Epoch [369/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06112\n",
      "128\n",
      "Time elasped: 0.2696695327758789\n",
      "Epoch [370/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06149\n",
      "128\n",
      "Time elasped: 0.2723515033721924\n",
      "Mean Error: 0.00048106155009008944% \n",
      "--------------------------\n",
      "Epoch [371/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06077\n",
      "128\n",
      "Time elasped: 0.23338842391967773\n",
      "Epoch [372/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05879\n",
      "128\n",
      "Time elasped: 0.25556492805480957\n",
      "Epoch [373/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06036\n",
      "128\n",
      "Time elasped: 0.3205733299255371\n",
      "Epoch [374/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06193\n",
      "128\n",
      "Time elasped: 0.2280271053314209\n",
      "Epoch [375/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06239\n",
      "128\n",
      "Time elasped: 0.21580100059509277\n",
      "Mean Error: 0.00047807482769712806% \n",
      "--------------------------\n",
      "Epoch [376/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06232\n",
      "128\n",
      "Time elasped: 0.2793116569519043\n",
      "Epoch [377/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06071\n",
      "128\n",
      "Time elasped: 0.2294905185699463\n",
      "Epoch [378/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06112\n",
      "128\n",
      "Time elasped: 0.2423412799835205\n",
      "Epoch [379/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05834\n",
      "128\n",
      "Time elasped: 0.2657477855682373\n",
      "Epoch [380/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06218\n",
      "128\n",
      "Time elasped: 0.22702383995056152\n",
      "Mean Error: 0.0004635017248801887% \n",
      "--------------------------\n",
      "Epoch [381/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05980\n",
      "128\n",
      "Time elasped: 0.234238862991333\n",
      "Epoch [382/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06459\n",
      "128\n",
      "Time elasped: 0.2427358627319336\n",
      "Epoch [383/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06495\n",
      "128\n",
      "Time elasped: 0.27131175994873047\n",
      "Epoch [384/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06389\n",
      "128\n",
      "Time elasped: 0.2316298484802246\n",
      "Epoch [385/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06277\n",
      "128\n",
      "Time elasped: 0.2518777847290039\n",
      "Mean Error: 0.00048470470937900245% \n",
      "--------------------------\n",
      "Epoch [386/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05820\n",
      "128\n",
      "Time elasped: 0.25855207443237305\n",
      "Epoch [387/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06218\n",
      "128\n",
      "Time elasped: 0.2542133331298828\n",
      "Epoch [388/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05936\n",
      "128\n",
      "Time elasped: 0.2576944828033447\n",
      "Epoch [389/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06195\n",
      "128\n",
      "Time elasped: 0.2615203857421875\n",
      "Epoch [390/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05906\n",
      "128\n",
      "Time elasped: 0.2623469829559326\n",
      "Mean Error: 0.00048439993406645954% \n",
      "--------------------------\n",
      "Epoch [391/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06164\n",
      "128\n",
      "Time elasped: 0.22666406631469727\n",
      "Epoch [392/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06517\n",
      "128\n",
      "Time elasped: 0.211167573928833\n",
      "Epoch [393/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06005\n",
      "128\n",
      "Time elasped: 0.2527017593383789\n",
      "Epoch [394/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05751\n",
      "128\n",
      "Time elasped: 0.22289323806762695\n",
      "Epoch [395/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05976\n",
      "128\n",
      "Time elasped: 0.27628493309020996\n",
      "Mean Error: 0.0004926221445202827% \n",
      "--------------------------\n",
      "Epoch [396/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05992\n",
      "128\n",
      "Time elasped: 0.2066488265991211\n",
      "Epoch [397/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05792\n",
      "128\n",
      "Time elasped: 0.25608181953430176\n",
      "Epoch [398/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06439\n",
      "128\n",
      "Time elasped: 0.2416520118713379\n",
      "Epoch [399/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06126\n",
      "128\n",
      "Time elasped: 0.20592617988586426\n",
      "Epoch [400/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.05891\n",
      "128\n",
      "Time elasped: 0.21907830238342285\n",
      "Mean Error: 0.0004772906540893018% \n",
      "--------------------------\n",
      "Epoch [401/3000], learning_rates 0.000857, 0.857375\n",
      "Step [1/1], Loss: 0.06018\n",
      "128\n",
      "Time elasped: 0.23482346534729004\n",
      "Epoch [402/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06447\n",
      "128\n",
      "Time elasped: 0.21974492073059082\n",
      "Epoch [403/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05779\n",
      "128\n",
      "Time elasped: 0.24199223518371582\n",
      "Epoch [404/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05935\n",
      "128\n",
      "Time elasped: 0.20963811874389648\n",
      "Epoch [405/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05958\n",
      "128\n",
      "Time elasped: 0.19837093353271484\n",
      "Mean Error: 0.00046519620809704065% \n",
      "--------------------------\n",
      "Epoch [406/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06492\n",
      "128\n",
      "Time elasped: 0.2369062900543213\n",
      "Epoch [407/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06125\n",
      "128\n",
      "Time elasped: 0.22310662269592285\n",
      "Epoch [408/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06020\n",
      "128\n",
      "Time elasped: 0.24032950401306152\n",
      "Epoch [409/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06007\n",
      "128\n",
      "Time elasped: 0.22978973388671875\n",
      "Epoch [410/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05992\n",
      "128\n",
      "Time elasped: 0.2277061939239502\n",
      "Mean Error: 0.00045560221769846976% \n",
      "--------------------------\n",
      "Epoch [411/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06330\n",
      "128\n",
      "Time elasped: 0.22954034805297852\n",
      "Epoch [412/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06038\n",
      "128\n",
      "Time elasped: 0.20035886764526367\n",
      "Epoch [413/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06003\n",
      "128\n",
      "Time elasped: 0.28006768226623535\n",
      "Epoch [414/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06073\n",
      "128\n",
      "Time elasped: 0.241546630859375\n",
      "Epoch [415/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06118\n",
      "128\n",
      "Time elasped: 0.23716330528259277\n",
      "Mean Error: 0.0004931829171255231% \n",
      "--------------------------\n",
      "Epoch [416/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06065\n",
      "128\n",
      "Time elasped: 0.2032637596130371\n",
      "Epoch [417/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06437\n",
      "128\n",
      "Time elasped: 0.23868632316589355\n",
      "Epoch [418/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05859\n",
      "128\n",
      "Time elasped: 0.21295714378356934\n",
      "Epoch [419/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06342\n",
      "128\n",
      "Time elasped: 0.21465253829956055\n",
      "Epoch [420/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05976\n",
      "128\n",
      "Time elasped: 0.19522690773010254\n",
      "Mean Error: 0.00048316357424482703% \n",
      "--------------------------\n",
      "Epoch [421/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06305\n",
      "128\n",
      "Time elasped: 0.2048335075378418\n",
      "Epoch [422/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06260\n",
      "128\n",
      "Time elasped: 0.2602672576904297\n",
      "Epoch [423/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06092\n",
      "128\n",
      "Time elasped: 0.24961113929748535\n",
      "Epoch [424/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05377\n",
      "128\n",
      "Time elasped: 0.24424505233764648\n",
      "Epoch [425/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05886\n",
      "128\n",
      "Time elasped: 0.2281639575958252\n",
      "Mean Error: 0.0004534777835942805% \n",
      "--------------------------\n",
      "Epoch [426/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06360\n",
      "128\n",
      "Time elasped: 0.24411988258361816\n",
      "Epoch [427/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06040\n",
      "128\n",
      "Time elasped: 0.21983885765075684\n",
      "Epoch [428/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05967\n",
      "128\n",
      "Time elasped: 0.21571898460388184\n",
      "Epoch [429/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06169\n",
      "128\n",
      "Time elasped: 0.19980740547180176\n",
      "Epoch [430/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05796\n",
      "128\n",
      "Time elasped: 0.22284936904907227\n",
      "Mean Error: 0.0004717422998510301% \n",
      "--------------------------\n",
      "Epoch [431/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05747\n",
      "128\n",
      "Time elasped: 0.25416111946105957\n",
      "Epoch [432/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05832\n",
      "128\n",
      "Time elasped: 0.24551773071289062\n",
      "Epoch [433/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05829\n",
      "128\n",
      "Time elasped: 0.22089433670043945\n",
      "Epoch [434/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05988\n",
      "128\n",
      "Time elasped: 0.20748496055603027\n",
      "Epoch [435/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05519\n",
      "128\n",
      "Time elasped: 0.21486759185791016\n",
      "Mean Error: 0.0004721991717815399% \n",
      "--------------------------\n",
      "Epoch [436/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05851\n",
      "128\n",
      "Time elasped: 0.2183849811553955\n",
      "Epoch [437/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06127\n",
      "128\n",
      "Time elasped: 0.23668622970581055\n",
      "Epoch [438/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06057\n",
      "128\n",
      "Time elasped: 0.25002121925354004\n",
      "Epoch [439/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06388\n",
      "128\n",
      "Time elasped: 0.23765921592712402\n",
      "Epoch [440/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06521\n",
      "128\n",
      "Time elasped: 0.2002277374267578\n",
      "Mean Error: 0.000496433989610523% \n",
      "--------------------------\n",
      "Epoch [441/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06265\n",
      "128\n",
      "Time elasped: 0.28975629806518555\n",
      "Epoch [442/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05861\n",
      "128\n",
      "Time elasped: 0.2643136978149414\n",
      "Epoch [443/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06002\n",
      "128\n",
      "Time elasped: 0.24251461029052734\n",
      "Epoch [444/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06518\n",
      "128\n",
      "Time elasped: 0.2877969741821289\n",
      "Epoch [445/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06009\n",
      "128\n",
      "Time elasped: 0.21967172622680664\n",
      "Mean Error: 0.000462173018604517% \n",
      "--------------------------\n",
      "Epoch [446/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06184\n",
      "128\n",
      "Time elasped: 0.22881865501403809\n",
      "Epoch [447/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05935\n",
      "128\n",
      "Time elasped: 0.1910996437072754\n",
      "Epoch [448/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06344\n",
      "128\n",
      "Time elasped: 0.25115275382995605\n",
      "Epoch [449/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06007\n",
      "128\n",
      "Time elasped: 0.2276146411895752\n",
      "Epoch [450/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05774\n",
      "128\n",
      "Time elasped: 0.21100115776062012\n",
      "Mean Error: 0.0004966155393049121% \n",
      "--------------------------\n",
      "Epoch [451/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05676\n",
      "128\n",
      "Time elasped: 0.24357366561889648\n",
      "Epoch [452/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06372\n",
      "128\n",
      "Time elasped: 0.1996748447418213\n",
      "Epoch [453/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05798\n",
      "128\n",
      "Time elasped: 0.25519251823425293\n",
      "Epoch [454/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05901\n",
      "128\n",
      "Time elasped: 0.19986176490783691\n",
      "Epoch [455/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06118\n",
      "128\n",
      "Time elasped: 0.2068333625793457\n",
      "Mean Error: 0.0005023934645578265% \n",
      "--------------------------\n",
      "Epoch [456/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06272\n",
      "128\n",
      "Time elasped: 0.23691344261169434\n",
      "Epoch [457/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06174\n",
      "128\n",
      "Time elasped: 0.22940850257873535\n",
      "Epoch [458/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05954\n",
      "128\n",
      "Time elasped: 0.2532057762145996\n",
      "Epoch [459/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06149\n",
      "128\n",
      "Time elasped: 0.3079521656036377\n",
      "Epoch [460/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06007\n",
      "128\n",
      "Time elasped: 0.2067584991455078\n",
      "Mean Error: 0.0004439089389052242% \n",
      "--------------------------\n",
      "Epoch [461/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05597\n",
      "128\n",
      "Time elasped: 0.2101898193359375\n",
      "Epoch [462/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06146\n",
      "128\n",
      "Time elasped: 0.2397615909576416\n",
      "Epoch [463/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05943\n",
      "128\n",
      "Time elasped: 0.21431350708007812\n",
      "Epoch [464/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05972\n",
      "128\n",
      "Time elasped: 0.23597073554992676\n",
      "Epoch [465/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06346\n",
      "128\n",
      "Time elasped: 0.20482444763183594\n",
      "Mean Error: 0.0004815153661184013% \n",
      "--------------------------\n",
      "Epoch [466/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05779\n",
      "128\n",
      "Time elasped: 0.24075531959533691\n",
      "Epoch [467/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05892\n",
      "128\n",
      "Time elasped: 0.2017819881439209\n",
      "Epoch [468/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06237\n",
      "128\n",
      "Time elasped: 0.20966792106628418\n",
      "Epoch [469/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06116\n",
      "128\n",
      "Time elasped: 0.2278895378112793\n",
      "Epoch [470/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06335\n",
      "128\n",
      "Time elasped: 0.2301023006439209\n",
      "Mean Error: 0.00047603013808839023% \n",
      "--------------------------\n",
      "Epoch [471/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05701\n",
      "128\n",
      "Time elasped: 0.24680089950561523\n",
      "Epoch [472/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06407\n",
      "128\n",
      "Time elasped: 0.25017714500427246\n",
      "Epoch [473/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05947\n",
      "128\n",
      "Time elasped: 0.23815250396728516\n",
      "Epoch [474/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06251\n",
      "128\n",
      "Time elasped: 0.23863887786865234\n",
      "Epoch [475/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05478\n",
      "128\n",
      "Time elasped: 0.252396821975708\n",
      "Mean Error: 0.0004903621156699955% \n",
      "--------------------------\n",
      "Epoch [476/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05842\n",
      "128\n",
      "Time elasped: 0.25716447830200195\n",
      "Epoch [477/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06032\n",
      "128\n",
      "Time elasped: 0.23262429237365723\n",
      "Epoch [478/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06631\n",
      "128\n",
      "Time elasped: 0.25018906593322754\n",
      "Epoch [479/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05887\n",
      "128\n",
      "Time elasped: 0.22025132179260254\n",
      "Epoch [480/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06431\n",
      "128\n",
      "Time elasped: 0.24950242042541504\n",
      "Mean Error: 0.00047620112309232354% \n",
      "--------------------------\n",
      "Epoch [481/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06091\n",
      "128\n",
      "Time elasped: 0.23318195343017578\n",
      "Epoch [482/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06021\n",
      "128\n",
      "Time elasped: 0.24407720565795898\n",
      "Epoch [483/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06608\n",
      "128\n",
      "Time elasped: 0.27841901779174805\n",
      "Epoch [484/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06123\n",
      "128\n",
      "Time elasped: 0.24387073516845703\n",
      "Epoch [485/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06007\n",
      "128\n",
      "Time elasped: 0.2506237030029297\n",
      "Mean Error: 0.00047225027810782194% \n",
      "--------------------------\n",
      "Epoch [486/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06367\n",
      "128\n",
      "Time elasped: 0.25515103340148926\n",
      "Epoch [487/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05935\n",
      "128\n",
      "Time elasped: 0.22520709037780762\n",
      "Epoch [488/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06231\n",
      "128\n",
      "Time elasped: 0.2298734188079834\n",
      "Epoch [489/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06032\n",
      "128\n",
      "Time elasped: 0.26510000228881836\n",
      "Epoch [490/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06111\n",
      "128\n",
      "Time elasped: 0.23562097549438477\n",
      "Mean Error: 0.0005127139738760889% \n",
      "--------------------------\n",
      "Epoch [491/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05770\n",
      "128\n",
      "Time elasped: 0.2391514778137207\n",
      "Epoch [492/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05902\n",
      "128\n",
      "Time elasped: 0.26093459129333496\n",
      "Epoch [493/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06197\n",
      "128\n",
      "Time elasped: 0.24997472763061523\n",
      "Epoch [494/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05952\n",
      "128\n",
      "Time elasped: 0.22991275787353516\n",
      "Epoch [495/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05807\n",
      "128\n",
      "Time elasped: 0.21007323265075684\n",
      "Mean Error: 0.0004646402085199952% \n",
      "--------------------------\n",
      "Epoch [496/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05349\n",
      "128\n",
      "Time elasped: 0.22534728050231934\n",
      "Epoch [497/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05832\n",
      "128\n",
      "Time elasped: 0.2081003189086914\n",
      "Epoch [498/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06057\n",
      "128\n",
      "Time elasped: 0.20336151123046875\n",
      "Epoch [499/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.05930\n",
      "128\n",
      "Time elasped: 0.21398162841796875\n",
      "Epoch [500/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06603\n",
      "128\n",
      "Time elasped: 0.20596790313720703\n",
      "Mean Error: 0.00045945157762616873% \n",
      "--------------------------\n",
      "Epoch [501/3000], learning_rates 0.000815, 0.814506\n",
      "Step [1/1], Loss: 0.06063\n",
      "128\n",
      "Time elasped: 0.23737859725952148\n",
      "Epoch [502/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05985\n",
      "128\n",
      "Time elasped: 0.21267008781433105\n",
      "Epoch [503/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05959\n",
      "128\n",
      "Time elasped: 0.20138120651245117\n",
      "Epoch [504/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05945\n",
      "128\n",
      "Time elasped: 0.22981500625610352\n",
      "Epoch [505/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06099\n",
      "128\n",
      "Time elasped: 0.20258259773254395\n",
      "Mean Error: 0.00046645579277537763% \n",
      "--------------------------\n",
      "Epoch [506/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06220\n",
      "128\n",
      "Time elasped: 0.19492816925048828\n",
      "Epoch [507/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05939\n",
      "128\n",
      "Time elasped: 0.21548819541931152\n",
      "Epoch [508/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06218\n",
      "128\n",
      "Time elasped: 0.21689248085021973\n",
      "Epoch [509/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05914\n",
      "128\n",
      "Time elasped: 0.2156083583831787\n",
      "Epoch [510/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05720\n",
      "128\n",
      "Time elasped: 0.22971224784851074\n",
      "Mean Error: 0.0004715838877018541% \n",
      "--------------------------\n",
      "Epoch [511/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06222\n",
      "128\n",
      "Time elasped: 0.2576255798339844\n",
      "Epoch [512/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05827\n",
      "128\n",
      "Time elasped: 0.235792875289917\n",
      "Epoch [513/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06117\n",
      "128\n",
      "Time elasped: 0.23579144477844238\n",
      "Epoch [514/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05717\n",
      "128\n",
      "Time elasped: 0.2177591323852539\n",
      "Epoch [515/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05545\n",
      "128\n",
      "Time elasped: 0.19999361038208008\n",
      "Mean Error: 0.00046340643893927336% \n",
      "--------------------------\n",
      "Epoch [516/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05609\n",
      "128\n",
      "Time elasped: 0.21813726425170898\n",
      "Epoch [517/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06077\n",
      "128\n",
      "Time elasped: 0.21764469146728516\n",
      "Epoch [518/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06563\n",
      "128\n",
      "Time elasped: 0.21783852577209473\n",
      "Epoch [519/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06079\n",
      "128\n",
      "Time elasped: 0.2949819564819336\n",
      "Epoch [520/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05925\n",
      "128\n",
      "Time elasped: 0.2267932891845703\n",
      "Mean Error: 0.0004578772932291031% \n",
      "--------------------------\n",
      "Epoch [521/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06036\n",
      "128\n",
      "Time elasped: 0.21555399894714355\n",
      "Epoch [522/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06218\n",
      "128\n",
      "Time elasped: 0.2280564308166504\n",
      "Epoch [523/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06230\n",
      "128\n",
      "Time elasped: 0.20964431762695312\n",
      "Epoch [524/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06562\n",
      "128\n",
      "Time elasped: 0.21839499473571777\n",
      "Epoch [525/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05722\n",
      "128\n",
      "Time elasped: 0.19652700424194336\n",
      "Mean Error: 0.0004690897767432034% \n",
      "--------------------------\n",
      "Epoch [526/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05634\n",
      "128\n",
      "Time elasped: 0.21554875373840332\n",
      "Epoch [527/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06070\n",
      "128\n",
      "Time elasped: 0.2036898136138916\n",
      "Epoch [528/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05775\n",
      "128\n",
      "Time elasped: 0.21773791313171387\n",
      "Epoch [529/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05938\n",
      "128\n",
      "Time elasped: 0.23564672470092773\n",
      "Epoch [530/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05802\n",
      "128\n",
      "Time elasped: 0.23777508735656738\n",
      "Mean Error: 0.0004498636699281633% \n",
      "--------------------------\n",
      "Epoch [531/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05932\n",
      "128\n",
      "Time elasped: 0.24333810806274414\n",
      "Epoch [532/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06031\n",
      "128\n",
      "Time elasped: 0.25531983375549316\n",
      "Epoch [533/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05802\n",
      "128\n",
      "Time elasped: 0.21495723724365234\n",
      "Epoch [534/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06009\n",
      "128\n",
      "Time elasped: 0.21103286743164062\n",
      "Epoch [535/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05686\n",
      "128\n",
      "Time elasped: 0.26012349128723145\n",
      "Mean Error: 0.0004779496230185032% \n",
      "--------------------------\n",
      "Epoch [536/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05785\n",
      "128\n",
      "Time elasped: 0.22220659255981445\n",
      "Epoch [537/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06144\n",
      "128\n",
      "Time elasped: 0.2373371124267578\n",
      "Epoch [538/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05741\n",
      "128\n",
      "Time elasped: 0.28284668922424316\n",
      "Epoch [539/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05897\n",
      "128\n",
      "Time elasped: 0.20943403244018555\n",
      "Epoch [540/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05623\n",
      "128\n",
      "Time elasped: 0.2599220275878906\n",
      "Mean Error: 0.0004239774134475738% \n",
      "--------------------------\n",
      "Epoch [541/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05504\n",
      "128\n",
      "Time elasped: 0.21216893196105957\n",
      "Epoch [542/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05964\n",
      "128\n",
      "Time elasped: 0.2068634033203125\n",
      "Epoch [543/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05856\n",
      "128\n",
      "Time elasped: 0.19596004486083984\n",
      "Epoch [544/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06460\n",
      "128\n",
      "Time elasped: 0.22476959228515625\n",
      "Epoch [545/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05727\n",
      "128\n",
      "Time elasped: 0.21867728233337402\n",
      "Mean Error: 0.00046461011515930295% \n",
      "--------------------------\n",
      "Epoch [546/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05559\n",
      "128\n",
      "Time elasped: 0.21097946166992188\n",
      "Epoch [547/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05868\n",
      "128\n",
      "Time elasped: 0.22830963134765625\n",
      "Epoch [548/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05688\n",
      "128\n",
      "Time elasped: 0.28013062477111816\n",
      "Epoch [549/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05841\n",
      "128\n",
      "Time elasped: 0.21027159690856934\n",
      "Epoch [550/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05823\n",
      "128\n",
      "Time elasped: 0.26663947105407715\n",
      "Mean Error: 0.00046122874482534826% \n",
      "--------------------------\n",
      "Epoch [551/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05459\n",
      "128\n",
      "Time elasped: 0.23034191131591797\n",
      "Epoch [552/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05517\n",
      "128\n",
      "Time elasped: 0.21693778038024902\n",
      "Epoch [553/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06166\n",
      "128\n",
      "Time elasped: 0.20692133903503418\n",
      "Epoch [554/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05603\n",
      "128\n",
      "Time elasped: 0.24302053451538086\n",
      "Epoch [555/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05922\n",
      "128\n",
      "Time elasped: 0.21694731712341309\n",
      "Mean Error: 0.00043360330164432526% \n",
      "--------------------------\n",
      "Epoch [556/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05907\n",
      "128\n",
      "Time elasped: 0.24043655395507812\n",
      "Epoch [557/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05917\n",
      "128\n",
      "Time elasped: 0.2199544906616211\n",
      "Epoch [558/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05507\n",
      "128\n",
      "Time elasped: 0.21962690353393555\n",
      "Epoch [559/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06024\n",
      "128\n",
      "Time elasped: 0.24005651473999023\n",
      "Epoch [560/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06076\n",
      "128\n",
      "Time elasped: 0.19269180297851562\n",
      "Mean Error: 0.0004295693652238697% \n",
      "--------------------------\n",
      "Epoch [561/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05546\n",
      "128\n",
      "Time elasped: 0.23353195190429688\n",
      "Epoch [562/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05698\n",
      "128\n",
      "Time elasped: 0.23487377166748047\n",
      "Epoch [563/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05404\n",
      "128\n",
      "Time elasped: 0.21494770050048828\n",
      "Epoch [564/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05781\n",
      "128\n",
      "Time elasped: 0.23454999923706055\n",
      "Epoch [565/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05965\n",
      "128\n",
      "Time elasped: 0.21806907653808594\n",
      "Mean Error: 0.0004703648737631738% \n",
      "--------------------------\n",
      "Epoch [566/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05634\n",
      "128\n",
      "Time elasped: 0.21955633163452148\n",
      "Epoch [567/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06271\n",
      "128\n",
      "Time elasped: 0.21797943115234375\n",
      "Epoch [568/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06122\n",
      "128\n",
      "Time elasped: 0.2297353744506836\n",
      "Epoch [569/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05873\n",
      "128\n",
      "Time elasped: 0.22510218620300293\n",
      "Epoch [570/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05746\n",
      "128\n",
      "Time elasped: 0.20700454711914062\n",
      "Mean Error: 0.0004032663127873093% \n",
      "--------------------------\n",
      "Epoch [571/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05901\n",
      "128\n",
      "Time elasped: 0.26128530502319336\n",
      "Epoch [572/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05893\n",
      "128\n",
      "Time elasped: 0.24027514457702637\n",
      "Epoch [573/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06027\n",
      "128\n",
      "Time elasped: 0.21198201179504395\n",
      "Epoch [574/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05462\n",
      "128\n",
      "Time elasped: 0.22902536392211914\n",
      "Epoch [575/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05525\n",
      "128\n",
      "Time elasped: 0.20081782341003418\n",
      "Mean Error: 0.00047220595297403634% \n",
      "--------------------------\n",
      "Epoch [576/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06276\n",
      "128\n",
      "Time elasped: 0.22626233100891113\n",
      "Epoch [577/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06134\n",
      "128\n",
      "Time elasped: 0.26669883728027344\n",
      "Epoch [578/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05639\n",
      "128\n",
      "Time elasped: 0.27197694778442383\n",
      "Epoch [579/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05947\n",
      "128\n",
      "Time elasped: 0.21111059188842773\n",
      "Epoch [580/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05721\n",
      "128\n",
      "Time elasped: 0.24937868118286133\n",
      "Mean Error: 0.00044949896982870996% \n",
      "--------------------------\n",
      "Epoch [581/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05742\n",
      "128\n",
      "Time elasped: 0.2141399383544922\n",
      "Epoch [582/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06169\n",
      "128\n",
      "Time elasped: 0.22533845901489258\n",
      "Epoch [583/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05726\n",
      "128\n",
      "Time elasped: 0.24966979026794434\n",
      "Epoch [584/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.06062\n",
      "128\n",
      "Time elasped: 0.23006749153137207\n",
      "Epoch [585/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05669\n",
      "128\n",
      "Time elasped: 0.20006823539733887\n",
      "Mean Error: 0.0004750152875203639% \n",
      "--------------------------\n",
      "Epoch [586/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05792\n",
      "128\n",
      "Time elasped: 0.22891998291015625\n",
      "Epoch [587/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05856\n",
      "128\n",
      "Time elasped: 0.21604681015014648\n",
      "Epoch [588/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05868\n",
      "128\n",
      "Time elasped: 0.21676039695739746\n",
      "Epoch [589/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05658\n",
      "128\n",
      "Time elasped: 0.20783543586730957\n",
      "Epoch [590/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05884\n",
      "128\n",
      "Time elasped: 0.20876097679138184\n",
      "Mean Error: 0.0004611335461959243% \n",
      "--------------------------\n",
      "Epoch [591/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05701\n",
      "128\n",
      "Time elasped: 0.2267322540283203\n",
      "Epoch [592/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05777\n",
      "128\n",
      "Time elasped: 0.2757875919342041\n",
      "Epoch [593/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05724\n",
      "128\n",
      "Time elasped: 0.2502458095550537\n",
      "Epoch [594/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05570\n",
      "128\n",
      "Time elasped: 0.20976591110229492\n",
      "Epoch [595/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05827\n",
      "128\n",
      "Time elasped: 0.2098383903503418\n",
      "Mean Error: 0.0004372145049273968% \n",
      "--------------------------\n",
      "Epoch [596/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05632\n",
      "128\n",
      "Time elasped: 0.2262411117553711\n",
      "Epoch [597/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05882\n",
      "128\n",
      "Time elasped: 0.20375919342041016\n",
      "Epoch [598/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05947\n",
      "128\n",
      "Time elasped: 0.27294468879699707\n",
      "Epoch [599/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05500\n",
      "128\n",
      "Time elasped: 0.23271894454956055\n",
      "Epoch [600/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05804\n",
      "128\n",
      "Time elasped: 0.23393988609313965\n",
      "Mean Error: 0.0004160473763477057% \n",
      "--------------------------\n",
      "Epoch [601/3000], learning_rates 0.000774, 0.773781\n",
      "Step [1/1], Loss: 0.05274\n",
      "128\n",
      "Time elasped: 0.22116732597351074\n",
      "Epoch [602/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05526\n",
      "128\n",
      "Time elasped: 0.1938800811767578\n",
      "Epoch [603/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05947\n",
      "128\n",
      "Time elasped: 0.23984885215759277\n",
      "Epoch [604/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05179\n",
      "128\n",
      "Time elasped: 0.21581602096557617\n",
      "Epoch [605/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05752\n",
      "128\n",
      "Time elasped: 0.22394323348999023\n",
      "Mean Error: 0.0004455981543287635% \n",
      "--------------------------\n",
      "Epoch [606/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05737\n",
      "128\n",
      "Time elasped: 0.23186683654785156\n",
      "Epoch [607/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05888\n",
      "128\n",
      "Time elasped: 0.2607097625732422\n",
      "Epoch [608/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05738\n",
      "128\n",
      "Time elasped: 0.21687722206115723\n",
      "Epoch [609/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06036\n",
      "128\n",
      "Time elasped: 0.22330331802368164\n",
      "Epoch [610/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06240\n",
      "128\n",
      "Time elasped: 0.23925399780273438\n",
      "Mean Error: 0.0004300675936974585% \n",
      "--------------------------\n",
      "Epoch [611/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05640\n",
      "128\n",
      "Time elasped: 0.23337507247924805\n",
      "Epoch [612/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05873\n",
      "128\n",
      "Time elasped: 0.2397463321685791\n",
      "Epoch [613/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05682\n",
      "128\n",
      "Time elasped: 0.2528090476989746\n",
      "Epoch [614/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05876\n",
      "128\n",
      "Time elasped: 0.19242262840270996\n",
      "Epoch [615/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05503\n",
      "128\n",
      "Time elasped: 0.24113965034484863\n",
      "Mean Error: 0.0004833033890463412% \n",
      "--------------------------\n",
      "Epoch [616/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05106\n",
      "128\n",
      "Time elasped: 0.24942612648010254\n",
      "Epoch [617/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05527\n",
      "128\n",
      "Time elasped: 0.25077247619628906\n",
      "Epoch [618/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05748\n",
      "128\n",
      "Time elasped: 0.24482250213623047\n",
      "Epoch [619/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05683\n",
      "128\n",
      "Time elasped: 0.21387743949890137\n",
      "Epoch [620/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05678\n",
      "128\n",
      "Time elasped: 0.22599577903747559\n",
      "Mean Error: 0.00046238338109105825% \n",
      "--------------------------\n",
      "Epoch [621/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06152\n",
      "128\n",
      "Time elasped: 0.23913192749023438\n",
      "Epoch [622/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06142\n",
      "128\n",
      "Time elasped: 0.26686835289001465\n",
      "Epoch [623/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05562\n",
      "128\n",
      "Time elasped: 0.2330634593963623\n",
      "Epoch [624/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05731\n",
      "128\n",
      "Time elasped: 0.25676512718200684\n",
      "Epoch [625/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06042\n",
      "128\n",
      "Time elasped: 0.22943782806396484\n",
      "Mean Error: 0.0004627258167602122% \n",
      "--------------------------\n",
      "Epoch [626/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05892\n",
      "128\n",
      "Time elasped: 0.24404358863830566\n",
      "Epoch [627/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06073\n",
      "128\n",
      "Time elasped: 0.23438525199890137\n",
      "Epoch [628/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05479\n",
      "128\n",
      "Time elasped: 0.22270607948303223\n",
      "Epoch [629/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05730\n",
      "128\n",
      "Time elasped: 0.22613883018493652\n",
      "Epoch [630/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06034\n",
      "128\n",
      "Time elasped: 0.2234807014465332\n",
      "Mean Error: 0.00045199194573797286% \n",
      "--------------------------\n",
      "Epoch [631/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05469\n",
      "128\n",
      "Time elasped: 0.22496795654296875\n",
      "Epoch [632/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05559\n",
      "128\n",
      "Time elasped: 0.20906376838684082\n",
      "Epoch [633/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05789\n",
      "128\n",
      "Time elasped: 0.22022342681884766\n",
      "Epoch [634/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05907\n",
      "128\n",
      "Time elasped: 0.19969582557678223\n",
      "Epoch [635/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05349\n",
      "128\n",
      "Time elasped: 0.2602996826171875\n",
      "Mean Error: 0.00043964837095700204% \n",
      "--------------------------\n",
      "Epoch [636/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05869\n",
      "128\n",
      "Time elasped: 0.20753908157348633\n",
      "Epoch [637/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05758\n",
      "128\n",
      "Time elasped: 0.19683289527893066\n",
      "Epoch [638/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05775\n",
      "128\n",
      "Time elasped: 0.22727584838867188\n",
      "Epoch [639/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05327\n",
      "128\n",
      "Time elasped: 0.23844552040100098\n",
      "Epoch [640/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06149\n",
      "128\n",
      "Time elasped: 0.23440957069396973\n",
      "Mean Error: 0.00046813025255687535% \n",
      "--------------------------\n",
      "Epoch [641/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05659\n",
      "128\n",
      "Time elasped: 0.2520639896392822\n",
      "Epoch [642/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05239\n",
      "128\n",
      "Time elasped: 0.23794054985046387\n",
      "Epoch [643/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05219\n",
      "128\n",
      "Time elasped: 0.2706024646759033\n",
      "Epoch [644/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05890\n",
      "128\n",
      "Time elasped: 0.2192063331604004\n",
      "Epoch [645/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05999\n",
      "128\n",
      "Time elasped: 0.2742431163787842\n",
      "Mean Error: 0.0004613687633536756% \n",
      "--------------------------\n",
      "Epoch [646/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05590\n",
      "128\n",
      "Time elasped: 0.24544692039489746\n",
      "Epoch [647/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05923\n",
      "128\n",
      "Time elasped: 0.1948530673980713\n",
      "Epoch [648/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05582\n",
      "128\n",
      "Time elasped: 0.24685263633728027\n",
      "Epoch [649/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05613\n",
      "128\n",
      "Time elasped: 0.2748415470123291\n",
      "Epoch [650/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05622\n",
      "128\n",
      "Time elasped: 0.22690796852111816\n",
      "Mean Error: 0.00046673158067278564% \n",
      "--------------------------\n",
      "Epoch [651/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05834\n",
      "128\n",
      "Time elasped: 0.20500445365905762\n",
      "Epoch [652/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05986\n",
      "128\n",
      "Time elasped: 0.21685075759887695\n",
      "Epoch [653/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06087\n",
      "128\n",
      "Time elasped: 0.21786165237426758\n",
      "Epoch [654/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05880\n",
      "128\n",
      "Time elasped: 0.19838666915893555\n",
      "Epoch [655/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05944\n",
      "128\n",
      "Time elasped: 0.2618541717529297\n",
      "Mean Error: 0.00045363305252976716% \n",
      "--------------------------\n",
      "Epoch [656/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05792\n",
      "128\n",
      "Time elasped: 0.21603131294250488\n",
      "Epoch [657/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05658\n",
      "128\n",
      "Time elasped: 0.23139691352844238\n",
      "Epoch [658/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05634\n",
      "128\n",
      "Time elasped: 0.2511930465698242\n",
      "Epoch [659/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05938\n",
      "128\n",
      "Time elasped: 0.2809619903564453\n",
      "Epoch [660/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05292\n",
      "128\n",
      "Time elasped: 0.19926667213439941\n",
      "Mean Error: 0.00045265277731232345% \n",
      "--------------------------\n",
      "Epoch [661/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05971\n",
      "128\n",
      "Time elasped: 0.22882461547851562\n",
      "Epoch [662/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05450\n",
      "128\n",
      "Time elasped: 0.25096821784973145\n",
      "Epoch [663/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05764\n",
      "128\n",
      "Time elasped: 0.21908044815063477\n",
      "Epoch [664/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05654\n",
      "128\n",
      "Time elasped: 0.21401095390319824\n",
      "Epoch [665/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05705\n",
      "128\n",
      "Time elasped: 0.20998239517211914\n",
      "Mean Error: 0.00048623926704749465% \n",
      "--------------------------\n",
      "Epoch [666/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05639\n",
      "128\n",
      "Time elasped: 0.24999713897705078\n",
      "Epoch [667/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05789\n",
      "128\n",
      "Time elasped: 0.2605245113372803\n",
      "Epoch [668/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05722\n",
      "128\n",
      "Time elasped: 0.202744722366333\n",
      "Epoch [669/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05929\n",
      "128\n",
      "Time elasped: 0.24410009384155273\n",
      "Epoch [670/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06299\n",
      "128\n",
      "Time elasped: 0.23638343811035156\n",
      "Mean Error: 0.00045396326459012926% \n",
      "--------------------------\n",
      "Epoch [671/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05525\n",
      "128\n",
      "Time elasped: 0.21609783172607422\n",
      "Epoch [672/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05756\n",
      "128\n",
      "Time elasped: 0.2133009433746338\n",
      "Epoch [673/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05708\n",
      "128\n",
      "Time elasped: 0.20662617683410645\n",
      "Epoch [674/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05720\n",
      "128\n",
      "Time elasped: 0.1969459056854248\n",
      "Epoch [675/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05467\n",
      "128\n",
      "Time elasped: 0.22972702980041504\n",
      "Mean Error: 0.0004772087559103966% \n",
      "--------------------------\n",
      "Epoch [676/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05564\n",
      "128\n",
      "Time elasped: 0.24007344245910645\n",
      "Epoch [677/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05723\n",
      "128\n",
      "Time elasped: 0.21955204010009766\n",
      "Epoch [678/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05762\n",
      "128\n",
      "Time elasped: 0.20913314819335938\n",
      "Epoch [679/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05505\n",
      "128\n",
      "Time elasped: 0.20822906494140625\n",
      "Epoch [680/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05647\n",
      "128\n",
      "Time elasped: 0.20575857162475586\n",
      "Mean Error: 0.0004537200147751719% \n",
      "--------------------------\n",
      "Epoch [681/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05961\n",
      "128\n",
      "Time elasped: 0.2308363914489746\n",
      "Epoch [682/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05790\n",
      "128\n",
      "Time elasped: 0.19594526290893555\n",
      "Epoch [683/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05886\n",
      "128\n",
      "Time elasped: 0.21297192573547363\n",
      "Epoch [684/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05419\n",
      "128\n",
      "Time elasped: 0.1963644027709961\n",
      "Epoch [685/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05273\n",
      "128\n",
      "Time elasped: 0.22071051597595215\n",
      "Mean Error: 0.0004149982996750623% \n",
      "--------------------------\n",
      "Epoch [686/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05645\n",
      "128\n",
      "Time elasped: 0.21681594848632812\n",
      "Epoch [687/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05695\n",
      "128\n",
      "Time elasped: 0.23728132247924805\n",
      "Epoch [688/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05532\n",
      "128\n",
      "Time elasped: 0.25603652000427246\n",
      "Epoch [689/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05827\n",
      "128\n",
      "Time elasped: 0.21668338775634766\n",
      "Epoch [690/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05597\n",
      "128\n",
      "Time elasped: 0.19960761070251465\n",
      "Mean Error: 0.000437327689724043% \n",
      "--------------------------\n",
      "Epoch [691/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05641\n",
      "128\n",
      "Time elasped: 0.2266080379486084\n",
      "Epoch [692/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05469\n",
      "128\n",
      "Time elasped: 0.19794726371765137\n",
      "Epoch [693/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05517\n",
      "128\n",
      "Time elasped: 0.20034170150756836\n",
      "Epoch [694/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05776\n",
      "128\n",
      "Time elasped: 0.21182775497436523\n",
      "Epoch [695/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05122\n",
      "128\n",
      "Time elasped: 0.23048710823059082\n",
      "Mean Error: 0.0004282066074665636% \n",
      "--------------------------\n",
      "Epoch [696/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05769\n",
      "128\n",
      "Time elasped: 0.23498773574829102\n",
      "Epoch [697/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05393\n",
      "128\n",
      "Time elasped: 0.23908400535583496\n",
      "Epoch [698/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.06084\n",
      "128\n",
      "Time elasped: 0.2208559513092041\n",
      "Epoch [699/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05129\n",
      "128\n",
      "Time elasped: 0.23010730743408203\n",
      "Epoch [700/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05704\n",
      "128\n",
      "Time elasped: 0.2315526008605957\n",
      "Mean Error: 0.0004473364388104528% \n",
      "--------------------------\n",
      "Epoch [701/3000], learning_rates 0.000735, 0.735092\n",
      "Step [1/1], Loss: 0.05307\n",
      "128\n",
      "Time elasped: 0.22318148612976074\n",
      "Epoch [702/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05507\n",
      "128\n",
      "Time elasped: 0.1951274871826172\n",
      "Epoch [703/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05498\n",
      "128\n",
      "Time elasped: 0.22742056846618652\n",
      "Epoch [704/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05562\n",
      "128\n",
      "Time elasped: 0.2125089168548584\n",
      "Epoch [705/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05722\n",
      "128\n",
      "Time elasped: 0.23182010650634766\n",
      "Mean Error: 0.0004493876185733825% \n",
      "--------------------------\n",
      "Epoch [706/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05271\n",
      "128\n",
      "Time elasped: 0.27422499656677246\n",
      "Epoch [707/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05371\n",
      "128\n",
      "Time elasped: 0.2259654998779297\n",
      "Epoch [708/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05605\n",
      "128\n",
      "Time elasped: 0.24715423583984375\n",
      "Epoch [709/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05722\n",
      "128\n",
      "Time elasped: 0.2442183494567871\n",
      "Epoch [710/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05844\n",
      "128\n",
      "Time elasped: 0.20233678817749023\n",
      "Mean Error: 0.0004318428400438279% \n",
      "--------------------------\n",
      "Epoch [711/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05532\n",
      "128\n",
      "Time elasped: 0.2101590633392334\n",
      "Epoch [712/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05793\n",
      "128\n",
      "Time elasped: 0.21583223342895508\n",
      "Epoch [713/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05691\n",
      "128\n",
      "Time elasped: 0.1905043125152588\n",
      "Epoch [714/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05764\n",
      "128\n",
      "Time elasped: 0.21733641624450684\n",
      "Epoch [715/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05420\n",
      "128\n",
      "Time elasped: 0.22247982025146484\n",
      "Mean Error: 0.0004214401706121862% \n",
      "--------------------------\n",
      "Epoch [716/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05131\n",
      "128\n",
      "Time elasped: 0.22043681144714355\n",
      "Epoch [717/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05669\n",
      "128\n",
      "Time elasped: 0.20991849899291992\n",
      "Epoch [718/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05201\n",
      "128\n",
      "Time elasped: 0.23397541046142578\n",
      "Epoch [719/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05638\n",
      "128\n",
      "Time elasped: 0.19887161254882812\n",
      "Epoch [720/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05437\n",
      "128\n",
      "Time elasped: 0.21020054817199707\n",
      "Mean Error: 0.00042984209721907973% \n",
      "--------------------------\n",
      "Epoch [721/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05600\n",
      "128\n",
      "Time elasped: 0.2197551727294922\n",
      "Epoch [722/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05396\n",
      "128\n",
      "Time elasped: 0.23364591598510742\n",
      "Epoch [723/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05394\n",
      "128\n",
      "Time elasped: 0.24733996391296387\n",
      "Epoch [724/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05624\n",
      "128\n",
      "Time elasped: 0.21918916702270508\n",
      "Epoch [725/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05678\n",
      "128\n",
      "Time elasped: 0.2229156494140625\n",
      "Mean Error: 0.0004447168030310422% \n",
      "--------------------------\n",
      "Epoch [726/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05282\n",
      "128\n",
      "Time elasped: 0.21038341522216797\n",
      "Epoch [727/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05454\n",
      "128\n",
      "Time elasped: 0.2125539779663086\n",
      "Epoch [728/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05121\n",
      "128\n",
      "Time elasped: 0.2073514461517334\n",
      "Epoch [729/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05591\n",
      "128\n",
      "Time elasped: 0.3022899627685547\n",
      "Epoch [730/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05710\n",
      "128\n",
      "Time elasped: 0.24359726905822754\n",
      "Mean Error: 0.00043925485806539655% \n",
      "--------------------------\n",
      "Epoch [731/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05132\n",
      "128\n",
      "Time elasped: 0.20284581184387207\n",
      "Epoch [732/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05273\n",
      "128\n",
      "Time elasped: 0.25429654121398926\n",
      "Epoch [733/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04969\n",
      "128\n",
      "Time elasped: 0.21510839462280273\n",
      "Epoch [734/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05378\n",
      "128\n",
      "Time elasped: 0.22950959205627441\n",
      "Epoch [735/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05390\n",
      "128\n",
      "Time elasped: 0.19699740409851074\n",
      "Mean Error: 0.00043206734699197114% \n",
      "--------------------------\n",
      "Epoch [736/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05271\n",
      "128\n",
      "Time elasped: 0.2328953742980957\n",
      "Epoch [737/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05219\n",
      "128\n",
      "Time elasped: 0.20027780532836914\n",
      "Epoch [738/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05574\n",
      "128\n",
      "Time elasped: 0.2165238857269287\n",
      "Epoch [739/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04914\n",
      "128\n",
      "Time elasped: 0.25157928466796875\n",
      "Epoch [740/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05129\n",
      "128\n",
      "Time elasped: 0.25342249870300293\n",
      "Mean Error: 0.0004089045396540314% \n",
      "--------------------------\n",
      "Epoch [741/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05380\n",
      "128\n",
      "Time elasped: 0.2330951690673828\n",
      "Epoch [742/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05174\n",
      "128\n",
      "Time elasped: 0.2219374179840088\n",
      "Epoch [743/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05443\n",
      "128\n",
      "Time elasped: 0.20128965377807617\n",
      "Epoch [744/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05352\n",
      "128\n",
      "Time elasped: 0.2003176212310791\n",
      "Epoch [745/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05282\n",
      "128\n",
      "Time elasped: 0.19985580444335938\n",
      "Mean Error: 0.00044899174827151% \n",
      "--------------------------\n",
      "Epoch [746/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05212\n",
      "128\n",
      "Time elasped: 0.2145850658416748\n",
      "Epoch [747/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05319\n",
      "128\n",
      "Time elasped: 0.2552638053894043\n",
      "Epoch [748/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04947\n",
      "128\n",
      "Time elasped: 0.22117280960083008\n",
      "Epoch [749/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05291\n",
      "128\n",
      "Time elasped: 0.2586979866027832\n",
      "Epoch [750/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05185\n",
      "128\n",
      "Time elasped: 0.20357537269592285\n",
      "Mean Error: 0.0004014996229670942% \n",
      "--------------------------\n",
      "Epoch [751/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05314\n",
      "128\n",
      "Time elasped: 0.2200150489807129\n",
      "Epoch [752/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05452\n",
      "128\n",
      "Time elasped: 0.25424766540527344\n",
      "Epoch [753/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05307\n",
      "128\n",
      "Time elasped: 0.23613905906677246\n",
      "Epoch [754/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05357\n",
      "128\n",
      "Time elasped: 0.20009779930114746\n",
      "Epoch [755/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04952\n",
      "128\n",
      "Time elasped: 0.23317742347717285\n",
      "Mean Error: 0.00041115289786830544% \n",
      "--------------------------\n",
      "Epoch [756/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05419\n",
      "128\n",
      "Time elasped: 0.21698999404907227\n",
      "Epoch [757/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04931\n",
      "128\n",
      "Time elasped: 0.21412873268127441\n",
      "Epoch [758/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04996\n",
      "128\n",
      "Time elasped: 0.20003771781921387\n",
      "Epoch [759/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05139\n",
      "128\n",
      "Time elasped: 0.22530245780944824\n",
      "Epoch [760/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05328\n",
      "128\n",
      "Time elasped: 0.22007107734680176\n",
      "Mean Error: 0.0004182374686934054% \n",
      "--------------------------\n",
      "Epoch [761/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05585\n",
      "128\n",
      "Time elasped: 0.22423815727233887\n",
      "Epoch [762/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05356\n",
      "128\n",
      "Time elasped: 0.2304842472076416\n",
      "Epoch [763/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05264\n",
      "128\n",
      "Time elasped: 0.19855546951293945\n",
      "Epoch [764/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05293\n",
      "128\n",
      "Time elasped: 0.20192360877990723\n",
      "Epoch [765/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05221\n",
      "128\n",
      "Time elasped: 0.1944725513458252\n",
      "Mean Error: 0.00042066810419782996% \n",
      "--------------------------\n",
      "Epoch [766/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04953\n",
      "128\n",
      "Time elasped: 0.21409058570861816\n",
      "Epoch [767/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04935\n",
      "128\n",
      "Time elasped: 0.24564576148986816\n",
      "Epoch [768/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05160\n",
      "128\n",
      "Time elasped: 0.22340607643127441\n",
      "Epoch [769/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05414\n",
      "128\n",
      "Time elasped: 0.24685454368591309\n",
      "Epoch [770/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05069\n",
      "128\n",
      "Time elasped: 0.20612859725952148\n",
      "Mean Error: 0.00039444747380912304% \n",
      "--------------------------\n",
      "Epoch [771/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05341\n",
      "128\n",
      "Time elasped: 0.2367846965789795\n",
      "Epoch [772/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04919\n",
      "128\n",
      "Time elasped: 0.22056889533996582\n",
      "Epoch [773/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04713\n",
      "128\n",
      "Time elasped: 0.20561695098876953\n",
      "Epoch [774/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05421\n",
      "128\n",
      "Time elasped: 0.23934698104858398\n",
      "Epoch [775/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04859\n",
      "128\n",
      "Time elasped: 0.2108173370361328\n",
      "Mean Error: 0.00039405206916853786% \n",
      "--------------------------\n",
      "Epoch [776/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05334\n",
      "128\n",
      "Time elasped: 0.22887372970581055\n",
      "Epoch [777/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05162\n",
      "128\n",
      "Time elasped: 0.2502729892730713\n",
      "Epoch [778/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05260\n",
      "128\n",
      "Time elasped: 0.27709412574768066\n",
      "Epoch [779/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04980\n",
      "128\n",
      "Time elasped: 0.2777087688446045\n",
      "Epoch [780/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05211\n",
      "128\n",
      "Time elasped: 0.3318488597869873\n",
      "Mean Error: 0.00041307142237201333% \n",
      "--------------------------\n",
      "Epoch [781/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05035\n",
      "128\n",
      "Time elasped: 0.29149746894836426\n",
      "Epoch [782/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04916\n",
      "128\n",
      "Time elasped: 0.2798619270324707\n",
      "Epoch [783/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05145\n",
      "128\n",
      "Time elasped: 0.26030921936035156\n",
      "Epoch [784/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05308\n",
      "128\n",
      "Time elasped: 0.2569911479949951\n",
      "Epoch [785/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05042\n",
      "128\n",
      "Time elasped: 0.2607560157775879\n",
      "Mean Error: 0.0003972099511884153% \n",
      "--------------------------\n",
      "Epoch [786/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05153\n",
      "128\n",
      "Time elasped: 0.20231270790100098\n",
      "Epoch [787/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04814\n",
      "128\n",
      "Time elasped: 0.23194479942321777\n",
      "Epoch [788/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04848\n",
      "128\n",
      "Time elasped: 0.24933457374572754\n",
      "Epoch [789/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05121\n",
      "128\n",
      "Time elasped: 0.2686634063720703\n",
      "Epoch [790/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05334\n",
      "128\n",
      "Time elasped: 0.2500641345977783\n",
      "Mean Error: 0.00039149916847236454% \n",
      "--------------------------\n",
      "Epoch [791/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04783\n",
      "128\n",
      "Time elasped: 0.23601436614990234\n",
      "Epoch [792/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04497\n",
      "128\n",
      "Time elasped: 0.2206282615661621\n",
      "Epoch [793/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05329\n",
      "128\n",
      "Time elasped: 0.21942591667175293\n",
      "Epoch [794/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04739\n",
      "128\n",
      "Time elasped: 0.22355294227600098\n",
      "Epoch [795/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05445\n",
      "128\n",
      "Time elasped: 0.20666289329528809\n",
      "Mean Error: 0.0003839716373477131% \n",
      "--------------------------\n",
      "Epoch [796/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04940\n",
      "128\n",
      "Time elasped: 0.23313069343566895\n",
      "Epoch [797/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05287\n",
      "128\n",
      "Time elasped: 0.25286340713500977\n",
      "Epoch [798/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04862\n",
      "128\n",
      "Time elasped: 0.23006176948547363\n",
      "Epoch [799/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05002\n",
      "128\n",
      "Time elasped: 0.22043204307556152\n",
      "Epoch [800/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.05043\n",
      "128\n",
      "Time elasped: 0.22960972785949707\n",
      "Mean Error: 0.0004340350569691509% \n",
      "--------------------------\n",
      "Epoch [801/3000], learning_rates 0.000698, 0.698337\n",
      "Step [1/1], Loss: 0.04687\n",
      "128\n",
      "Time elasped: 0.2668590545654297\n",
      "Epoch [802/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04672\n",
      "128\n",
      "Time elasped: 0.21641969680786133\n",
      "Epoch [803/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04880\n",
      "128\n",
      "Time elasped: 0.2000267505645752\n",
      "Epoch [804/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04669\n",
      "128\n",
      "Time elasped: 0.2197272777557373\n",
      "Epoch [805/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04999\n",
      "128\n",
      "Time elasped: 0.20042705535888672\n",
      "Mean Error: 0.0003655388136394322% \n",
      "--------------------------\n",
      "Epoch [806/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04806\n",
      "128\n",
      "Time elasped: 0.21362829208374023\n",
      "Epoch [807/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05174\n",
      "128\n",
      "Time elasped: 0.23822760581970215\n",
      "Epoch [808/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05112\n",
      "128\n",
      "Time elasped: 0.211256742477417\n",
      "Epoch [809/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05156\n",
      "128\n",
      "Time elasped: 0.22060871124267578\n",
      "Epoch [810/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05129\n",
      "128\n",
      "Time elasped: 0.21618890762329102\n",
      "Mean Error: 0.00037360555143095553% \n",
      "--------------------------\n",
      "Epoch [811/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04921\n",
      "128\n",
      "Time elasped: 0.24246430397033691\n",
      "Epoch [812/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05027\n",
      "128\n",
      "Time elasped: 0.22348403930664062\n",
      "Epoch [813/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04985\n",
      "128\n",
      "Time elasped: 0.21254181861877441\n",
      "Epoch [814/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05364\n",
      "128\n",
      "Time elasped: 0.21189355850219727\n",
      "Epoch [815/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04932\n",
      "128\n",
      "Time elasped: 0.2653074264526367\n",
      "Mean Error: 0.0003928385558538139% \n",
      "--------------------------\n",
      "Epoch [816/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05026\n",
      "128\n",
      "Time elasped: 0.2039029598236084\n",
      "Epoch [817/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04609\n",
      "128\n",
      "Time elasped: 0.23028349876403809\n",
      "Epoch [818/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04629\n",
      "128\n",
      "Time elasped: 0.2248220443725586\n",
      "Epoch [819/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05168\n",
      "128\n",
      "Time elasped: 0.236100435256958\n",
      "Epoch [820/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05278\n",
      "128\n",
      "Time elasped: 0.2546236515045166\n",
      "Mean Error: 0.00036728865234181285% \n",
      "--------------------------\n",
      "Epoch [821/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05339\n",
      "128\n",
      "Time elasped: 0.22980690002441406\n",
      "Epoch [822/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05000\n",
      "128\n",
      "Time elasped: 0.25006747245788574\n",
      "Epoch [823/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05160\n",
      "128\n",
      "Time elasped: 0.20266127586364746\n",
      "Epoch [824/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05021\n",
      "128\n",
      "Time elasped: 0.19998931884765625\n",
      "Epoch [825/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05029\n",
      "128\n",
      "Time elasped: 0.22878432273864746\n",
      "Mean Error: 0.00039938921690918505% \n",
      "--------------------------\n",
      "Epoch [826/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05039\n",
      "128\n",
      "Time elasped: 0.22239422798156738\n",
      "Epoch [827/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04909\n",
      "128\n",
      "Time elasped: 0.2520594596862793\n",
      "Epoch [828/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04824\n",
      "128\n",
      "Time elasped: 0.23920297622680664\n",
      "Epoch [829/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05034\n",
      "128\n",
      "Time elasped: 0.23467326164245605\n",
      "Epoch [830/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04676\n",
      "128\n",
      "Time elasped: 0.2306976318359375\n",
      "Mean Error: 0.0004209239559713751% \n",
      "--------------------------\n",
      "Epoch [831/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04897\n",
      "128\n",
      "Time elasped: 0.1989293098449707\n",
      "Epoch [832/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04955\n",
      "128\n",
      "Time elasped: 0.20599365234375\n",
      "Epoch [833/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04924\n",
      "128\n",
      "Time elasped: 0.22932815551757812\n",
      "Epoch [834/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05173\n",
      "128\n",
      "Time elasped: 0.2699418067932129\n",
      "Epoch [835/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04962\n",
      "128\n",
      "Time elasped: 0.2248077392578125\n",
      "Mean Error: 0.00039590371306985617% \n",
      "--------------------------\n",
      "Epoch [836/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04838\n",
      "128\n",
      "Time elasped: 0.262631893157959\n",
      "Epoch [837/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04976\n",
      "128\n",
      "Time elasped: 0.2502772808074951\n",
      "Epoch [838/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05070\n",
      "128\n",
      "Time elasped: 0.22681069374084473\n",
      "Epoch [839/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04669\n",
      "128\n",
      "Time elasped: 0.21668219566345215\n",
      "Epoch [840/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05025\n",
      "128\n",
      "Time elasped: 0.23195624351501465\n",
      "Mean Error: 0.0003842491714749485% \n",
      "--------------------------\n",
      "Epoch [841/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05052\n",
      "128\n",
      "Time elasped: 0.20395135879516602\n",
      "Epoch [842/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05137\n",
      "128\n",
      "Time elasped: 0.23984384536743164\n",
      "Epoch [843/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04862\n",
      "128\n",
      "Time elasped: 0.24855256080627441\n",
      "Epoch [844/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05078\n",
      "128\n",
      "Time elasped: 0.23461222648620605\n",
      "Epoch [845/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05445\n",
      "128\n",
      "Time elasped: 0.2034440040588379\n",
      "Mean Error: 0.00038149423198774457% \n",
      "--------------------------\n",
      "Epoch [846/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04630\n",
      "128\n",
      "Time elasped: 0.2404637336730957\n",
      "Epoch [847/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05257\n",
      "128\n",
      "Time elasped: 0.20296692848205566\n",
      "Epoch [848/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04473\n",
      "128\n",
      "Time elasped: 0.21324753761291504\n",
      "Epoch [849/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04743\n",
      "128\n",
      "Time elasped: 0.2369675636291504\n",
      "Epoch [850/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04884\n",
      "128\n",
      "Time elasped: 0.2400369644165039\n",
      "Mean Error: 0.0004086188564542681% \n",
      "--------------------------\n",
      "Epoch [851/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05275\n",
      "128\n",
      "Time elasped: 0.2401130199432373\n",
      "Epoch [852/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04879\n",
      "128\n",
      "Time elasped: 0.21772336959838867\n",
      "Epoch [853/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05042\n",
      "128\n",
      "Time elasped: 0.19751548767089844\n",
      "Epoch [854/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04842\n",
      "128\n",
      "Time elasped: 0.2276294231414795\n",
      "Epoch [855/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04652\n",
      "128\n",
      "Time elasped: 0.21600651741027832\n",
      "Mean Error: 0.00041209274786524475% \n",
      "--------------------------\n",
      "Epoch [856/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05195\n",
      "128\n",
      "Time elasped: 0.21414661407470703\n",
      "Epoch [857/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04640\n",
      "128\n",
      "Time elasped: 0.19808220863342285\n",
      "Epoch [858/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04960\n",
      "128\n",
      "Time elasped: 0.23759031295776367\n",
      "Epoch [859/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04798\n",
      "128\n",
      "Time elasped: 0.2124316692352295\n",
      "Epoch [860/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04784\n",
      "128\n",
      "Time elasped: 0.21754837036132812\n",
      "Mean Error: 0.0003987261443398893% \n",
      "--------------------------\n",
      "Epoch [861/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04819\n",
      "128\n",
      "Time elasped: 0.24158215522766113\n",
      "Epoch [862/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04605\n",
      "128\n",
      "Time elasped: 0.21369218826293945\n",
      "Epoch [863/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04743\n",
      "128\n",
      "Time elasped: 0.20682501792907715\n",
      "Epoch [864/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04916\n",
      "128\n",
      "Time elasped: 0.21512317657470703\n",
      "Epoch [865/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05040\n",
      "128\n",
      "Time elasped: 0.23489093780517578\n",
      "Mean Error: 0.0003914419503416866% \n",
      "--------------------------\n",
      "Epoch [866/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04958\n",
      "128\n",
      "Time elasped: 0.2597620487213135\n",
      "Epoch [867/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05022\n",
      "128\n",
      "Time elasped: 0.23437166213989258\n",
      "Epoch [868/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05100\n",
      "128\n",
      "Time elasped: 0.2492842674255371\n",
      "Epoch [869/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05181\n",
      "128\n",
      "Time elasped: 0.19504785537719727\n",
      "Epoch [870/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04682\n",
      "128\n",
      "Time elasped: 0.2624516487121582\n",
      "Mean Error: 0.0003781887062359601% \n",
      "--------------------------\n",
      "Epoch [871/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04780\n",
      "128\n",
      "Time elasped: 0.21460342407226562\n",
      "Epoch [872/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04988\n",
      "128\n",
      "Time elasped: 0.2263171672821045\n",
      "Epoch [873/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04777\n",
      "128\n",
      "Time elasped: 0.21723556518554688\n",
      "Epoch [874/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05261\n",
      "128\n",
      "Time elasped: 0.21609973907470703\n",
      "Epoch [875/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04349\n",
      "128\n",
      "Time elasped: 0.207061767578125\n",
      "Mean Error: 0.00039792220923118293% \n",
      "--------------------------\n",
      "Epoch [876/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04687\n",
      "128\n",
      "Time elasped: 0.21901345252990723\n",
      "Epoch [877/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04708\n",
      "128\n",
      "Time elasped: 0.22038793563842773\n",
      "Epoch [878/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04771\n",
      "128\n",
      "Time elasped: 0.21030306816101074\n",
      "Epoch [879/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04909\n",
      "128\n",
      "Time elasped: 0.22401785850524902\n",
      "Epoch [880/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04838\n",
      "128\n",
      "Time elasped: 0.21541190147399902\n",
      "Mean Error: 0.0003835531242657453% \n",
      "--------------------------\n",
      "Epoch [881/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04828\n",
      "128\n",
      "Time elasped: 0.2759721279144287\n",
      "Epoch [882/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04651\n",
      "128\n",
      "Time elasped: 0.23057293891906738\n",
      "Epoch [883/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05354\n",
      "128\n",
      "Time elasped: 0.20937132835388184\n",
      "Epoch [884/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05144\n",
      "128\n",
      "Time elasped: 0.22369742393493652\n",
      "Epoch [885/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04514\n",
      "128\n",
      "Time elasped: 0.21679234504699707\n",
      "Mean Error: 0.0003911925887223333% \n",
      "--------------------------\n",
      "Epoch [886/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04981\n",
      "128\n",
      "Time elasped: 0.20672821998596191\n",
      "Epoch [887/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04681\n",
      "128\n",
      "Time elasped: 0.19974541664123535\n",
      "Epoch [888/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04831\n",
      "128\n",
      "Time elasped: 0.2199079990386963\n",
      "Epoch [889/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04923\n",
      "128\n",
      "Time elasped: 0.22524046897888184\n",
      "Epoch [890/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04893\n",
      "128\n",
      "Time elasped: 0.22495436668395996\n",
      "Mean Error: 0.00037850518128834665% \n",
      "--------------------------\n",
      "Epoch [891/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04852\n",
      "128\n",
      "Time elasped: 0.22681021690368652\n",
      "Epoch [892/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04859\n",
      "128\n",
      "Time elasped: 0.20891189575195312\n",
      "Epoch [893/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05002\n",
      "128\n",
      "Time elasped: 0.20729494094848633\n",
      "Epoch [894/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04707\n",
      "128\n",
      "Time elasped: 0.20680761337280273\n",
      "Epoch [895/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05289\n",
      "128\n",
      "Time elasped: 0.20713329315185547\n",
      "Mean Error: 0.00038247331394813955% \n",
      "--------------------------\n",
      "Epoch [896/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05057\n",
      "128\n",
      "Time elasped: 0.2299337387084961\n",
      "Epoch [897/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04671\n",
      "128\n",
      "Time elasped: 0.24802350997924805\n",
      "Epoch [898/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04955\n",
      "128\n",
      "Time elasped: 0.23586153984069824\n",
      "Epoch [899/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04904\n",
      "128\n",
      "Time elasped: 0.2221975326538086\n",
      "Epoch [900/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.04534\n",
      "128\n",
      "Time elasped: 0.2976052761077881\n",
      "Mean Error: 0.000377777440007776% \n",
      "--------------------------\n",
      "Epoch [901/3000], learning_rates 0.000663, 0.663420\n",
      "Step [1/1], Loss: 0.05004\n",
      "128\n",
      "Time elasped: 0.24877452850341797\n",
      "Epoch [902/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04799\n",
      "128\n",
      "Time elasped: 0.2333676815032959\n",
      "Epoch [903/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05166\n",
      "128\n",
      "Time elasped: 0.200883150100708\n",
      "Epoch [904/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05046\n",
      "128\n",
      "Time elasped: 0.23209595680236816\n",
      "Epoch [905/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04745\n",
      "128\n",
      "Time elasped: 0.19965338706970215\n",
      "Mean Error: 0.0004070245486218482% \n",
      "--------------------------\n",
      "Epoch [906/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04897\n",
      "128\n",
      "Time elasped: 0.23447203636169434\n",
      "Epoch [907/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05138\n",
      "128\n",
      "Time elasped: 0.19884538650512695\n",
      "Epoch [908/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04876\n",
      "128\n",
      "Time elasped: 0.20975136756896973\n",
      "Epoch [909/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04736\n",
      "128\n",
      "Time elasped: 0.21375679969787598\n",
      "Epoch [910/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04929\n",
      "128\n",
      "Time elasped: 0.2436511516571045\n",
      "Mean Error: 0.0004051056457683444% \n",
      "--------------------------\n",
      "Epoch [911/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04824\n",
      "128\n",
      "Time elasped: 0.2700693607330322\n",
      "Epoch [912/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04906\n",
      "128\n",
      "Time elasped: 0.22029399871826172\n",
      "Epoch [913/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04811\n",
      "128\n",
      "Time elasped: 0.23946809768676758\n",
      "Epoch [914/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04711\n",
      "128\n",
      "Time elasped: 0.23018813133239746\n",
      "Epoch [915/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04988\n",
      "128\n",
      "Time elasped: 0.2543015480041504\n",
      "Mean Error: 0.0003595916787162423% \n",
      "--------------------------\n",
      "Epoch [916/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04808\n",
      "128\n",
      "Time elasped: 0.2499406337738037\n",
      "Epoch [917/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05391\n",
      "128\n",
      "Time elasped: 0.2449643611907959\n",
      "Epoch [918/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04527\n",
      "128\n",
      "Time elasped: 0.24929571151733398\n",
      "Epoch [919/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05241\n",
      "128\n",
      "Time elasped: 0.21642422676086426\n",
      "Epoch [920/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04551\n",
      "128\n",
      "Time elasped: 0.2190258502960205\n",
      "Mean Error: 0.00038899367791600525% \n",
      "--------------------------\n",
      "Epoch [921/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04893\n",
      "128\n",
      "Time elasped: 0.20209407806396484\n",
      "Epoch [922/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04452\n",
      "128\n",
      "Time elasped: 0.20021653175354004\n",
      "Epoch [923/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05006\n",
      "128\n",
      "Time elasped: 0.22869110107421875\n",
      "Epoch [924/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04659\n",
      "128\n",
      "Time elasped: 0.2278590202331543\n",
      "Epoch [925/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04769\n",
      "128\n",
      "Time elasped: 0.24737143516540527\n",
      "Mean Error: 0.00035705737536773086% \n",
      "--------------------------\n",
      "Epoch [926/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04648\n",
      "128\n",
      "Time elasped: 0.24344325065612793\n",
      "Epoch [927/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05390\n",
      "128\n",
      "Time elasped: 0.2370607852935791\n",
      "Epoch [928/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04946\n",
      "128\n",
      "Time elasped: 0.21807360649108887\n",
      "Epoch [929/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04641\n",
      "128\n",
      "Time elasped: 0.22972750663757324\n",
      "Epoch [930/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04795\n",
      "128\n",
      "Time elasped: 0.21681857109069824\n",
      "Mean Error: 0.000394717586459592% \n",
      "--------------------------\n",
      "Epoch [931/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04623\n",
      "128\n",
      "Time elasped: 0.23771071434020996\n",
      "Epoch [932/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05149\n",
      "128\n",
      "Time elasped: 0.24919366836547852\n",
      "Epoch [933/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04443\n",
      "128\n",
      "Time elasped: 0.2065582275390625\n",
      "Epoch [934/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04842\n",
      "128\n",
      "Time elasped: 0.25006628036499023\n",
      "Epoch [935/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04228\n",
      "128\n",
      "Time elasped: 0.20589900016784668\n",
      "Mean Error: 0.0003478816070128232% \n",
      "--------------------------\n",
      "Epoch [936/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04940\n",
      "128\n",
      "Time elasped: 0.21650409698486328\n",
      "Epoch [937/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04825\n",
      "128\n",
      "Time elasped: 0.2929859161376953\n",
      "Epoch [938/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04986\n",
      "128\n",
      "Time elasped: 0.22773075103759766\n",
      "Epoch [939/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04886\n",
      "128\n",
      "Time elasped: 0.2388768196105957\n",
      "Epoch [940/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05299\n",
      "128\n",
      "Time elasped: 0.23314237594604492\n",
      "Mean Error: 0.0003514575946610421% \n",
      "--------------------------\n",
      "Epoch [941/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04774\n",
      "128\n",
      "Time elasped: 0.20649385452270508\n",
      "Epoch [942/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04888\n",
      "128\n",
      "Time elasped: 0.22012567520141602\n",
      "Epoch [943/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04796\n",
      "128\n",
      "Time elasped: 0.2124190330505371\n",
      "Epoch [944/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04667\n",
      "128\n",
      "Time elasped: 0.22157025337219238\n",
      "Epoch [945/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04740\n",
      "128\n",
      "Time elasped: 0.20705318450927734\n",
      "Mean Error: 0.00034891083487309515% \n",
      "--------------------------\n",
      "Epoch [946/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04675\n",
      "128\n",
      "Time elasped: 0.2732100486755371\n",
      "Epoch [947/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04494\n",
      "128\n",
      "Time elasped: 0.24499940872192383\n",
      "Epoch [948/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04954\n",
      "128\n",
      "Time elasped: 0.23205995559692383\n",
      "Epoch [949/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04493\n",
      "128\n",
      "Time elasped: 0.21920180320739746\n",
      "Epoch [950/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04839\n",
      "128\n",
      "Time elasped: 0.24664568901062012\n",
      "Mean Error: 0.0003465560730546713% \n",
      "--------------------------\n",
      "Epoch [951/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04748\n",
      "128\n",
      "Time elasped: 0.23583698272705078\n",
      "Epoch [952/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04705\n",
      "128\n",
      "Time elasped: 0.2754373550415039\n",
      "Epoch [953/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04649\n",
      "128\n",
      "Time elasped: 0.2742350101470947\n",
      "Epoch [954/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04702\n",
      "128\n",
      "Time elasped: 0.26100683212280273\n",
      "Epoch [955/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04939\n",
      "128\n",
      "Time elasped: 0.24011707305908203\n",
      "Mean Error: 0.00037584122037515044% \n",
      "--------------------------\n",
      "Epoch [956/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04999\n",
      "128\n",
      "Time elasped: 0.21831321716308594\n",
      "Epoch [957/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04830\n",
      "128\n",
      "Time elasped: 0.21380257606506348\n",
      "Epoch [958/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05440\n",
      "128\n",
      "Time elasped: 0.3075127601623535\n",
      "Epoch [959/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04824\n",
      "128\n",
      "Time elasped: 0.23342347145080566\n",
      "Epoch [960/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05043\n",
      "128\n",
      "Time elasped: 0.2772822380065918\n",
      "Mean Error: 0.00032673461828380823% \n",
      "--------------------------\n",
      "Epoch [961/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04791\n",
      "128\n",
      "Time elasped: 0.25876402854919434\n",
      "Epoch [962/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04823\n",
      "128\n",
      "Time elasped: 0.2427809238433838\n",
      "Epoch [963/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04606\n",
      "128\n",
      "Time elasped: 0.28020334243774414\n",
      "Epoch [964/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04631\n",
      "128\n",
      "Time elasped: 0.25371718406677246\n",
      "Epoch [965/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04359\n",
      "128\n",
      "Time elasped: 0.2895846366882324\n",
      "Mean Error: 0.00039719059714116156% \n",
      "--------------------------\n",
      "Epoch [966/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04695\n",
      "128\n",
      "Time elasped: 0.26203155517578125\n",
      "Epoch [967/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04665\n",
      "128\n",
      "Time elasped: 0.34438157081604004\n",
      "Epoch [968/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04648\n",
      "128\n",
      "Time elasped: 0.3239634037017822\n",
      "Epoch [969/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04673\n",
      "128\n",
      "Time elasped: 0.3279275894165039\n",
      "Epoch [970/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04503\n",
      "128\n",
      "Time elasped: 0.4982607364654541\n",
      "Mean Error: 0.0003469859657343477% \n",
      "--------------------------\n",
      "Epoch [971/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04777\n",
      "128\n",
      "Time elasped: 0.32299137115478516\n",
      "Epoch [972/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04915\n",
      "128\n",
      "Time elasped: 0.24686813354492188\n",
      "Epoch [973/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04375\n",
      "128\n",
      "Time elasped: 0.2619643211364746\n",
      "Epoch [974/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04836\n",
      "128\n",
      "Time elasped: 0.2643251419067383\n",
      "Epoch [975/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04685\n",
      "128\n",
      "Time elasped: 0.29352831840515137\n",
      "Mean Error: 0.00036449023173190653% \n",
      "--------------------------\n",
      "Epoch [976/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04574\n",
      "128\n",
      "Time elasped: 0.3033771514892578\n",
      "Epoch [977/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04678\n",
      "128\n",
      "Time elasped: 0.29195070266723633\n",
      "Epoch [978/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04713\n",
      "128\n",
      "Time elasped: 0.31821608543395996\n",
      "Epoch [979/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04889\n",
      "128\n",
      "Time elasped: 0.3064608573913574\n",
      "Epoch [980/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04587\n",
      "128\n",
      "Time elasped: 0.44173145294189453\n",
      "Mean Error: 0.00036691094283014536% \n",
      "--------------------------\n",
      "Epoch [981/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04395\n",
      "128\n",
      "Time elasped: 0.2928469181060791\n",
      "Epoch [982/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04641\n",
      "128\n",
      "Time elasped: 0.2765481472015381\n",
      "Epoch [983/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05311\n",
      "128\n",
      "Time elasped: 0.2731504440307617\n",
      "Epoch [984/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04259\n",
      "128\n",
      "Time elasped: 0.2701680660247803\n",
      "Epoch [985/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04756\n",
      "128\n",
      "Time elasped: 0.2599985599517822\n",
      "Mean Error: 0.0003466792986728251% \n",
      "--------------------------\n",
      "Epoch [986/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04697\n",
      "128\n",
      "Time elasped: 0.2836577892303467\n",
      "Epoch [987/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05054\n",
      "128\n",
      "Time elasped: 0.2433624267578125\n",
      "Epoch [988/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04442\n",
      "128\n",
      "Time elasped: 0.2862093448638916\n",
      "Epoch [989/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04697\n",
      "128\n",
      "Time elasped: 0.2587599754333496\n",
      "Epoch [990/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04816\n",
      "128\n",
      "Time elasped: 0.24134445190429688\n",
      "Mean Error: 0.00037742129643447697% \n",
      "--------------------------\n",
      "Epoch [991/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04482\n",
      "128\n",
      "Time elasped: 0.2687549591064453\n",
      "Epoch [992/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04524\n",
      "128\n",
      "Time elasped: 0.269970178604126\n",
      "Epoch [993/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04468\n",
      "128\n",
      "Time elasped: 0.27524852752685547\n",
      "Epoch [994/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05122\n",
      "128\n",
      "Time elasped: 0.2387092113494873\n",
      "Epoch [995/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04458\n",
      "128\n",
      "Time elasped: 0.2167191505432129\n",
      "Mean Error: 0.00037462517502717674% \n",
      "--------------------------\n",
      "Epoch [996/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04518\n",
      "128\n",
      "Time elasped: 0.2866513729095459\n",
      "Epoch [997/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04476\n",
      "128\n",
      "Time elasped: 0.2474672794342041\n",
      "Epoch [998/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04518\n",
      "128\n",
      "Time elasped: 0.2327587604522705\n",
      "Epoch [999/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04470\n",
      "128\n",
      "Time elasped: 0.2336406707763672\n",
      "Epoch [1000/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.05003\n",
      "128\n",
      "Time elasped: 0.24614858627319336\n",
      "Mean Error: 0.00035900744842365384% \n",
      "--------------------------\n",
      "Epoch [1001/3000], learning_rates 0.000630, 0.630249\n",
      "Step [1/1], Loss: 0.04323\n",
      "128\n",
      "Time elasped: 0.2566709518432617\n",
      "Epoch [1002/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04659\n",
      "128\n",
      "Time elasped: 0.2392735481262207\n",
      "Epoch [1003/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.05109\n",
      "128\n",
      "Time elasped: 0.2536170482635498\n",
      "Epoch [1004/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04572\n",
      "128\n",
      "Time elasped: 0.26348447799682617\n",
      "Epoch [1005/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04874\n",
      "128\n",
      "Time elasped: 0.24197006225585938\n",
      "Mean Error: 0.00035802373895421624% \n",
      "--------------------------\n",
      "Epoch [1006/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04953\n",
      "128\n",
      "Time elasped: 0.27014851570129395\n",
      "Epoch [1007/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04572\n",
      "128\n",
      "Time elasped: 0.2502162456512451\n",
      "Epoch [1008/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04664\n",
      "128\n",
      "Time elasped: 0.22293353080749512\n",
      "Epoch [1009/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04923\n",
      "128\n",
      "Time elasped: 0.2871732711791992\n",
      "Epoch [1010/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04901\n",
      "128\n",
      "Time elasped: 0.2895658016204834\n",
      "Mean Error: 0.0003647704725153744% \n",
      "--------------------------\n",
      "Epoch [1011/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04624\n",
      "128\n",
      "Time elasped: 0.260164737701416\n",
      "Epoch [1012/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04959\n",
      "128\n",
      "Time elasped: 0.27274560928344727\n",
      "Epoch [1013/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04711\n",
      "128\n",
      "Time elasped: 0.2501983642578125\n",
      "Epoch [1014/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04864\n",
      "128\n",
      "Time elasped: 0.25899195671081543\n",
      "Epoch [1015/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04540\n",
      "128\n",
      "Time elasped: 0.2407543659210205\n",
      "Mean Error: 0.00035439571365714073% \n",
      "--------------------------\n",
      "Epoch [1016/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04535\n",
      "128\n",
      "Time elasped: 0.23296618461608887\n",
      "Epoch [1017/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04852\n",
      "128\n",
      "Time elasped: 0.23001909255981445\n",
      "Epoch [1018/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04394\n",
      "128\n",
      "Time elasped: 0.30693888664245605\n",
      "Epoch [1019/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.05012\n",
      "128\n",
      "Time elasped: 0.2748391628265381\n",
      "Epoch [1020/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04740\n",
      "128\n",
      "Time elasped: 0.2351973056793213\n",
      "Mean Error: 0.000397884170524776% \n",
      "--------------------------\n",
      "Epoch [1021/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04815\n",
      "128\n",
      "Time elasped: 0.22617268562316895\n",
      "Epoch [1022/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04620\n",
      "128\n",
      "Time elasped: 0.26131510734558105\n",
      "Epoch [1023/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04965\n",
      "128\n",
      "Time elasped: 0.25023365020751953\n",
      "Epoch [1024/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04584\n",
      "128\n",
      "Time elasped: 0.22037482261657715\n",
      "Epoch [1025/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.05056\n",
      "128\n",
      "Time elasped: 0.23521780967712402\n",
      "Mean Error: 0.0003550815745256841% \n",
      "--------------------------\n",
      "Epoch [1026/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04578\n",
      "128\n",
      "Time elasped: 0.24865126609802246\n",
      "Epoch [1027/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04556\n",
      "128\n",
      "Time elasped: 0.22760820388793945\n",
      "Epoch [1028/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04363\n",
      "128\n",
      "Time elasped: 0.3218655586242676\n",
      "Epoch [1029/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04221\n",
      "128\n",
      "Time elasped: 0.3230733871459961\n",
      "Epoch [1030/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04675\n",
      "128\n",
      "Time elasped: 0.2714066505432129\n",
      "Mean Error: 0.0003628428094089031% \n",
      "--------------------------\n",
      "Epoch [1031/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04658\n",
      "128\n",
      "Time elasped: 0.25687432289123535\n",
      "Epoch [1032/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04747\n",
      "128\n",
      "Time elasped: 0.2393941879272461\n",
      "Epoch [1033/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04813\n",
      "128\n",
      "Time elasped: 0.24580788612365723\n",
      "Epoch [1034/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04381\n",
      "128\n",
      "Time elasped: 0.22903013229370117\n",
      "Epoch [1035/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04501\n",
      "128\n",
      "Time elasped: 0.22486257553100586\n",
      "Mean Error: 0.0003749454044736922% \n",
      "--------------------------\n",
      "Epoch [1036/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04716\n",
      "128\n",
      "Time elasped: 0.2716493606567383\n",
      "Epoch [1037/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04585\n",
      "128\n",
      "Time elasped: 0.2692439556121826\n",
      "Epoch [1038/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04609\n",
      "128\n",
      "Time elasped: 0.2532541751861572\n",
      "Epoch [1039/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04574\n",
      "128\n",
      "Time elasped: 0.2865617275238037\n",
      "Epoch [1040/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04531\n",
      "128\n",
      "Time elasped: 0.2673332691192627\n",
      "Mean Error: 0.0003672438324429095% \n",
      "--------------------------\n",
      "Epoch [1041/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04935\n",
      "128\n",
      "Time elasped: 0.2957782745361328\n",
      "Epoch [1042/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04883\n",
      "128\n",
      "Time elasped: 0.4462156295776367\n",
      "Epoch [1043/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04185\n",
      "128\n",
      "Time elasped: 0.4376060962677002\n",
      "Epoch [1044/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04813\n",
      "128\n",
      "Time elasped: 0.29012417793273926\n",
      "Epoch [1045/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04530\n",
      "128\n",
      "Time elasped: 0.3404374122619629\n",
      "Mean Error: 0.00036802529939450324% \n",
      "--------------------------\n",
      "Epoch [1046/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.05088\n",
      "128\n",
      "Time elasped: 0.25575780868530273\n",
      "Epoch [1047/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04568\n",
      "128\n",
      "Time elasped: 0.2864048480987549\n",
      "Epoch [1048/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04822\n",
      "128\n",
      "Time elasped: 0.3274550437927246\n",
      "Epoch [1049/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04624\n",
      "128\n",
      "Time elasped: 0.27624011039733887\n",
      "Epoch [1050/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04270\n",
      "128\n",
      "Time elasped: 0.3995192050933838\n",
      "Mean Error: 0.0003596525639295578% \n",
      "--------------------------\n",
      "Epoch [1051/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04876\n",
      "128\n",
      "Time elasped: 0.2617945671081543\n",
      "Epoch [1052/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04773\n",
      "128\n",
      "Time elasped: 0.2672550678253174\n",
      "Epoch [1053/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04503\n",
      "128\n",
      "Time elasped: 0.28302741050720215\n",
      "Epoch [1054/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04538\n",
      "128\n",
      "Time elasped: 0.2730741500854492\n",
      "Epoch [1055/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04573\n",
      "128\n",
      "Time elasped: 0.2846972942352295\n",
      "Mean Error: 0.00037402461748570204% \n",
      "--------------------------\n",
      "Epoch [1056/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04916\n",
      "128\n",
      "Time elasped: 0.23297739028930664\n",
      "Epoch [1057/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04635\n",
      "128\n",
      "Time elasped: 0.26839470863342285\n",
      "Epoch [1058/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04546\n",
      "128\n",
      "Time elasped: 0.2714664936065674\n",
      "Epoch [1059/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04312\n",
      "128\n",
      "Time elasped: 0.23338913917541504\n",
      "Epoch [1060/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04558\n",
      "128\n",
      "Time elasped: 0.3164355754852295\n",
      "Mean Error: 0.0003548717650119215% \n",
      "--------------------------\n",
      "Epoch [1061/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04502\n",
      "128\n",
      "Time elasped: 0.3412463665008545\n",
      "Epoch [1062/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04487\n",
      "128\n",
      "Time elasped: 0.3386650085449219\n",
      "Epoch [1063/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04451\n",
      "128\n",
      "Time elasped: 0.3439207077026367\n",
      "Epoch [1064/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04722\n",
      "128\n",
      "Time elasped: 0.34014153480529785\n",
      "Epoch [1065/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04785\n",
      "128\n",
      "Time elasped: 0.3266878128051758\n",
      "Mean Error: 0.0003877870040014386% \n",
      "--------------------------\n",
      "Epoch [1066/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04789\n",
      "128\n",
      "Time elasped: 0.31060171127319336\n",
      "Epoch [1067/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04753\n",
      "128\n",
      "Time elasped: 0.3500039577484131\n",
      "Epoch [1068/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04664\n",
      "128\n",
      "Time elasped: 0.343097448348999\n",
      "Epoch [1069/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04369\n",
      "128\n",
      "Time elasped: 0.3632316589355469\n",
      "Epoch [1070/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04630\n",
      "128\n",
      "Time elasped: 0.3478538990020752\n",
      "Mean Error: 0.0003778059617616236% \n",
      "--------------------------\n",
      "Epoch [1071/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04752\n",
      "128\n",
      "Time elasped: 0.3593318462371826\n",
      "Epoch [1072/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04624\n",
      "128\n",
      "Time elasped: 0.32037854194641113\n",
      "Epoch [1073/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04435\n",
      "128\n",
      "Time elasped: 0.32613253593444824\n",
      "Epoch [1074/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04469\n",
      "128\n",
      "Time elasped: 0.3325076103210449\n",
      "Epoch [1075/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04492\n",
      "128\n",
      "Time elasped: 0.3317689895629883\n",
      "Mean Error: 0.0003637986665125936% \n",
      "--------------------------\n",
      "Epoch [1076/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04341\n",
      "128\n",
      "Time elasped: 0.35025858879089355\n",
      "Epoch [1077/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04797\n",
      "128\n",
      "Time elasped: 0.34949827194213867\n",
      "Epoch [1078/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04328\n",
      "128\n",
      "Time elasped: 0.339799165725708\n",
      "Epoch [1079/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04675\n",
      "128\n",
      "Time elasped: 0.33839869499206543\n",
      "Epoch [1080/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04523\n",
      "128\n",
      "Time elasped: 0.3325824737548828\n",
      "Mean Error: 0.0003776398953050375% \n",
      "--------------------------\n",
      "Epoch [1081/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04854\n",
      "128\n",
      "Time elasped: 0.33043360710144043\n",
      "Epoch [1082/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04751\n",
      "128\n",
      "Time elasped: 0.31407904624938965\n",
      "Epoch [1083/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04646\n",
      "128\n",
      "Time elasped: 0.32541871070861816\n",
      "Epoch [1084/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04390\n",
      "128\n",
      "Time elasped: 0.34470534324645996\n",
      "Epoch [1085/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04306\n",
      "128\n",
      "Time elasped: 0.3431973457336426\n",
      "Mean Error: 0.0003418602282181382% \n",
      "--------------------------\n",
      "Epoch [1086/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04367\n",
      "128\n",
      "Time elasped: 0.32233381271362305\n",
      "Epoch [1087/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04184\n",
      "128\n",
      "Time elasped: 0.3376448154449463\n",
      "Epoch [1088/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04416\n",
      "128\n",
      "Time elasped: 0.32706189155578613\n",
      "Epoch [1089/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04687\n",
      "128\n",
      "Time elasped: 0.3385341167449951\n",
      "Epoch [1090/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04227\n",
      "128\n",
      "Time elasped: 0.33476924896240234\n",
      "Mean Error: 0.00034952154965139925% \n",
      "--------------------------\n",
      "Epoch [1091/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.05022\n",
      "128\n",
      "Time elasped: 0.3251152038574219\n",
      "Epoch [1092/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04226\n",
      "128\n",
      "Time elasped: 0.34458184242248535\n",
      "Epoch [1093/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04558\n",
      "128\n",
      "Time elasped: 0.33019137382507324\n",
      "Epoch [1094/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04641\n",
      "128\n",
      "Time elasped: 0.3216285705566406\n",
      "Epoch [1095/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04191\n",
      "128\n",
      "Time elasped: 0.3254885673522949\n",
      "Mean Error: 0.0003661811933852732% \n",
      "--------------------------\n",
      "Epoch [1096/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04477\n",
      "128\n",
      "Time elasped: 0.34009695053100586\n",
      "Epoch [1097/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04440\n",
      "128\n",
      "Time elasped: 0.336345911026001\n",
      "Epoch [1098/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04370\n",
      "128\n",
      "Time elasped: 0.3207108974456787\n",
      "Epoch [1099/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04249\n",
      "128\n",
      "Time elasped: 0.3461458683013916\n",
      "Epoch [1100/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04236\n",
      "128\n",
      "Time elasped: 0.3310697078704834\n",
      "Mean Error: 0.0003408656921237707% \n",
      "--------------------------\n",
      "Epoch [1101/3000], learning_rates 0.000599, 0.598737\n",
      "Step [1/1], Loss: 0.04184\n",
      "128\n",
      "Time elasped: 0.3330199718475342\n",
      "Epoch [1102/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04470\n",
      "128\n",
      "Time elasped: 0.3268871307373047\n",
      "Epoch [1103/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04613\n",
      "128\n",
      "Time elasped: 0.3130509853363037\n",
      "Epoch [1104/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04294\n",
      "128\n",
      "Time elasped: 0.32929086685180664\n",
      "Epoch [1105/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04784\n",
      "128\n",
      "Time elasped: 0.32376694679260254\n",
      "Mean Error: 0.0003596926399040967% \n",
      "--------------------------\n",
      "Epoch [1106/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04294\n",
      "128\n",
      "Time elasped: 0.326796293258667\n",
      "Epoch [1107/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04351\n",
      "128\n",
      "Time elasped: 0.31746768951416016\n",
      "Epoch [1108/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04599\n",
      "128\n",
      "Time elasped: 0.30995798110961914\n",
      "Epoch [1109/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04376\n",
      "128\n",
      "Time elasped: 0.3185422420501709\n",
      "Epoch [1110/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04134\n",
      "128\n",
      "Time elasped: 0.32140183448791504\n",
      "Mean Error: 0.00034601803054101765% \n",
      "--------------------------\n",
      "Epoch [1111/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04372\n",
      "128\n",
      "Time elasped: 0.317349910736084\n",
      "Epoch [1112/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04520\n",
      "128\n",
      "Time elasped: 0.3319203853607178\n",
      "Epoch [1113/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04098\n",
      "128\n",
      "Time elasped: 0.31839418411254883\n",
      "Epoch [1114/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04426\n",
      "128\n",
      "Time elasped: 0.31972718238830566\n",
      "Epoch [1115/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04694\n",
      "128\n",
      "Time elasped: 0.3117976188659668\n",
      "Mean Error: 0.0003164348308928311% \n",
      "--------------------------\n",
      "Epoch [1116/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04441\n",
      "128\n",
      "Time elasped: 0.33884191513061523\n",
      "Epoch [1117/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04624\n",
      "128\n",
      "Time elasped: 0.31571459770202637\n",
      "Epoch [1118/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04726\n",
      "128\n",
      "Time elasped: 0.3298454284667969\n",
      "Epoch [1119/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04384\n",
      "128\n",
      "Time elasped: 0.32538509368896484\n",
      "Epoch [1120/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04214\n",
      "128\n",
      "Time elasped: 0.3383049964904785\n",
      "Mean Error: 0.0003609879349824041% \n",
      "--------------------------\n",
      "Epoch [1121/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04702\n",
      "128\n",
      "Time elasped: 0.3211197853088379\n",
      "Epoch [1122/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04452\n",
      "128\n",
      "Time elasped: 0.3101787567138672\n",
      "Epoch [1123/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04607\n",
      "128\n",
      "Time elasped: 0.32578420639038086\n",
      "Epoch [1124/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04513\n",
      "128\n",
      "Time elasped: 0.33538293838500977\n",
      "Epoch [1125/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04491\n",
      "128\n",
      "Time elasped: 0.33911895751953125\n",
      "Mean Error: 0.0003275161434430629% \n",
      "--------------------------\n",
      "Epoch [1126/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04563\n",
      "128\n",
      "Time elasped: 0.3150160312652588\n",
      "Epoch [1127/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04393\n",
      "128\n",
      "Time elasped: 0.33861422538757324\n",
      "Epoch [1128/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04753\n",
      "128\n",
      "Time elasped: 0.3233182430267334\n",
      "Epoch [1129/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04411\n",
      "128\n",
      "Time elasped: 0.3203587532043457\n",
      "Epoch [1130/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04673\n",
      "128\n",
      "Time elasped: 0.3329756259918213\n",
      "Mean Error: 0.0003588411782402545% \n",
      "--------------------------\n",
      "Epoch [1131/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04560\n",
      "128\n",
      "Time elasped: 0.33345556259155273\n",
      "Epoch [1132/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04453\n",
      "128\n",
      "Time elasped: 0.3166539669036865\n",
      "Epoch [1133/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04674\n",
      "128\n",
      "Time elasped: 0.32627344131469727\n",
      "Epoch [1134/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04814\n",
      "128\n",
      "Time elasped: 0.34000253677368164\n",
      "Epoch [1135/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04511\n",
      "128\n",
      "Time elasped: 0.3811624050140381\n",
      "Mean Error: 0.00036729173734784126% \n",
      "--------------------------\n",
      "Epoch [1136/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04558\n",
      "128\n",
      "Time elasped: 0.31692075729370117\n",
      "Epoch [1137/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04796\n",
      "128\n",
      "Time elasped: 0.3171718120574951\n",
      "Epoch [1138/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04146\n",
      "128\n",
      "Time elasped: 0.33289194107055664\n",
      "Epoch [1139/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04449\n",
      "128\n",
      "Time elasped: 0.3202989101409912\n",
      "Epoch [1140/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04332\n",
      "128\n",
      "Time elasped: 0.31581807136535645\n",
      "Mean Error: 0.0003455873520579189% \n",
      "--------------------------\n",
      "Epoch [1141/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04298\n",
      "128\n",
      "Time elasped: 0.3334498405456543\n",
      "Epoch [1142/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04621\n",
      "128\n",
      "Time elasped: 0.3065807819366455\n",
      "Epoch [1143/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04586\n",
      "128\n",
      "Time elasped: 0.3365905284881592\n",
      "Epoch [1144/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04482\n",
      "128\n",
      "Time elasped: 0.3331143856048584\n",
      "Epoch [1145/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04777\n",
      "128\n",
      "Time elasped: 0.31818199157714844\n",
      "Mean Error: 0.0003536902950145304% \n",
      "--------------------------\n",
      "Epoch [1146/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04137\n",
      "128\n",
      "Time elasped: 0.31017041206359863\n",
      "Epoch [1147/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04354\n",
      "128\n",
      "Time elasped: 0.310788631439209\n",
      "Epoch [1148/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04617\n",
      "128\n",
      "Time elasped: 0.3205544948577881\n",
      "Epoch [1149/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04274\n",
      "128\n",
      "Time elasped: 0.32882261276245117\n",
      "Epoch [1150/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04447\n",
      "128\n",
      "Time elasped: 0.327716588973999\n",
      "Mean Error: 0.0003406937757972628% \n",
      "--------------------------\n",
      "Epoch [1151/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04415\n",
      "128\n",
      "Time elasped: 0.3168048858642578\n",
      "Epoch [1152/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04174\n",
      "128\n",
      "Time elasped: 0.3256864547729492\n",
      "Epoch [1153/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04376\n",
      "128\n",
      "Time elasped: 0.32019567489624023\n",
      "Epoch [1154/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04645\n",
      "128\n",
      "Time elasped: 0.3251063823699951\n",
      "Epoch [1155/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04406\n",
      "128\n",
      "Time elasped: 0.3347594738006592\n",
      "Mean Error: 0.00036238128086552024% \n",
      "--------------------------\n",
      "Epoch [1156/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04285\n",
      "128\n",
      "Time elasped: 0.33838844299316406\n",
      "Epoch [1157/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04459\n",
      "128\n",
      "Time elasped: 0.3522152900695801\n",
      "Epoch [1158/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04262\n",
      "128\n",
      "Time elasped: 0.33734989166259766\n",
      "Epoch [1159/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04605\n",
      "128\n",
      "Time elasped: 0.3335108757019043\n",
      "Epoch [1160/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04852\n",
      "128\n",
      "Time elasped: 0.33282923698425293\n",
      "Mean Error: 0.0003533720155246556% \n",
      "--------------------------\n",
      "Epoch [1161/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04708\n",
      "128\n",
      "Time elasped: 0.3950371742248535\n",
      "Epoch [1162/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04659\n",
      "128\n",
      "Time elasped: 0.38590383529663086\n",
      "Epoch [1163/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04654\n",
      "128\n",
      "Time elasped: 0.31697869300842285\n",
      "Epoch [1164/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04439\n",
      "128\n",
      "Time elasped: 0.3241398334503174\n",
      "Epoch [1165/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04084\n",
      "128\n",
      "Time elasped: 0.3509023189544678\n",
      "Mean Error: 0.00033569513470865786% \n",
      "--------------------------\n",
      "Epoch [1166/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04283\n",
      "128\n",
      "Time elasped: 0.36194348335266113\n",
      "Epoch [1167/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04662\n",
      "128\n",
      "Time elasped: 0.35164856910705566\n",
      "Epoch [1168/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04457\n",
      "128\n",
      "Time elasped: 0.34606194496154785\n",
      "Epoch [1169/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04202\n",
      "128\n",
      "Time elasped: 0.4052712917327881\n",
      "Epoch [1170/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04659\n",
      "128\n",
      "Time elasped: 0.3630666732788086\n",
      "Mean Error: 0.00034033958218060434% \n",
      "--------------------------\n",
      "Epoch [1171/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04676\n",
      "128\n",
      "Time elasped: 0.3417932987213135\n",
      "Epoch [1172/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04565\n",
      "128\n",
      "Time elasped: 0.33474278450012207\n",
      "Epoch [1173/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04167\n",
      "128\n",
      "Time elasped: 0.4444539546966553\n",
      "Epoch [1174/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04510\n",
      "128\n",
      "Time elasped: 0.37525439262390137\n",
      "Epoch [1175/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04294\n",
      "128\n",
      "Time elasped: 0.3708491325378418\n",
      "Mean Error: 0.00033340667141601443% \n",
      "--------------------------\n",
      "Epoch [1176/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04503\n",
      "128\n",
      "Time elasped: 0.3694274425506592\n",
      "Epoch [1177/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04265\n",
      "128\n",
      "Time elasped: 0.5231008529663086\n",
      "Epoch [1178/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04245\n",
      "128\n",
      "Time elasped: 0.38992738723754883\n",
      "Epoch [1179/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04300\n",
      "128\n",
      "Time elasped: 0.41682887077331543\n",
      "Epoch [1180/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04303\n",
      "128\n",
      "Time elasped: 0.3766460418701172\n",
      "Mean Error: 0.0003496561839710921% \n",
      "--------------------------\n",
      "Epoch [1181/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04425\n",
      "128\n",
      "Time elasped: 0.3721301555633545\n",
      "Epoch [1182/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04406\n",
      "128\n",
      "Time elasped: 0.3905184268951416\n",
      "Epoch [1183/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04465\n",
      "128\n",
      "Time elasped: 0.4131457805633545\n",
      "Epoch [1184/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.03854\n",
      "128\n",
      "Time elasped: 0.35466957092285156\n",
      "Epoch [1185/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04359\n",
      "128\n",
      "Time elasped: 0.3748013973236084\n",
      "Mean Error: 0.00036019994877278805% \n",
      "--------------------------\n",
      "Epoch [1186/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04278\n",
      "128\n",
      "Time elasped: 0.35727620124816895\n",
      "Epoch [1187/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04550\n",
      "128\n",
      "Time elasped: 0.3384838104248047\n",
      "Epoch [1188/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04600\n",
      "128\n",
      "Time elasped: 0.3385164737701416\n",
      "Epoch [1189/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04337\n",
      "128\n",
      "Time elasped: 0.37538981437683105\n",
      "Epoch [1190/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04455\n",
      "128\n",
      "Time elasped: 0.351468563079834\n",
      "Mean Error: 0.00036656210431829095% \n",
      "--------------------------\n",
      "Epoch [1191/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04719\n",
      "128\n",
      "Time elasped: 0.3898770809173584\n",
      "Epoch [1192/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04357\n",
      "128\n",
      "Time elasped: 0.36505842208862305\n",
      "Epoch [1193/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04966\n",
      "128\n",
      "Time elasped: 0.36895132064819336\n",
      "Epoch [1194/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04377\n",
      "128\n",
      "Time elasped: 0.3454616069793701\n",
      "Epoch [1195/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04384\n",
      "128\n",
      "Time elasped: 0.3324282169342041\n",
      "Mean Error: 0.0003274116897955537% \n",
      "--------------------------\n",
      "Epoch [1196/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04427\n",
      "128\n",
      "Time elasped: 0.3775815963745117\n",
      "Epoch [1197/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04860\n",
      "128\n",
      "Time elasped: 0.3546593189239502\n",
      "Epoch [1198/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04391\n",
      "128\n",
      "Time elasped: 0.3473358154296875\n",
      "Epoch [1199/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04480\n",
      "128\n",
      "Time elasped: 0.3421957492828369\n",
      "Epoch [1200/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04754\n",
      "128\n",
      "Time elasped: 0.37252020835876465\n",
      "Mean Error: 0.0003455087717156857% \n",
      "--------------------------\n",
      "Epoch [1201/3000], learning_rates 0.000569, 0.568800\n",
      "Step [1/1], Loss: 0.04711\n",
      "128\n",
      "Time elasped: 0.3823866844177246\n",
      "Epoch [1202/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04365\n",
      "128\n",
      "Time elasped: 0.4032552242279053\n",
      "Epoch [1203/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04176\n",
      "128\n",
      "Time elasped: 0.3345601558685303\n",
      "Epoch [1204/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04425\n",
      "128\n",
      "Time elasped: 0.34055066108703613\n",
      "Epoch [1205/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04111\n",
      "128\n",
      "Time elasped: 0.3486490249633789\n",
      "Mean Error: 0.0003305303689558059% \n",
      "--------------------------\n",
      "Epoch [1206/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04298\n",
      "128\n",
      "Time elasped: 0.34360694885253906\n",
      "Epoch [1207/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04358\n",
      "128\n",
      "Time elasped: 0.33876514434814453\n",
      "Epoch [1208/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04228\n",
      "128\n",
      "Time elasped: 0.3830282688140869\n",
      "Epoch [1209/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04419\n",
      "128\n",
      "Time elasped: 0.35016465187072754\n",
      "Epoch [1210/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04057\n",
      "128\n",
      "Time elasped: 0.3704378604888916\n",
      "Mean Error: 0.00032050826121121645% \n",
      "--------------------------\n",
      "Epoch [1211/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04229\n",
      "128\n",
      "Time elasped: 0.4204082489013672\n",
      "Epoch [1212/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04340\n",
      "128\n",
      "Time elasped: 0.36159515380859375\n",
      "Epoch [1213/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04525\n",
      "128\n",
      "Time elasped: 0.34047961235046387\n",
      "Epoch [1214/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04445\n",
      "128\n",
      "Time elasped: 0.3563995361328125\n",
      "Epoch [1215/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04940\n",
      "128\n",
      "Time elasped: 0.36342883110046387\n",
      "Mean Error: 0.00032317242585122585% \n",
      "--------------------------\n",
      "Epoch [1216/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04313\n",
      "128\n",
      "Time elasped: 0.36246824264526367\n",
      "Epoch [1217/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04474\n",
      "128\n",
      "Time elasped: 0.35884642601013184\n",
      "Epoch [1218/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04334\n",
      "128\n",
      "Time elasped: 0.3509254455566406\n",
      "Epoch [1219/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04399\n",
      "128\n",
      "Time elasped: 0.33641648292541504\n",
      "Epoch [1220/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04387\n",
      "128\n",
      "Time elasped: 0.3604104518890381\n",
      "Mean Error: 0.0003500967286527157% \n",
      "--------------------------\n",
      "Epoch [1221/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04406\n",
      "128\n",
      "Time elasped: 0.3357868194580078\n",
      "Epoch [1222/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04472\n",
      "128\n",
      "Time elasped: 0.3620460033416748\n",
      "Epoch [1223/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04204\n",
      "128\n",
      "Time elasped: 0.3542816638946533\n",
      "Epoch [1224/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04317\n",
      "128\n",
      "Time elasped: 0.34117889404296875\n",
      "Epoch [1225/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04461\n",
      "128\n",
      "Time elasped: 0.35246872901916504\n",
      "Mean Error: 0.0003361263661645353% \n",
      "--------------------------\n",
      "Epoch [1226/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04406\n",
      "128\n",
      "Time elasped: 0.34147191047668457\n",
      "Epoch [1227/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04416\n",
      "128\n",
      "Time elasped: 0.33458423614501953\n",
      "Epoch [1228/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04368\n",
      "128\n",
      "Time elasped: 0.35831570625305176\n",
      "Epoch [1229/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04599\n",
      "128\n",
      "Time elasped: 0.355884313583374\n",
      "Epoch [1230/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04243\n",
      "128\n",
      "Time elasped: 0.3554093837738037\n",
      "Mean Error: 0.000298753147944808% \n",
      "--------------------------\n",
      "Epoch [1231/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03967\n",
      "128\n",
      "Time elasped: 0.37183570861816406\n",
      "Epoch [1232/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04013\n",
      "128\n",
      "Time elasped: 0.3301277160644531\n",
      "Epoch [1233/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03822\n",
      "128\n",
      "Time elasped: 0.3746962547302246\n",
      "Epoch [1234/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04632\n",
      "128\n",
      "Time elasped: 0.36373329162597656\n",
      "Epoch [1235/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04336\n",
      "128\n",
      "Time elasped: 0.36369800567626953\n",
      "Mean Error: 0.0003395903913769871% \n",
      "--------------------------\n",
      "Epoch [1236/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04451\n",
      "128\n",
      "Time elasped: 0.35820627212524414\n",
      "Epoch [1237/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04431\n",
      "128\n",
      "Time elasped: 0.36286282539367676\n",
      "Epoch [1238/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04162\n",
      "128\n",
      "Time elasped: 0.33325624465942383\n",
      "Epoch [1239/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04302\n",
      "128\n",
      "Time elasped: 0.33850789070129395\n",
      "Epoch [1240/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04179\n",
      "128\n",
      "Time elasped: 0.34337663650512695\n",
      "Mean Error: 0.00035954482154920697% \n",
      "--------------------------\n",
      "Epoch [1241/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04377\n",
      "128\n",
      "Time elasped: 0.3279707431793213\n",
      "Epoch [1242/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04350\n",
      "128\n",
      "Time elasped: 0.3407449722290039\n",
      "Epoch [1243/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04279\n",
      "128\n",
      "Time elasped: 0.34714603424072266\n",
      "Epoch [1244/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04503\n",
      "128\n",
      "Time elasped: 0.3291585445404053\n",
      "Epoch [1245/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04801\n",
      "128\n",
      "Time elasped: 0.3360927104949951\n",
      "Mean Error: 0.000338105543050915% \n",
      "--------------------------\n",
      "Epoch [1246/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04202\n",
      "128\n",
      "Time elasped: 0.32662105560302734\n",
      "Epoch [1247/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04264\n",
      "128\n",
      "Time elasped: 0.3498694896697998\n",
      "Epoch [1248/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04018\n",
      "128\n",
      "Time elasped: 0.35642361640930176\n",
      "Epoch [1249/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04375\n",
      "128\n",
      "Time elasped: 0.34278130531311035\n",
      "Epoch [1250/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04243\n",
      "128\n",
      "Time elasped: 0.3372194766998291\n",
      "Mean Error: 0.00034208636498078704% \n",
      "--------------------------\n",
      "Epoch [1251/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04430\n",
      "128\n",
      "Time elasped: 0.3748021125793457\n",
      "Epoch [1252/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04260\n",
      "128\n",
      "Time elasped: 0.32994794845581055\n",
      "Epoch [1253/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04194\n",
      "128\n",
      "Time elasped: 0.3479783535003662\n",
      "Epoch [1254/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04379\n",
      "128\n",
      "Time elasped: 0.3555278778076172\n",
      "Epoch [1255/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04351\n",
      "128\n",
      "Time elasped: 0.34764671325683594\n",
      "Mean Error: 0.00036644874489866197% \n",
      "--------------------------\n",
      "Epoch [1256/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04401\n",
      "128\n",
      "Time elasped: 0.3309757709503174\n",
      "Epoch [1257/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04063\n",
      "128\n",
      "Time elasped: 0.343858003616333\n",
      "Epoch [1258/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04151\n",
      "128\n",
      "Time elasped: 0.3729062080383301\n",
      "Epoch [1259/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04429\n",
      "128\n",
      "Time elasped: 0.36514711380004883\n",
      "Epoch [1260/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04316\n",
      "128\n",
      "Time elasped: 0.41349291801452637\n",
      "Mean Error: 0.0003432028752285987% \n",
      "--------------------------\n",
      "Epoch [1261/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03892\n",
      "128\n",
      "Time elasped: 0.3832130432128906\n",
      "Epoch [1262/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04207\n",
      "128\n",
      "Time elasped: 0.36600685119628906\n",
      "Epoch [1263/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04395\n",
      "128\n",
      "Time elasped: 0.35117673873901367\n",
      "Epoch [1264/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04063\n",
      "128\n",
      "Time elasped: 0.3326599597930908\n",
      "Epoch [1265/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04115\n",
      "128\n",
      "Time elasped: 0.33765673637390137\n",
      "Mean Error: 0.00034097334719263017% \n",
      "--------------------------\n",
      "Epoch [1266/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04274\n",
      "128\n",
      "Time elasped: 0.3619654178619385\n",
      "Epoch [1267/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03996\n",
      "128\n",
      "Time elasped: 0.37021684646606445\n",
      "Epoch [1268/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04231\n",
      "128\n",
      "Time elasped: 0.3438761234283447\n",
      "Epoch [1269/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04298\n",
      "128\n",
      "Time elasped: 0.37684154510498047\n",
      "Epoch [1270/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04171\n",
      "128\n",
      "Time elasped: 0.3195934295654297\n",
      "Mean Error: 0.00034195237094536424% \n",
      "--------------------------\n",
      "Epoch [1271/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04462\n",
      "128\n",
      "Time elasped: 0.3411440849304199\n",
      "Epoch [1272/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03979\n",
      "128\n",
      "Time elasped: 0.3382251262664795\n",
      "Epoch [1273/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04000\n",
      "128\n",
      "Time elasped: 0.3637425899505615\n",
      "Epoch [1274/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03861\n",
      "128\n",
      "Time elasped: 0.3283536434173584\n",
      "Epoch [1275/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04666\n",
      "128\n",
      "Time elasped: 0.3316946029663086\n",
      "Mean Error: 0.000377257150830701% \n",
      "--------------------------\n",
      "Epoch [1276/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04528\n",
      "128\n",
      "Time elasped: 0.34456515312194824\n",
      "Epoch [1277/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04358\n",
      "128\n",
      "Time elasped: 0.3283107280731201\n",
      "Epoch [1278/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04275\n",
      "128\n",
      "Time elasped: 0.3534696102142334\n",
      "Epoch [1279/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03876\n",
      "128\n",
      "Time elasped: 0.3421785831451416\n",
      "Epoch [1280/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04091\n",
      "128\n",
      "Time elasped: 0.34589099884033203\n",
      "Mean Error: 0.0003518665907904506% \n",
      "--------------------------\n",
      "Epoch [1281/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04533\n",
      "128\n",
      "Time elasped: 0.3566112518310547\n",
      "Epoch [1282/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04117\n",
      "128\n",
      "Time elasped: 0.4078528881072998\n",
      "Epoch [1283/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04300\n",
      "128\n",
      "Time elasped: 0.39049458503723145\n",
      "Epoch [1284/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04270\n",
      "128\n",
      "Time elasped: 0.40372657775878906\n",
      "Epoch [1285/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04025\n",
      "128\n",
      "Time elasped: 0.39798426628112793\n",
      "Mean Error: 0.0003303566190879792% \n",
      "--------------------------\n",
      "Epoch [1286/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04432\n",
      "128\n",
      "Time elasped: 0.37035250663757324\n",
      "Epoch [1287/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04186\n",
      "128\n",
      "Time elasped: 0.3955237865447998\n",
      "Epoch [1288/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04127\n",
      "128\n",
      "Time elasped: 0.35502147674560547\n",
      "Epoch [1289/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04167\n",
      "128\n",
      "Time elasped: 0.3838627338409424\n",
      "Epoch [1290/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04419\n",
      "128\n",
      "Time elasped: 0.36674976348876953\n",
      "Mean Error: 0.00033131628879345953% \n",
      "--------------------------\n",
      "Epoch [1291/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04286\n",
      "128\n",
      "Time elasped: 0.3744525909423828\n",
      "Epoch [1292/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04009\n",
      "128\n",
      "Time elasped: 0.35132908821105957\n",
      "Epoch [1293/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04629\n",
      "128\n",
      "Time elasped: 0.33086585998535156\n",
      "Epoch [1294/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.03937\n",
      "128\n",
      "Time elasped: 0.34330058097839355\n",
      "Epoch [1295/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04231\n",
      "128\n",
      "Time elasped: 0.3385024070739746\n",
      "Mean Error: 0.00032519534579478204% \n",
      "--------------------------\n",
      "Epoch [1296/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04083\n",
      "128\n",
      "Time elasped: 0.36508989334106445\n",
      "Epoch [1297/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04231\n",
      "128\n",
      "Time elasped: 0.3496363162994385\n",
      "Epoch [1298/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04025\n",
      "128\n",
      "Time elasped: 0.3374314308166504\n",
      "Epoch [1299/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04215\n",
      "128\n",
      "Time elasped: 0.35375285148620605\n",
      "Epoch [1300/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04236\n",
      "128\n",
      "Time elasped: 0.34362196922302246\n",
      "Mean Error: 0.0003497130237519741% \n",
      "--------------------------\n",
      "Epoch [1301/3000], learning_rates 0.000540, 0.540360\n",
      "Step [1/1], Loss: 0.04189\n",
      "128\n",
      "Time elasped: 0.3615145683288574\n",
      "Epoch [1302/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04449\n",
      "128\n",
      "Time elasped: 0.34229540824890137\n",
      "Epoch [1303/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04272\n",
      "128\n",
      "Time elasped: 0.3581736087799072\n",
      "Epoch [1304/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04459\n",
      "128\n",
      "Time elasped: 0.34111928939819336\n",
      "Epoch [1305/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03942\n",
      "128\n",
      "Time elasped: 0.33968615531921387\n",
      "Mean Error: 0.0003621880314312875% \n",
      "--------------------------\n",
      "Epoch [1306/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04317\n",
      "128\n",
      "Time elasped: 0.3404722213745117\n",
      "Epoch [1307/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04396\n",
      "128\n",
      "Time elasped: 0.3429832458496094\n",
      "Epoch [1308/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03984\n",
      "128\n",
      "Time elasped: 0.36099982261657715\n",
      "Epoch [1309/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04247\n",
      "128\n",
      "Time elasped: 0.3415102958679199\n",
      "Epoch [1310/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03913\n",
      "128\n",
      "Time elasped: 0.32555580139160156\n",
      "Mean Error: 0.0002952836512122303% \n",
      "--------------------------\n",
      "Epoch [1311/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03873\n",
      "128\n",
      "Time elasped: 0.34341907501220703\n",
      "Epoch [1312/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04128\n",
      "128\n",
      "Time elasped: 0.3969120979309082\n",
      "Epoch [1313/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04559\n",
      "128\n",
      "Time elasped: 0.3419783115386963\n",
      "Epoch [1314/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04221\n",
      "128\n",
      "Time elasped: 0.3437623977661133\n",
      "Epoch [1315/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03898\n",
      "128\n",
      "Time elasped: 0.3442556858062744\n",
      "Mean Error: 0.0003299771051388234% \n",
      "--------------------------\n",
      "Epoch [1316/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04260\n",
      "128\n",
      "Time elasped: 0.3495197296142578\n",
      "Epoch [1317/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04049\n",
      "128\n",
      "Time elasped: 0.35379672050476074\n",
      "Epoch [1318/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04193\n",
      "128\n",
      "Time elasped: 0.3330082893371582\n",
      "Epoch [1319/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04347\n",
      "128\n",
      "Time elasped: 0.3666372299194336\n",
      "Epoch [1320/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04233\n",
      "128\n",
      "Time elasped: 0.3352510929107666\n",
      "Mean Error: 0.00033296950277872384% \n",
      "--------------------------\n",
      "Epoch [1321/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04449\n",
      "128\n",
      "Time elasped: 0.33504152297973633\n",
      "Epoch [1322/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04109\n",
      "128\n",
      "Time elasped: 0.33088040351867676\n",
      "Epoch [1323/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04352\n",
      "128\n",
      "Time elasped: 0.3266739845275879\n",
      "Epoch [1324/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04294\n",
      "128\n",
      "Time elasped: 0.38250732421875\n",
      "Epoch [1325/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04117\n",
      "128\n",
      "Time elasped: 0.3291780948638916\n",
      "Mean Error: 0.0003283092228230089% \n",
      "--------------------------\n",
      "Epoch [1326/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04110\n",
      "128\n",
      "Time elasped: 0.34223103523254395\n",
      "Epoch [1327/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04331\n",
      "128\n",
      "Time elasped: 0.35262084007263184\n",
      "Epoch [1328/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04152\n",
      "128\n",
      "Time elasped: 0.33069276809692383\n",
      "Epoch [1329/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04176\n",
      "128\n",
      "Time elasped: 0.3290228843688965\n",
      "Epoch [1330/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04253\n",
      "128\n",
      "Time elasped: 0.3362243175506592\n",
      "Mean Error: 0.0003205044777132571% \n",
      "--------------------------\n",
      "Epoch [1331/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04204\n",
      "128\n",
      "Time elasped: 0.3581376075744629\n",
      "Epoch [1332/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03952\n",
      "128\n",
      "Time elasped: 0.36337971687316895\n",
      "Epoch [1333/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04217\n",
      "128\n",
      "Time elasped: 0.38353919982910156\n",
      "Epoch [1334/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04287\n",
      "128\n",
      "Time elasped: 0.3534271717071533\n",
      "Epoch [1335/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04198\n",
      "128\n",
      "Time elasped: 0.3506767749786377\n",
      "Mean Error: 0.0003222835366614163% \n",
      "--------------------------\n",
      "Epoch [1336/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04549\n",
      "128\n",
      "Time elasped: 0.3328425884246826\n",
      "Epoch [1337/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03984\n",
      "128\n",
      "Time elasped: 0.3264951705932617\n",
      "Epoch [1338/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04454\n",
      "128\n",
      "Time elasped: 0.36978769302368164\n",
      "Epoch [1339/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04268\n",
      "128\n",
      "Time elasped: 0.3743727207183838\n",
      "Epoch [1340/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04181\n",
      "128\n",
      "Time elasped: 0.33053040504455566\n",
      "Mean Error: 0.0003155136073473841% \n",
      "--------------------------\n",
      "Epoch [1341/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04479\n",
      "128\n",
      "Time elasped: 0.36989259719848633\n",
      "Epoch [1342/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03974\n",
      "128\n",
      "Time elasped: 0.3424854278564453\n",
      "Epoch [1343/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04276\n",
      "128\n",
      "Time elasped: 0.36144137382507324\n",
      "Epoch [1344/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04029\n",
      "128\n",
      "Time elasped: 0.3410522937774658\n",
      "Epoch [1345/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04092\n",
      "128\n",
      "Time elasped: 0.34979963302612305\n",
      "Mean Error: 0.0003171763091813773% \n",
      "--------------------------\n",
      "Epoch [1346/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04264\n",
      "128\n",
      "Time elasped: 0.3287625312805176\n",
      "Epoch [1347/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04169\n",
      "128\n",
      "Time elasped: 0.3434743881225586\n",
      "Epoch [1348/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03935\n",
      "128\n",
      "Time elasped: 0.33542537689208984\n",
      "Epoch [1349/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04330\n",
      "128\n",
      "Time elasped: 0.3327333927154541\n",
      "Epoch [1350/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04120\n",
      "128\n",
      "Time elasped: 0.36479711532592773\n",
      "Mean Error: 0.00033215098665095866% \n",
      "--------------------------\n",
      "Epoch [1351/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04227\n",
      "128\n",
      "Time elasped: 0.32866930961608887\n",
      "Epoch [1352/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03814\n",
      "128\n",
      "Time elasped: 0.3670530319213867\n",
      "Epoch [1353/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04215\n",
      "128\n",
      "Time elasped: 0.3430626392364502\n",
      "Epoch [1354/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04140\n",
      "128\n",
      "Time elasped: 0.3353087902069092\n",
      "Epoch [1355/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03911\n",
      "128\n",
      "Time elasped: 0.3426988124847412\n",
      "Mean Error: 0.0003336730587761849% \n",
      "--------------------------\n",
      "Epoch [1356/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03985\n",
      "128\n",
      "Time elasped: 0.34246349334716797\n",
      "Epoch [1357/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04141\n",
      "128\n",
      "Time elasped: 0.33947062492370605\n",
      "Epoch [1358/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04300\n",
      "128\n",
      "Time elasped: 0.3506901264190674\n",
      "Epoch [1359/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04001\n",
      "128\n",
      "Time elasped: 0.3423953056335449\n",
      "Epoch [1360/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03930\n",
      "128\n",
      "Time elasped: 0.3511636257171631\n",
      "Mean Error: 0.00032947404542937875% \n",
      "--------------------------\n",
      "Epoch [1361/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03925\n",
      "128\n",
      "Time elasped: 0.34793829917907715\n",
      "Epoch [1362/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04295\n",
      "128\n",
      "Time elasped: 0.32371997833251953\n",
      "Epoch [1363/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04415\n",
      "128\n",
      "Time elasped: 0.33672595024108887\n",
      "Epoch [1364/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03813\n",
      "128\n",
      "Time elasped: 0.32802653312683105\n",
      "Epoch [1365/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04438\n",
      "128\n",
      "Time elasped: 0.33190083503723145\n",
      "Mean Error: 0.0003267274587415159% \n",
      "--------------------------\n",
      "Epoch [1366/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03996\n",
      "128\n",
      "Time elasped: 0.354231595993042\n",
      "Epoch [1367/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04195\n",
      "128\n",
      "Time elasped: 0.32981133460998535\n",
      "Epoch [1368/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03996\n",
      "128\n",
      "Time elasped: 0.371401309967041\n",
      "Epoch [1369/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04126\n",
      "128\n",
      "Time elasped: 0.34377408027648926\n",
      "Epoch [1370/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04030\n",
      "128\n",
      "Time elasped: 0.3326108455657959\n",
      "Mean Error: 0.00032178848050534725% \n",
      "--------------------------\n",
      "Epoch [1371/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04145\n",
      "128\n",
      "Time elasped: 0.3447294235229492\n",
      "Epoch [1372/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04360\n",
      "128\n",
      "Time elasped: 0.3409895896911621\n",
      "Epoch [1373/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04169\n",
      "128\n",
      "Time elasped: 0.32349610328674316\n",
      "Epoch [1374/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04438\n",
      "128\n",
      "Time elasped: 0.3431696891784668\n",
      "Epoch [1375/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04023\n",
      "128\n",
      "Time elasped: 0.3466815948486328\n",
      "Mean Error: 0.0003179199411533773% \n",
      "--------------------------\n",
      "Epoch [1376/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04216\n",
      "128\n",
      "Time elasped: 0.3546419143676758\n",
      "Epoch [1377/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03824\n",
      "128\n",
      "Time elasped: 0.33933186531066895\n",
      "Epoch [1378/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04176\n",
      "128\n",
      "Time elasped: 0.3409280776977539\n",
      "Epoch [1379/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04259\n",
      "128\n",
      "Time elasped: 0.3733482360839844\n",
      "Epoch [1380/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04416\n",
      "128\n",
      "Time elasped: 0.3478853702545166\n",
      "Mean Error: 0.0003613001899793744% \n",
      "--------------------------\n",
      "Epoch [1381/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04388\n",
      "128\n",
      "Time elasped: 0.34736204147338867\n",
      "Epoch [1382/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04067\n",
      "128\n",
      "Time elasped: 0.3511080741882324\n",
      "Epoch [1383/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04211\n",
      "128\n",
      "Time elasped: 0.34059643745422363\n",
      "Epoch [1384/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04280\n",
      "128\n",
      "Time elasped: 0.3592808246612549\n",
      "Epoch [1385/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04040\n",
      "128\n",
      "Time elasped: 0.34951066970825195\n",
      "Mean Error: 0.0003237227210775018% \n",
      "--------------------------\n",
      "Epoch [1386/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04117\n",
      "128\n",
      "Time elasped: 0.47536230087280273\n",
      "Epoch [1387/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04297\n",
      "128\n",
      "Time elasped: 0.3708186149597168\n",
      "Epoch [1388/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04160\n",
      "128\n",
      "Time elasped: 0.339235782623291\n",
      "Epoch [1389/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04137\n",
      "128\n",
      "Time elasped: 0.35296177864074707\n",
      "Epoch [1390/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04300\n",
      "128\n",
      "Time elasped: 0.33814120292663574\n",
      "Mean Error: 0.0003003779274877161% \n",
      "--------------------------\n",
      "Epoch [1391/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03996\n",
      "128\n",
      "Time elasped: 0.3780524730682373\n",
      "Epoch [1392/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04139\n",
      "128\n",
      "Time elasped: 0.35961031913757324\n",
      "Epoch [1393/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03826\n",
      "128\n",
      "Time elasped: 0.37498021125793457\n",
      "Epoch [1394/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04234\n",
      "128\n",
      "Time elasped: 0.3393993377685547\n",
      "Epoch [1395/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.03749\n",
      "128\n",
      "Time elasped: 0.3711390495300293\n",
      "Mean Error: 0.0003101894981227815% \n",
      "--------------------------\n",
      "Epoch [1396/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04169\n",
      "128\n",
      "Time elasped: 0.3873136043548584\n",
      "Epoch [1397/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04164\n",
      "128\n",
      "Time elasped: 0.3640940189361572\n",
      "Epoch [1398/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04001\n",
      "128\n",
      "Time elasped: 0.34075355529785156\n",
      "Epoch [1399/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04296\n",
      "128\n",
      "Time elasped: 0.3734743595123291\n",
      "Epoch [1400/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04016\n",
      "128\n",
      "Time elasped: 0.34659767150878906\n",
      "Mean Error: 0.0003281217650510371% \n",
      "--------------------------\n",
      "Epoch [1401/3000], learning_rates 0.000513, 0.513342\n",
      "Step [1/1], Loss: 0.04060\n",
      "128\n",
      "Time elasped: 0.3951585292816162\n",
      "Epoch [1402/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04185\n",
      "128\n",
      "Time elasped: 0.3811459541320801\n",
      "Epoch [1403/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04040\n",
      "128\n",
      "Time elasped: 0.33391642570495605\n",
      "Epoch [1404/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04091\n",
      "128\n",
      "Time elasped: 0.3517496585845947\n",
      "Epoch [1405/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04108\n",
      "128\n",
      "Time elasped: 0.34742283821105957\n",
      "Mean Error: 0.0003266911080572754% \n",
      "--------------------------\n",
      "Epoch [1406/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04269\n",
      "128\n",
      "Time elasped: 0.3583812713623047\n",
      "Epoch [1407/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04040\n",
      "128\n",
      "Time elasped: 0.36018872261047363\n",
      "Epoch [1408/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04253\n",
      "128\n",
      "Time elasped: 0.3328893184661865\n",
      "Epoch [1409/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03528\n",
      "128\n",
      "Time elasped: 0.3303699493408203\n",
      "Epoch [1410/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03845\n",
      "128\n",
      "Time elasped: 0.3584146499633789\n",
      "Mean Error: 0.000294841593131423% \n",
      "--------------------------\n",
      "Epoch [1411/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04188\n",
      "128\n",
      "Time elasped: 0.3477020263671875\n",
      "Epoch [1412/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04049\n",
      "128\n",
      "Time elasped: 0.332294225692749\n",
      "Epoch [1413/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03837\n",
      "128\n",
      "Time elasped: 0.3526430130004883\n",
      "Epoch [1414/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03821\n",
      "128\n",
      "Time elasped: 0.3360459804534912\n",
      "Epoch [1415/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04256\n",
      "128\n",
      "Time elasped: 0.3324930667877197\n",
      "Mean Error: 0.0003360237169545144% \n",
      "--------------------------\n",
      "Epoch [1416/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04134\n",
      "128\n",
      "Time elasped: 0.33765482902526855\n",
      "Epoch [1417/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03973\n",
      "128\n",
      "Time elasped: 0.34232449531555176\n",
      "Epoch [1418/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04187\n",
      "128\n",
      "Time elasped: 0.3413081169128418\n",
      "Epoch [1419/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03967\n",
      "128\n",
      "Time elasped: 0.33642148971557617\n",
      "Epoch [1420/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04032\n",
      "128\n",
      "Time elasped: 0.33191466331481934\n",
      "Mean Error: 0.0002985491300933063% \n",
      "--------------------------\n",
      "Epoch [1421/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04240\n",
      "128\n",
      "Time elasped: 0.33695411682128906\n",
      "Epoch [1422/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04099\n",
      "128\n",
      "Time elasped: 0.3459150791168213\n",
      "Epoch [1423/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04009\n",
      "128\n",
      "Time elasped: 0.34273457527160645\n",
      "Epoch [1424/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04164\n",
      "128\n",
      "Time elasped: 0.3221421241760254\n",
      "Epoch [1425/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04104\n",
      "128\n",
      "Time elasped: 0.3369104862213135\n",
      "Mean Error: 0.0003016520931851119% \n",
      "--------------------------\n",
      "Epoch [1426/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04189\n",
      "128\n",
      "Time elasped: 0.35540771484375\n",
      "Epoch [1427/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04319\n",
      "128\n",
      "Time elasped: 0.3352491855621338\n",
      "Epoch [1428/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03793\n",
      "128\n",
      "Time elasped: 0.33829760551452637\n",
      "Epoch [1429/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04113\n",
      "128\n",
      "Time elasped: 0.33413028717041016\n",
      "Epoch [1430/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03903\n",
      "128\n",
      "Time elasped: 0.35430479049682617\n",
      "Mean Error: 0.0003239836369175464% \n",
      "--------------------------\n",
      "Epoch [1431/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04187\n",
      "128\n",
      "Time elasped: 0.34360551834106445\n",
      "Epoch [1432/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03535\n",
      "128\n",
      "Time elasped: 0.32352256774902344\n",
      "Epoch [1433/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03469\n",
      "128\n",
      "Time elasped: 0.32674169540405273\n",
      "Epoch [1434/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04376\n",
      "128\n",
      "Time elasped: 0.3206639289855957\n",
      "Epoch [1435/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03791\n",
      "128\n",
      "Time elasped: 0.3629300594329834\n",
      "Mean Error: 0.00030721313669346273% \n",
      "--------------------------\n",
      "Epoch [1436/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03834\n",
      "128\n",
      "Time elasped: 0.33928823471069336\n",
      "Epoch [1437/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03839\n",
      "128\n",
      "Time elasped: 0.33650732040405273\n",
      "Epoch [1438/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04226\n",
      "128\n",
      "Time elasped: 0.3696920871734619\n",
      "Epoch [1439/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04110\n",
      "128\n",
      "Time elasped: 0.3536956310272217\n",
      "Epoch [1440/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03700\n",
      "128\n",
      "Time elasped: 0.3893158435821533\n",
      "Mean Error: 0.00031980930361896753% \n",
      "--------------------------\n",
      "Epoch [1441/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04275\n",
      "128\n",
      "Time elasped: 0.34739041328430176\n",
      "Epoch [1442/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03836\n",
      "128\n",
      "Time elasped: 0.3423805236816406\n",
      "Epoch [1443/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04009\n",
      "128\n",
      "Time elasped: 0.38436150550842285\n",
      "Epoch [1444/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04256\n",
      "128\n",
      "Time elasped: 0.3323500156402588\n",
      "Epoch [1445/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03706\n",
      "128\n",
      "Time elasped: 0.34129858016967773\n",
      "Mean Error: 0.0003260216035414487% \n",
      "--------------------------\n",
      "Epoch [1446/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04268\n",
      "128\n",
      "Time elasped: 0.3718903064727783\n",
      "Epoch [1447/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04002\n",
      "128\n",
      "Time elasped: 0.3624403476715088\n",
      "Epoch [1448/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04296\n",
      "128\n",
      "Time elasped: 0.33901047706604004\n",
      "Epoch [1449/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03999\n",
      "128\n",
      "Time elasped: 0.35327649116516113\n",
      "Epoch [1450/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04080\n",
      "128\n",
      "Time elasped: 0.3537712097167969\n",
      "Mean Error: 0.00031443306943401694% \n",
      "--------------------------\n",
      "Epoch [1451/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04326\n",
      "128\n",
      "Time elasped: 0.3955659866333008\n",
      "Epoch [1452/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04192\n",
      "128\n",
      "Time elasped: 0.36667871475219727\n",
      "Epoch [1453/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04116\n",
      "128\n",
      "Time elasped: 0.3505885601043701\n",
      "Epoch [1454/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04206\n",
      "128\n",
      "Time elasped: 0.37728285789489746\n",
      "Epoch [1455/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03898\n",
      "128\n",
      "Time elasped: 0.32976603507995605\n",
      "Mean Error: 0.0003005180333275348% \n",
      "--------------------------\n",
      "Epoch [1456/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03961\n",
      "128\n",
      "Time elasped: 0.3474576473236084\n",
      "Epoch [1457/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04232\n",
      "128\n",
      "Time elasped: 0.3462331295013428\n",
      "Epoch [1458/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04174\n",
      "128\n",
      "Time elasped: 0.33063268661499023\n",
      "Epoch [1459/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03870\n",
      "128\n",
      "Time elasped: 0.37404966354370117\n",
      "Epoch [1460/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03786\n",
      "128\n",
      "Time elasped: 0.3294081687927246\n",
      "Mean Error: 0.00030125584453344345% \n",
      "--------------------------\n",
      "Epoch [1461/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04169\n",
      "128\n",
      "Time elasped: 0.35472631454467773\n",
      "Epoch [1462/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03867\n",
      "128\n",
      "Time elasped: 0.3619682788848877\n",
      "Epoch [1463/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03978\n",
      "128\n",
      "Time elasped: 0.34796953201293945\n",
      "Epoch [1464/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04211\n",
      "128\n",
      "Time elasped: 0.3399374485015869\n",
      "Epoch [1465/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03830\n",
      "128\n",
      "Time elasped: 0.3345603942871094\n",
      "Mean Error: 0.00029184180311858654% \n",
      "--------------------------\n",
      "Epoch [1466/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04365\n",
      "128\n",
      "Time elasped: 0.31937456130981445\n",
      "Epoch [1467/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03855\n",
      "128\n",
      "Time elasped: 0.3465256690979004\n",
      "Epoch [1468/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04012\n",
      "128\n",
      "Time elasped: 0.3201322555541992\n",
      "Epoch [1469/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03884\n",
      "128\n",
      "Time elasped: 0.3328421115875244\n",
      "Epoch [1470/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03847\n",
      "128\n",
      "Time elasped: 0.3222837448120117\n",
      "Mean Error: 0.00032971458858810365% \n",
      "--------------------------\n",
      "Epoch [1471/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03768\n",
      "128\n",
      "Time elasped: 0.33048057556152344\n",
      "Epoch [1472/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04136\n",
      "128\n",
      "Time elasped: 0.3322172164916992\n",
      "Epoch [1473/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04418\n",
      "128\n",
      "Time elasped: 0.3320283889770508\n",
      "Epoch [1474/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04039\n",
      "128\n",
      "Time elasped: 0.3368062973022461\n",
      "Epoch [1475/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04256\n",
      "128\n",
      "Time elasped: 0.35517406463623047\n",
      "Mean Error: 0.00030638082535006106% \n",
      "--------------------------\n",
      "Epoch [1476/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04182\n",
      "128\n",
      "Time elasped: 0.3163187503814697\n",
      "Epoch [1477/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04191\n",
      "128\n",
      "Time elasped: 0.3317716121673584\n",
      "Epoch [1478/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04089\n",
      "128\n",
      "Time elasped: 0.32633209228515625\n",
      "Epoch [1479/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04219\n",
      "128\n",
      "Time elasped: 0.3517894744873047\n",
      "Epoch [1480/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04149\n",
      "128\n",
      "Time elasped: 0.3157382011413574\n",
      "Mean Error: 0.0003247505228500813% \n",
      "--------------------------\n",
      "Epoch [1481/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04067\n",
      "128\n",
      "Time elasped: 0.32927799224853516\n",
      "Epoch [1482/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03658\n",
      "128\n",
      "Time elasped: 0.36682963371276855\n",
      "Epoch [1483/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03775\n",
      "128\n",
      "Time elasped: 0.33650636672973633\n",
      "Epoch [1484/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04118\n",
      "128\n",
      "Time elasped: 0.3436393737792969\n",
      "Epoch [1485/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04241\n",
      "128\n",
      "Time elasped: 0.3363938331604004\n",
      "Mean Error: 0.00031136928009800613% \n",
      "--------------------------\n",
      "Epoch [1486/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04074\n",
      "128\n",
      "Time elasped: 0.34987592697143555\n",
      "Epoch [1487/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03759\n",
      "128\n",
      "Time elasped: 0.3279423713684082\n",
      "Epoch [1488/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04088\n",
      "128\n",
      "Time elasped: 0.3356597423553467\n",
      "Epoch [1489/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04169\n",
      "128\n",
      "Time elasped: 0.3526022434234619\n",
      "Epoch [1490/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04324\n",
      "128\n",
      "Time elasped: 0.33898472785949707\n",
      "Mean Error: 0.00035218201810494065% \n",
      "--------------------------\n",
      "Epoch [1491/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04202\n",
      "128\n",
      "Time elasped: 0.3201451301574707\n",
      "Epoch [1492/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03938\n",
      "128\n",
      "Time elasped: 0.33483338356018066\n",
      "Epoch [1493/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04390\n",
      "128\n",
      "Time elasped: 0.33800363540649414\n",
      "Epoch [1494/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04063\n",
      "128\n",
      "Time elasped: 0.33261895179748535\n",
      "Epoch [1495/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03791\n",
      "128\n",
      "Time elasped: 0.34116697311401367\n",
      "Mean Error: 0.0003154501609969884% \n",
      "--------------------------\n",
      "Epoch [1496/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04421\n",
      "128\n",
      "Time elasped: 0.3054921627044678\n",
      "Epoch [1497/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03815\n",
      "128\n",
      "Time elasped: 0.3250913619995117\n",
      "Epoch [1498/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04090\n",
      "128\n",
      "Time elasped: 0.3152594566345215\n",
      "Epoch [1499/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03986\n",
      "128\n",
      "Time elasped: 0.34063291549682617\n",
      "Epoch [1500/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.04067\n",
      "128\n",
      "Time elasped: 0.3533146381378174\n",
      "Mean Error: 0.0002883730339817703% \n",
      "--------------------------\n",
      "Epoch [1501/3000], learning_rates 0.000488, 0.487675\n",
      "Step [1/1], Loss: 0.03864\n",
      "128\n",
      "Time elasped: 0.35524725914001465\n",
      "Epoch [1502/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04253\n",
      "128\n",
      "Time elasped: 0.336728572845459\n",
      "Epoch [1503/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03782\n",
      "128\n",
      "Time elasped: 0.33359789848327637\n",
      "Epoch [1504/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04060\n",
      "128\n",
      "Time elasped: 0.33156824111938477\n",
      "Epoch [1505/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03706\n",
      "128\n",
      "Time elasped: 0.33884406089782715\n",
      "Mean Error: 0.0003214371099602431% \n",
      "--------------------------\n",
      "Epoch [1506/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03698\n",
      "128\n",
      "Time elasped: 0.32980799674987793\n",
      "Epoch [1507/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04109\n",
      "128\n",
      "Time elasped: 0.34433746337890625\n",
      "Epoch [1508/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04026\n",
      "128\n",
      "Time elasped: 0.31375622749328613\n",
      "Epoch [1509/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03905\n",
      "128\n",
      "Time elasped: 0.32212185859680176\n",
      "Epoch [1510/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04013\n",
      "128\n",
      "Time elasped: 0.32279086112976074\n",
      "Mean Error: 0.0003004830505233258% \n",
      "--------------------------\n",
      "Epoch [1511/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04035\n",
      "128\n",
      "Time elasped: 0.31652235984802246\n",
      "Epoch [1512/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03780\n",
      "128\n",
      "Time elasped: 0.3506903648376465\n",
      "Epoch [1513/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03959\n",
      "128\n",
      "Time elasped: 0.3458540439605713\n",
      "Epoch [1514/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04225\n",
      "128\n",
      "Time elasped: 0.32867431640625\n",
      "Epoch [1515/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03793\n",
      "128\n",
      "Time elasped: 0.3252091407775879\n",
      "Mean Error: 0.0002953030925709754% \n",
      "--------------------------\n",
      "Epoch [1516/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03936\n",
      "128\n",
      "Time elasped: 0.3464500904083252\n",
      "Epoch [1517/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04134\n",
      "128\n",
      "Time elasped: 0.3108537197113037\n",
      "Epoch [1518/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03911\n",
      "128\n",
      "Time elasped: 0.3326137065887451\n",
      "Epoch [1519/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04320\n",
      "128\n",
      "Time elasped: 0.3277010917663574\n",
      "Epoch [1520/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03983\n",
      "128\n",
      "Time elasped: 0.34267497062683105\n",
      "Mean Error: 0.0003114037390332669% \n",
      "--------------------------\n",
      "Epoch [1521/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03768\n",
      "128\n",
      "Time elasped: 0.3226618766784668\n",
      "Epoch [1522/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03936\n",
      "128\n",
      "Time elasped: 0.33239126205444336\n",
      "Epoch [1523/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03923\n",
      "128\n",
      "Time elasped: 0.32673072814941406\n",
      "Epoch [1524/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03511\n",
      "128\n",
      "Time elasped: 0.3277719020843506\n",
      "Epoch [1525/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03615\n",
      "128\n",
      "Time elasped: 0.32053542137145996\n",
      "Mean Error: 0.00029675877885892987% \n",
      "--------------------------\n",
      "Epoch [1526/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03948\n",
      "128\n",
      "Time elasped: 0.33366918563842773\n",
      "Epoch [1527/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04134\n",
      "128\n",
      "Time elasped: 0.32301998138427734\n",
      "Epoch [1528/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03982\n",
      "128\n",
      "Time elasped: 0.344346284866333\n",
      "Epoch [1529/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03787\n",
      "128\n",
      "Time elasped: 0.3242604732513428\n",
      "Epoch [1530/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03903\n",
      "128\n",
      "Time elasped: 0.34203362464904785\n",
      "Mean Error: 0.0003226594999432564% \n",
      "--------------------------\n",
      "Epoch [1531/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03901\n",
      "128\n",
      "Time elasped: 0.3319072723388672\n",
      "Epoch [1532/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03981\n",
      "128\n",
      "Time elasped: 0.34064698219299316\n",
      "Epoch [1533/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04003\n",
      "128\n",
      "Time elasped: 0.3345630168914795\n",
      "Epoch [1534/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04251\n",
      "128\n",
      "Time elasped: 0.3351578712463379\n",
      "Epoch [1535/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03957\n",
      "128\n",
      "Time elasped: 0.32900524139404297\n",
      "Mean Error: 0.00027645821683108807% \n",
      "--------------------------\n",
      "Epoch [1536/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03962\n",
      "128\n",
      "Time elasped: 0.3559236526489258\n",
      "Epoch [1537/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03791\n",
      "128\n",
      "Time elasped: 0.34987950325012207\n",
      "Epoch [1538/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03781\n",
      "128\n",
      "Time elasped: 0.3555128574371338\n",
      "Epoch [1539/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03778\n",
      "128\n",
      "Time elasped: 0.33585667610168457\n",
      "Epoch [1540/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04135\n",
      "128\n",
      "Time elasped: 0.34378910064697266\n",
      "Mean Error: 0.00031077463063411415% \n",
      "--------------------------\n",
      "Epoch [1541/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04174\n",
      "128\n",
      "Time elasped: 0.33400845527648926\n",
      "Epoch [1542/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04053\n",
      "128\n",
      "Time elasped: 0.3467693328857422\n",
      "Epoch [1543/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04025\n",
      "128\n",
      "Time elasped: 0.31333160400390625\n",
      "Epoch [1544/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03902\n",
      "128\n",
      "Time elasped: 0.32120394706726074\n",
      "Epoch [1545/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03913\n",
      "128\n",
      "Time elasped: 0.33626270294189453\n",
      "Mean Error: 0.0003086524666287005% \n",
      "--------------------------\n",
      "Epoch [1546/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03969\n",
      "128\n",
      "Time elasped: 0.32299184799194336\n",
      "Epoch [1547/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04040\n",
      "128\n",
      "Time elasped: 0.33731746673583984\n",
      "Epoch [1548/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03905\n",
      "128\n",
      "Time elasped: 0.3297905921936035\n",
      "Epoch [1549/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04088\n",
      "128\n",
      "Time elasped: 0.35205817222595215\n",
      "Epoch [1550/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03929\n",
      "128\n",
      "Time elasped: 0.3381154537200928\n",
      "Mean Error: 0.00031313320505432785% \n",
      "--------------------------\n",
      "Epoch [1551/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04157\n",
      "128\n",
      "Time elasped: 0.3629121780395508\n",
      "Epoch [1552/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03783\n",
      "128\n",
      "Time elasped: 0.3191204071044922\n",
      "Epoch [1553/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04364\n",
      "128\n",
      "Time elasped: 0.3278696537017822\n",
      "Epoch [1554/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03845\n",
      "128\n",
      "Time elasped: 0.33997488021850586\n",
      "Epoch [1555/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04021\n",
      "128\n",
      "Time elasped: 0.35263991355895996\n",
      "Mean Error: 0.0003182905784342438% \n",
      "--------------------------\n",
      "Epoch [1556/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03858\n",
      "128\n",
      "Time elasped: 0.3270576000213623\n",
      "Epoch [1557/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04293\n",
      "128\n",
      "Time elasped: 0.3426516056060791\n",
      "Epoch [1558/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03792\n",
      "128\n",
      "Time elasped: 0.33563876152038574\n",
      "Epoch [1559/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03850\n",
      "128\n",
      "Time elasped: 0.3184080123901367\n",
      "Epoch [1560/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04173\n",
      "128\n",
      "Time elasped: 0.3438718318939209\n",
      "Mean Error: 0.0002954969531856477% \n",
      "--------------------------\n",
      "Epoch [1561/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03492\n",
      "128\n",
      "Time elasped: 0.3287825584411621\n",
      "Epoch [1562/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03911\n",
      "128\n",
      "Time elasped: 0.34995174407958984\n",
      "Epoch [1563/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03920\n",
      "128\n",
      "Time elasped: 0.3284492492675781\n",
      "Epoch [1564/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04000\n",
      "128\n",
      "Time elasped: 0.32956433296203613\n",
      "Epoch [1565/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04265\n",
      "128\n",
      "Time elasped: 0.34259486198425293\n",
      "Mean Error: 0.00033151565003208816% \n",
      "--------------------------\n",
      "Epoch [1566/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03884\n",
      "128\n",
      "Time elasped: 0.3422739505767822\n",
      "Epoch [1567/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03709\n",
      "128\n",
      "Time elasped: 0.32607221603393555\n",
      "Epoch [1568/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03995\n",
      "128\n",
      "Time elasped: 0.35645270347595215\n",
      "Epoch [1569/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03858\n",
      "128\n",
      "Time elasped: 0.31471800804138184\n",
      "Epoch [1570/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03686\n",
      "128\n",
      "Time elasped: 0.3251187801361084\n",
      "Mean Error: 0.0003114267601631582% \n",
      "--------------------------\n",
      "Epoch [1571/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04028\n",
      "128\n",
      "Time elasped: 0.31493496894836426\n",
      "Epoch [1572/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04060\n",
      "128\n",
      "Time elasped: 0.32861757278442383\n",
      "Epoch [1573/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03774\n",
      "128\n",
      "Time elasped: 0.32509851455688477\n",
      "Epoch [1574/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03989\n",
      "128\n",
      "Time elasped: 0.32372045516967773\n",
      "Epoch [1575/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03602\n",
      "128\n",
      "Time elasped: 0.35027003288269043\n",
      "Mean Error: 0.00032413617009297013% \n",
      "--------------------------\n",
      "Epoch [1576/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03719\n",
      "128\n",
      "Time elasped: 0.32267117500305176\n",
      "Epoch [1577/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03879\n",
      "128\n",
      "Time elasped: 0.3395726680755615\n",
      "Epoch [1578/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03758\n",
      "128\n",
      "Time elasped: 0.34020113945007324\n",
      "Epoch [1579/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03771\n",
      "128\n",
      "Time elasped: 0.34253811836242676\n",
      "Epoch [1580/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03890\n",
      "128\n",
      "Time elasped: 0.3369767665863037\n",
      "Mean Error: 0.00028645756538026035% \n",
      "--------------------------\n",
      "Epoch [1581/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03968\n",
      "128\n",
      "Time elasped: 0.32750701904296875\n",
      "Epoch [1582/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03624\n",
      "128\n",
      "Time elasped: 0.3101675510406494\n",
      "Epoch [1583/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04200\n",
      "128\n",
      "Time elasped: 0.32985544204711914\n",
      "Epoch [1584/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03744\n",
      "128\n",
      "Time elasped: 0.3255598545074463\n",
      "Epoch [1585/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04272\n",
      "128\n",
      "Time elasped: 0.3265213966369629\n",
      "Mean Error: 0.0003132235142402351% \n",
      "--------------------------\n",
      "Epoch [1586/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04145\n",
      "128\n",
      "Time elasped: 0.3356034755706787\n",
      "Epoch [1587/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03901\n",
      "128\n",
      "Time elasped: 0.3333010673522949\n",
      "Epoch [1588/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03834\n",
      "128\n",
      "Time elasped: 0.32178544998168945\n",
      "Epoch [1589/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04037\n",
      "128\n",
      "Time elasped: 0.32371068000793457\n",
      "Epoch [1590/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03626\n",
      "128\n",
      "Time elasped: 0.32024550437927246\n",
      "Mean Error: 0.00031163933454081416% \n",
      "--------------------------\n",
      "Epoch [1591/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03960\n",
      "128\n",
      "Time elasped: 0.31949806213378906\n",
      "Epoch [1592/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03890\n",
      "128\n",
      "Time elasped: 0.32443785667419434\n",
      "Epoch [1593/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04218\n",
      "128\n",
      "Time elasped: 0.3285026550292969\n",
      "Epoch [1594/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03960\n",
      "128\n",
      "Time elasped: 0.31786012649536133\n",
      "Epoch [1595/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03668\n",
      "128\n",
      "Time elasped: 0.3416619300842285\n",
      "Mean Error: 0.000283652771031484% \n",
      "--------------------------\n",
      "Epoch [1596/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03751\n",
      "128\n",
      "Time elasped: 0.3199481964111328\n",
      "Epoch [1597/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03920\n",
      "128\n",
      "Time elasped: 0.3098461627960205\n",
      "Epoch [1598/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03983\n",
      "128\n",
      "Time elasped: 0.3171272277832031\n",
      "Epoch [1599/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.04172\n",
      "128\n",
      "Time elasped: 0.3413355350494385\n",
      "Epoch [1600/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03857\n",
      "128\n",
      "Time elasped: 0.34136223793029785\n",
      "Mean Error: 0.0003202497900929302% \n",
      "--------------------------\n",
      "Epoch [1601/3000], learning_rates 0.000463, 0.463291\n",
      "Step [1/1], Loss: 0.03955\n",
      "128\n",
      "Time elasped: 0.33173441886901855\n",
      "Epoch [1602/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04123\n",
      "128\n",
      "Time elasped: 0.32542967796325684\n",
      "Epoch [1603/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04149\n",
      "128\n",
      "Time elasped: 0.3428494930267334\n",
      "Epoch [1604/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04424\n",
      "128\n",
      "Time elasped: 0.35495638847351074\n",
      "Epoch [1605/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04096\n",
      "128\n",
      "Time elasped: 0.32502269744873047\n",
      "Mean Error: 0.0003129152755718678% \n",
      "--------------------------\n",
      "Epoch [1606/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04143\n",
      "128\n",
      "Time elasped: 0.32164597511291504\n",
      "Epoch [1607/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03825\n",
      "128\n",
      "Time elasped: 0.34569454193115234\n",
      "Epoch [1608/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04084\n",
      "128\n",
      "Time elasped: 0.32476210594177246\n",
      "Epoch [1609/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03668\n",
      "128\n",
      "Time elasped: 0.31158447265625\n",
      "Epoch [1610/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03824\n",
      "128\n",
      "Time elasped: 0.34082627296447754\n",
      "Mean Error: 0.0003031458181794733% \n",
      "--------------------------\n",
      "Epoch [1611/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03932\n",
      "128\n",
      "Time elasped: 0.36115455627441406\n",
      "Epoch [1612/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04119\n",
      "128\n",
      "Time elasped: 0.342482328414917\n",
      "Epoch [1613/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04061\n",
      "128\n",
      "Time elasped: 0.36818361282348633\n",
      "Epoch [1614/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04161\n",
      "128\n",
      "Time elasped: 0.33463001251220703\n",
      "Epoch [1615/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03893\n",
      "128\n",
      "Time elasped: 0.33310532569885254\n",
      "Mean Error: 0.00028965025558136404% \n",
      "--------------------------\n",
      "Epoch [1616/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04255\n",
      "128\n",
      "Time elasped: 0.34500741958618164\n",
      "Epoch [1617/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03608\n",
      "128\n",
      "Time elasped: 0.3499317169189453\n",
      "Epoch [1618/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04044\n",
      "128\n",
      "Time elasped: 0.3421943187713623\n",
      "Epoch [1619/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03751\n",
      "128\n",
      "Time elasped: 0.32051920890808105\n",
      "Epoch [1620/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03730\n",
      "128\n",
      "Time elasped: 0.34766292572021484\n",
      "Mean Error: 0.00031367718474939466% \n",
      "--------------------------\n",
      "Epoch [1621/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04242\n",
      "128\n",
      "Time elasped: 0.35485076904296875\n",
      "Epoch [1622/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03953\n",
      "128\n",
      "Time elasped: 0.319643497467041\n",
      "Epoch [1623/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03636\n",
      "128\n",
      "Time elasped: 0.33031129837036133\n",
      "Epoch [1624/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03699\n",
      "128\n",
      "Time elasped: 0.32972192764282227\n",
      "Epoch [1625/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03947\n",
      "128\n",
      "Time elasped: 0.3200960159301758\n",
      "Mean Error: 0.0003114278079010546% \n",
      "--------------------------\n",
      "Epoch [1626/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03879\n",
      "128\n",
      "Time elasped: 0.3290860652923584\n",
      "Epoch [1627/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03774\n",
      "128\n",
      "Time elasped: 0.3318498134613037\n",
      "Epoch [1628/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03636\n",
      "128\n",
      "Time elasped: 0.32556724548339844\n",
      "Epoch [1629/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03701\n",
      "128\n",
      "Time elasped: 0.3259541988372803\n",
      "Epoch [1630/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03747\n",
      "128\n",
      "Time elasped: 0.3136727809906006\n",
      "Mean Error: 0.000321246829116717% \n",
      "--------------------------\n",
      "Epoch [1631/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04043\n",
      "128\n",
      "Time elasped: 0.31113457679748535\n",
      "Epoch [1632/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04331\n",
      "128\n",
      "Time elasped: 0.3385486602783203\n",
      "Epoch [1633/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04016\n",
      "128\n",
      "Time elasped: 0.3205742835998535\n",
      "Epoch [1634/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03557\n",
      "128\n",
      "Time elasped: 0.3548862934112549\n",
      "Epoch [1635/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03911\n",
      "128\n",
      "Time elasped: 0.3330237865447998\n",
      "Mean Error: 0.0003167097456753254% \n",
      "--------------------------\n",
      "Epoch [1636/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04000\n",
      "128\n",
      "Time elasped: 0.3324313163757324\n",
      "Epoch [1637/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03789\n",
      "128\n",
      "Time elasped: 0.31324243545532227\n",
      "Epoch [1638/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03632\n",
      "128\n",
      "Time elasped: 0.31645774841308594\n",
      "Epoch [1639/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03836\n",
      "128\n",
      "Time elasped: 0.33980464935302734\n",
      "Epoch [1640/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03784\n",
      "128\n",
      "Time elasped: 0.3260657787322998\n",
      "Mean Error: 0.00032161991111934185% \n",
      "--------------------------\n",
      "Epoch [1641/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03790\n",
      "128\n",
      "Time elasped: 0.3140285015106201\n",
      "Epoch [1642/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03959\n",
      "128\n",
      "Time elasped: 0.32341551780700684\n",
      "Epoch [1643/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04315\n",
      "128\n",
      "Time elasped: 0.3320159912109375\n",
      "Epoch [1644/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03846\n",
      "128\n",
      "Time elasped: 0.32602834701538086\n",
      "Epoch [1645/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03783\n",
      "128\n",
      "Time elasped: 0.3121159076690674\n",
      "Mean Error: 0.00032533961348235607% \n",
      "--------------------------\n",
      "Epoch [1646/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03881\n",
      "128\n",
      "Time elasped: 0.31764721870422363\n",
      "Epoch [1647/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03870\n",
      "128\n",
      "Time elasped: 0.32866358757019043\n",
      "Epoch [1648/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03594\n",
      "128\n",
      "Time elasped: 0.34091877937316895\n",
      "Epoch [1649/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03675\n",
      "128\n",
      "Time elasped: 0.3209812641143799\n",
      "Epoch [1650/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03888\n",
      "128\n",
      "Time elasped: 0.31702589988708496\n",
      "Mean Error: 0.0003055500565096736% \n",
      "--------------------------\n",
      "Epoch [1651/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03781\n",
      "128\n",
      "Time elasped: 0.32136106491088867\n",
      "Epoch [1652/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03975\n",
      "128\n",
      "Time elasped: 0.33360767364501953\n",
      "Epoch [1653/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04131\n",
      "128\n",
      "Time elasped: 0.3239400386810303\n",
      "Epoch [1654/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03720\n",
      "128\n",
      "Time elasped: 0.32058167457580566\n",
      "Epoch [1655/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04167\n",
      "128\n",
      "Time elasped: 0.3464059829711914\n",
      "Mean Error: 0.00030906934989616275% \n",
      "--------------------------\n",
      "Epoch [1656/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04256\n",
      "128\n",
      "Time elasped: 0.32745814323425293\n",
      "Epoch [1657/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03883\n",
      "128\n",
      "Time elasped: 0.330061674118042\n",
      "Epoch [1658/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03830\n",
      "128\n",
      "Time elasped: 0.33600640296936035\n",
      "Epoch [1659/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03858\n",
      "128\n",
      "Time elasped: 0.32329273223876953\n",
      "Epoch [1660/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03680\n",
      "128\n",
      "Time elasped: 0.3196423053741455\n",
      "Mean Error: 0.00031858295551501215% \n",
      "--------------------------\n",
      "Epoch [1661/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04336\n",
      "128\n",
      "Time elasped: 0.34084081649780273\n",
      "Epoch [1662/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04076\n",
      "128\n",
      "Time elasped: 0.3104538917541504\n",
      "Epoch [1663/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04062\n",
      "128\n",
      "Time elasped: 0.3347921371459961\n",
      "Epoch [1664/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03740\n",
      "128\n",
      "Time elasped: 0.33869266510009766\n",
      "Epoch [1665/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03885\n",
      "128\n",
      "Time elasped: 0.4238710403442383\n",
      "Mean Error: 0.0003084558993577957% \n",
      "--------------------------\n",
      "Epoch [1666/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04052\n",
      "128\n",
      "Time elasped: 0.31995177268981934\n",
      "Epoch [1667/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03505\n",
      "128\n",
      "Time elasped: 0.33089733123779297\n",
      "Epoch [1668/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04026\n",
      "128\n",
      "Time elasped: 0.32909250259399414\n",
      "Epoch [1669/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03868\n",
      "128\n",
      "Time elasped: 0.32632946968078613\n",
      "Epoch [1670/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04056\n",
      "128\n",
      "Time elasped: 0.3246886730194092\n",
      "Mean Error: 0.00029781516059301794% \n",
      "--------------------------\n",
      "Epoch [1671/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04074\n",
      "128\n",
      "Time elasped: 0.32302141189575195\n",
      "Epoch [1672/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04054\n",
      "128\n",
      "Time elasped: 0.31722545623779297\n",
      "Epoch [1673/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03891\n",
      "128\n",
      "Time elasped: 0.32950592041015625\n",
      "Epoch [1674/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04204\n",
      "128\n",
      "Time elasped: 0.32513427734375\n",
      "Epoch [1675/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03757\n",
      "128\n",
      "Time elasped: 0.3447859287261963\n",
      "Mean Error: 0.0003189576673321426% \n",
      "--------------------------\n",
      "Epoch [1676/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03980\n",
      "128\n",
      "Time elasped: 0.3500545024871826\n",
      "Epoch [1677/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03931\n",
      "128\n",
      "Time elasped: 0.32421088218688965\n",
      "Epoch [1678/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03695\n",
      "128\n",
      "Time elasped: 0.3421602249145508\n",
      "Epoch [1679/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03791\n",
      "128\n",
      "Time elasped: 0.32333993911743164\n",
      "Epoch [1680/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03674\n",
      "128\n",
      "Time elasped: 0.3297390937805176\n",
      "Mean Error: 0.0002973049704451114% \n",
      "--------------------------\n",
      "Epoch [1681/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03810\n",
      "128\n",
      "Time elasped: 0.3131856918334961\n",
      "Epoch [1682/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03790\n",
      "128\n",
      "Time elasped: 0.3290534019470215\n",
      "Epoch [1683/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03741\n",
      "128\n",
      "Time elasped: 0.3140592575073242\n",
      "Epoch [1684/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03703\n",
      "128\n",
      "Time elasped: 0.318803071975708\n",
      "Epoch [1685/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03578\n",
      "128\n",
      "Time elasped: 0.3395533561706543\n",
      "Mean Error: 0.0002923104038927704% \n",
      "--------------------------\n",
      "Epoch [1686/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03902\n",
      "128\n",
      "Time elasped: 0.34221386909484863\n",
      "Epoch [1687/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03642\n",
      "128\n",
      "Time elasped: 0.357607364654541\n",
      "Epoch [1688/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03826\n",
      "128\n",
      "Time elasped: 0.32660841941833496\n",
      "Epoch [1689/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04265\n",
      "128\n",
      "Time elasped: 0.3448512554168701\n",
      "Epoch [1690/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03465\n",
      "128\n",
      "Time elasped: 0.34163951873779297\n",
      "Mean Error: 0.00030283271917141974% \n",
      "--------------------------\n",
      "Epoch [1691/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04027\n",
      "128\n",
      "Time elasped: 0.3283240795135498\n",
      "Epoch [1692/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03974\n",
      "128\n",
      "Time elasped: 0.31600427627563477\n",
      "Epoch [1693/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03671\n",
      "128\n",
      "Time elasped: 0.3239145278930664\n",
      "Epoch [1694/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.04053\n",
      "128\n",
      "Time elasped: 0.3199193477630615\n",
      "Epoch [1695/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03949\n",
      "128\n",
      "Time elasped: 0.34852170944213867\n",
      "Mean Error: 0.00029120699036866426% \n",
      "--------------------------\n",
      "Epoch [1696/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03745\n",
      "128\n",
      "Time elasped: 0.3531949520111084\n",
      "Epoch [1697/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03901\n",
      "128\n",
      "Time elasped: 0.33672475814819336\n",
      "Epoch [1698/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03589\n",
      "128\n",
      "Time elasped: 0.335634708404541\n",
      "Epoch [1699/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03678\n",
      "128\n",
      "Time elasped: 0.33660387992858887\n",
      "Epoch [1700/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03754\n",
      "128\n",
      "Time elasped: 0.34304213523864746\n",
      "Mean Error: 0.00030585823697037995% \n",
      "--------------------------\n",
      "Epoch [1701/3000], learning_rates 0.000440, 0.440127\n",
      "Step [1/1], Loss: 0.03993\n",
      "128\n",
      "Time elasped: 0.3403747081756592\n",
      "Epoch [1702/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04075\n",
      "128\n",
      "Time elasped: 0.335712194442749\n",
      "Epoch [1703/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03898\n",
      "128\n",
      "Time elasped: 0.3538048267364502\n",
      "Epoch [1704/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03783\n",
      "128\n",
      "Time elasped: 0.3234894275665283\n",
      "Epoch [1705/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04060\n",
      "128\n",
      "Time elasped: 0.33860254287719727\n",
      "Mean Error: 0.0003158063627779484% \n",
      "--------------------------\n",
      "Epoch [1706/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04017\n",
      "128\n",
      "Time elasped: 0.33464598655700684\n",
      "Epoch [1707/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03808\n",
      "128\n",
      "Time elasped: 0.35672664642333984\n",
      "Epoch [1708/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03459\n",
      "128\n",
      "Time elasped: 0.3413252830505371\n",
      "Epoch [1709/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03766\n",
      "128\n",
      "Time elasped: 0.342090368270874\n",
      "Epoch [1710/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03878\n",
      "128\n",
      "Time elasped: 0.34898829460144043\n",
      "Mean Error: 0.00031204987317323685% \n",
      "--------------------------\n",
      "Epoch [1711/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04151\n",
      "128\n",
      "Time elasped: 0.33318066596984863\n",
      "Epoch [1712/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03725\n",
      "128\n",
      "Time elasped: 0.3441481590270996\n",
      "Epoch [1713/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03519\n",
      "128\n",
      "Time elasped: 0.3252544403076172\n",
      "Epoch [1714/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03615\n",
      "128\n",
      "Time elasped: 0.33093929290771484\n",
      "Epoch [1715/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03807\n",
      "128\n",
      "Time elasped: 0.3607912063598633\n",
      "Mean Error: 0.00031592315644957125% \n",
      "--------------------------\n",
      "Epoch [1716/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03919\n",
      "128\n",
      "Time elasped: 0.3432800769805908\n",
      "Epoch [1717/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03767\n",
      "128\n",
      "Time elasped: 0.3518564701080322\n",
      "Epoch [1718/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03983\n",
      "128\n",
      "Time elasped: 0.3244497776031494\n",
      "Epoch [1719/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03687\n",
      "128\n",
      "Time elasped: 0.3248171806335449\n",
      "Epoch [1720/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03656\n",
      "128\n",
      "Time elasped: 0.3251323699951172\n",
      "Mean Error: 0.0003007282211910933% \n",
      "--------------------------\n",
      "Epoch [1721/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03744\n",
      "128\n",
      "Time elasped: 0.3426849842071533\n",
      "Epoch [1722/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03635\n",
      "128\n",
      "Time elasped: 0.3098459243774414\n",
      "Epoch [1723/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03831\n",
      "128\n",
      "Time elasped: 0.3295905590057373\n",
      "Epoch [1724/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03971\n",
      "128\n",
      "Time elasped: 0.347747802734375\n",
      "Epoch [1725/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03262\n",
      "128\n",
      "Time elasped: 0.3098568916320801\n",
      "Mean Error: 0.00031110303825698793% \n",
      "--------------------------\n",
      "Epoch [1726/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03589\n",
      "128\n",
      "Time elasped: 0.3480536937713623\n",
      "Epoch [1727/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03980\n",
      "128\n",
      "Time elasped: 0.31449007987976074\n",
      "Epoch [1728/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04082\n",
      "128\n",
      "Time elasped: 0.31996679306030273\n",
      "Epoch [1729/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04080\n",
      "128\n",
      "Time elasped: 0.33010387420654297\n",
      "Epoch [1730/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04024\n",
      "128\n",
      "Time elasped: 0.3125789165496826\n",
      "Mean Error: 0.0003021782322321087% \n",
      "--------------------------\n",
      "Epoch [1731/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04143\n",
      "128\n",
      "Time elasped: 0.33431148529052734\n",
      "Epoch [1732/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03734\n",
      "128\n",
      "Time elasped: 0.32318544387817383\n",
      "Epoch [1733/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03766\n",
      "128\n",
      "Time elasped: 0.32061171531677246\n",
      "Epoch [1734/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03821\n",
      "128\n",
      "Time elasped: 0.3267357349395752\n",
      "Epoch [1735/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03943\n",
      "128\n",
      "Time elasped: 0.3282442092895508\n",
      "Mean Error: 0.00031021080212667584% \n",
      "--------------------------\n",
      "Epoch [1736/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03998\n",
      "128\n",
      "Time elasped: 0.32990145683288574\n",
      "Epoch [1737/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03991\n",
      "128\n",
      "Time elasped: 0.3343641757965088\n",
      "Epoch [1738/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03891\n",
      "128\n",
      "Time elasped: 0.3243248462677002\n",
      "Epoch [1739/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03741\n",
      "128\n",
      "Time elasped: 0.3317418098449707\n",
      "Epoch [1740/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03996\n",
      "128\n",
      "Time elasped: 0.3206503391265869\n",
      "Mean Error: 0.0003108812670689076% \n",
      "--------------------------\n",
      "Epoch [1741/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03846\n",
      "128\n",
      "Time elasped: 0.3217644691467285\n",
      "Epoch [1742/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03893\n",
      "128\n",
      "Time elasped: 0.3383636474609375\n",
      "Epoch [1743/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03418\n",
      "128\n",
      "Time elasped: 0.3448779582977295\n",
      "Epoch [1744/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03984\n",
      "128\n",
      "Time elasped: 0.3483273983001709\n",
      "Epoch [1745/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03792\n",
      "128\n",
      "Time elasped: 0.3450126647949219\n",
      "Mean Error: 0.00030601012986153364% \n",
      "--------------------------\n",
      "Epoch [1746/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03615\n",
      "128\n",
      "Time elasped: 0.33064961433410645\n",
      "Epoch [1747/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03976\n",
      "128\n",
      "Time elasped: 0.3236377239227295\n",
      "Epoch [1748/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03984\n",
      "128\n",
      "Time elasped: 0.3226461410522461\n",
      "Epoch [1749/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03830\n",
      "128\n",
      "Time elasped: 0.34882235527038574\n",
      "Epoch [1750/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03982\n",
      "128\n",
      "Time elasped: 0.3233449459075928\n",
      "Mean Error: 0.00030772341415286064% \n",
      "--------------------------\n",
      "Epoch [1751/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03891\n",
      "128\n",
      "Time elasped: 0.34958982467651367\n",
      "Epoch [1752/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03947\n",
      "128\n",
      "Time elasped: 0.3199796676635742\n",
      "Epoch [1753/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03718\n",
      "128\n",
      "Time elasped: 0.3207559585571289\n",
      "Epoch [1754/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03823\n",
      "128\n",
      "Time elasped: 0.3548557758331299\n",
      "Epoch [1755/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03807\n",
      "128\n",
      "Time elasped: 0.3442535400390625\n",
      "Mean Error: 0.00030903221340849996% \n",
      "--------------------------\n",
      "Epoch [1756/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04152\n",
      "128\n",
      "Time elasped: 0.3287925720214844\n",
      "Epoch [1757/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03440\n",
      "128\n",
      "Time elasped: 0.3569478988647461\n",
      "Epoch [1758/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03932\n",
      "128\n",
      "Time elasped: 0.34232664108276367\n",
      "Epoch [1759/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03843\n",
      "128\n",
      "Time elasped: 0.3324418067932129\n",
      "Epoch [1760/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03847\n",
      "128\n",
      "Time elasped: 0.3363380432128906\n",
      "Mean Error: 0.0003027395287062973% \n",
      "--------------------------\n",
      "Epoch [1761/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04158\n",
      "128\n",
      "Time elasped: 0.35638880729675293\n",
      "Epoch [1762/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03561\n",
      "128\n",
      "Time elasped: 0.3710193634033203\n",
      "Epoch [1763/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03877\n",
      "128\n",
      "Time elasped: 0.3727109432220459\n",
      "Epoch [1764/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03983\n",
      "128\n",
      "Time elasped: 0.4476969242095947\n",
      "Epoch [1765/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03841\n",
      "128\n",
      "Time elasped: 0.37682151794433594\n",
      "Mean Error: 0.0003045907651539892% \n",
      "--------------------------\n",
      "Epoch [1766/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03781\n",
      "128\n",
      "Time elasped: 0.41696643829345703\n",
      "Epoch [1767/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04365\n",
      "128\n",
      "Time elasped: 0.40050840377807617\n",
      "Epoch [1768/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03885\n",
      "128\n",
      "Time elasped: 0.45644617080688477\n",
      "Epoch [1769/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03678\n",
      "128\n",
      "Time elasped: 0.4194333553314209\n",
      "Epoch [1770/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03686\n",
      "128\n",
      "Time elasped: 0.3617093563079834\n",
      "Mean Error: 0.00028890621615573764% \n",
      "--------------------------\n",
      "Epoch [1771/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03838\n",
      "128\n",
      "Time elasped: 0.38399410247802734\n",
      "Epoch [1772/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03484\n",
      "128\n",
      "Time elasped: 0.36603212356567383\n",
      "Epoch [1773/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03830\n",
      "128\n",
      "Time elasped: 0.37221741676330566\n",
      "Epoch [1774/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03547\n",
      "128\n",
      "Time elasped: 0.40107035636901855\n",
      "Epoch [1775/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03839\n",
      "128\n",
      "Time elasped: 0.3204019069671631\n",
      "Mean Error: 0.0002911594929173589% \n",
      "--------------------------\n",
      "Epoch [1776/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04069\n",
      "128\n",
      "Time elasped: 0.35018253326416016\n",
      "Epoch [1777/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03446\n",
      "128\n",
      "Time elasped: 0.36567211151123047\n",
      "Epoch [1778/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03743\n",
      "128\n",
      "Time elasped: 0.3471407890319824\n",
      "Epoch [1779/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03414\n",
      "128\n",
      "Time elasped: 0.36113929748535156\n",
      "Epoch [1780/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03838\n",
      "128\n",
      "Time elasped: 0.347761869430542\n",
      "Mean Error: 0.00027458780095912516% \n",
      "--------------------------\n",
      "Epoch [1781/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03880\n",
      "128\n",
      "Time elasped: 0.3465592861175537\n",
      "Epoch [1782/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03829\n",
      "128\n",
      "Time elasped: 0.34066319465637207\n",
      "Epoch [1783/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04087\n",
      "128\n",
      "Time elasped: 0.3515300750732422\n",
      "Epoch [1784/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03972\n",
      "128\n",
      "Time elasped: 0.3619077205657959\n",
      "Epoch [1785/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03721\n",
      "128\n",
      "Time elasped: 0.33809733390808105\n",
      "Mean Error: 0.00029039554647170007% \n",
      "--------------------------\n",
      "Epoch [1786/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03694\n",
      "128\n",
      "Time elasped: 0.34026265144348145\n",
      "Epoch [1787/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03748\n",
      "128\n",
      "Time elasped: 0.3392188549041748\n",
      "Epoch [1788/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03768\n",
      "128\n",
      "Time elasped: 0.33565354347229004\n",
      "Epoch [1789/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03844\n",
      "128\n",
      "Time elasped: 0.3519420623779297\n",
      "Epoch [1790/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03747\n",
      "128\n",
      "Time elasped: 0.35405731201171875\n",
      "Mean Error: 0.0003075346176046878% \n",
      "--------------------------\n",
      "Epoch [1791/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03772\n",
      "128\n",
      "Time elasped: 0.332195520401001\n",
      "Epoch [1792/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03554\n",
      "128\n",
      "Time elasped: 0.34064197540283203\n",
      "Epoch [1793/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03465\n",
      "128\n",
      "Time elasped: 0.34670114517211914\n",
      "Epoch [1794/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03728\n",
      "128\n",
      "Time elasped: 0.34163665771484375\n",
      "Epoch [1795/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03714\n",
      "128\n",
      "Time elasped: 0.3342456817626953\n",
      "Mean Error: 0.0002872662153095007% \n",
      "--------------------------\n",
      "Epoch [1796/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.04198\n",
      "128\n",
      "Time elasped: 0.350250244140625\n",
      "Epoch [1797/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03577\n",
      "128\n",
      "Time elasped: 0.3782963752746582\n",
      "Epoch [1798/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03980\n",
      "128\n",
      "Time elasped: 0.32574987411499023\n",
      "Epoch [1799/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03796\n",
      "128\n",
      "Time elasped: 0.32525110244750977\n",
      "Epoch [1800/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03842\n",
      "128\n",
      "Time elasped: 0.3712031841278076\n",
      "Mean Error: 0.0002973030786961317% \n",
      "--------------------------\n",
      "Epoch [1801/3000], learning_rates 0.000418, 0.418120\n",
      "Step [1/1], Loss: 0.03859\n",
      "128\n",
      "Time elasped: 0.33931636810302734\n",
      "Epoch [1802/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03696\n",
      "128\n",
      "Time elasped: 0.34205198287963867\n",
      "Epoch [1803/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03950\n",
      "128\n",
      "Time elasped: 0.31462693214416504\n",
      "Epoch [1804/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03551\n",
      "128\n",
      "Time elasped: 0.34129762649536133\n",
      "Epoch [1805/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03691\n",
      "128\n",
      "Time elasped: 0.34657931327819824\n",
      "Mean Error: 0.0002812895108945668% \n",
      "--------------------------\n",
      "Epoch [1806/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03847\n",
      "128\n",
      "Time elasped: 0.38544321060180664\n",
      "Epoch [1807/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04008\n",
      "128\n",
      "Time elasped: 0.36309146881103516\n",
      "Epoch [1808/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03538\n",
      "128\n",
      "Time elasped: 0.3404252529144287\n",
      "Epoch [1809/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03820\n",
      "128\n",
      "Time elasped: 0.32639622688293457\n",
      "Epoch [1810/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03747\n",
      "128\n",
      "Time elasped: 0.34145617485046387\n",
      "Mean Error: 0.0002736753667704761% \n",
      "--------------------------\n",
      "Epoch [1811/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03818\n",
      "128\n",
      "Time elasped: 0.34685635566711426\n",
      "Epoch [1812/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03766\n",
      "128\n",
      "Time elasped: 0.33597517013549805\n",
      "Epoch [1813/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03822\n",
      "128\n",
      "Time elasped: 0.3473939895629883\n",
      "Epoch [1814/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03789\n",
      "128\n",
      "Time elasped: 0.32747793197631836\n",
      "Epoch [1815/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03775\n",
      "128\n",
      "Time elasped: 0.3224356174468994\n",
      "Mean Error: 0.000321735831676051% \n",
      "--------------------------\n",
      "Epoch [1816/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03939\n",
      "128\n",
      "Time elasped: 0.34087657928466797\n",
      "Epoch [1817/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03754\n",
      "128\n",
      "Time elasped: 0.33484697341918945\n",
      "Epoch [1818/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03816\n",
      "128\n",
      "Time elasped: 0.341092586517334\n",
      "Epoch [1819/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03746\n",
      "128\n",
      "Time elasped: 0.3449740409851074\n",
      "Epoch [1820/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03919\n",
      "128\n",
      "Time elasped: 0.3299727439880371\n",
      "Mean Error: 0.0002908551541622728% \n",
      "--------------------------\n",
      "Epoch [1821/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03872\n",
      "128\n",
      "Time elasped: 0.3626985549926758\n",
      "Epoch [1822/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03532\n",
      "128\n",
      "Time elasped: 0.35072803497314453\n",
      "Epoch [1823/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03767\n",
      "128\n",
      "Time elasped: 0.3224811553955078\n",
      "Epoch [1824/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03849\n",
      "128\n",
      "Time elasped: 0.3607327938079834\n",
      "Epoch [1825/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03601\n",
      "128\n",
      "Time elasped: 0.33673572540283203\n",
      "Mean Error: 0.0002923327265307307% \n",
      "--------------------------\n",
      "Epoch [1826/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03831\n",
      "128\n",
      "Time elasped: 0.3479597568511963\n",
      "Epoch [1827/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03718\n",
      "128\n",
      "Time elasped: 0.3242979049682617\n",
      "Epoch [1828/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03652\n",
      "128\n",
      "Time elasped: 0.3706197738647461\n",
      "Epoch [1829/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03483\n",
      "128\n",
      "Time elasped: 0.3631305694580078\n",
      "Epoch [1830/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03653\n",
      "128\n",
      "Time elasped: 0.3571813106536865\n",
      "Mean Error: 0.0003072673862334341% \n",
      "--------------------------\n",
      "Epoch [1831/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03803\n",
      "128\n",
      "Time elasped: 0.3427886962890625\n",
      "Epoch [1832/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03847\n",
      "128\n",
      "Time elasped: 0.33554959297180176\n",
      "Epoch [1833/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04140\n",
      "128\n",
      "Time elasped: 0.3479466438293457\n",
      "Epoch [1834/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03579\n",
      "128\n",
      "Time elasped: 0.33496952056884766\n",
      "Epoch [1835/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03639\n",
      "128\n",
      "Time elasped: 0.3232100009918213\n",
      "Mean Error: 0.00029370709671638906% \n",
      "--------------------------\n",
      "Epoch [1836/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03831\n",
      "128\n",
      "Time elasped: 0.3433518409729004\n",
      "Epoch [1837/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03896\n",
      "128\n",
      "Time elasped: 0.35500049591064453\n",
      "Epoch [1838/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03313\n",
      "128\n",
      "Time elasped: 0.32160115242004395\n",
      "Epoch [1839/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03441\n",
      "128\n",
      "Time elasped: 0.3381383419036865\n",
      "Epoch [1840/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04054\n",
      "128\n",
      "Time elasped: 0.333341121673584\n",
      "Mean Error: 0.0003057406283915043% \n",
      "--------------------------\n",
      "Epoch [1841/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03454\n",
      "128\n",
      "Time elasped: 0.34845876693725586\n",
      "Epoch [1842/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03905\n",
      "128\n",
      "Time elasped: 0.35467076301574707\n",
      "Epoch [1843/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03659\n",
      "128\n",
      "Time elasped: 0.36589813232421875\n",
      "Epoch [1844/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03820\n",
      "128\n",
      "Time elasped: 0.35539960861206055\n",
      "Epoch [1845/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03612\n",
      "128\n",
      "Time elasped: 0.3565673828125\n",
      "Mean Error: 0.0002991840592585504% \n",
      "--------------------------\n",
      "Epoch [1846/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03596\n",
      "128\n",
      "Time elasped: 0.34449338912963867\n",
      "Epoch [1847/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03546\n",
      "128\n",
      "Time elasped: 0.327864408493042\n",
      "Epoch [1848/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03935\n",
      "128\n",
      "Time elasped: 0.3399183750152588\n",
      "Epoch [1849/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03476\n",
      "128\n",
      "Time elasped: 0.34085845947265625\n",
      "Epoch [1850/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03590\n",
      "128\n",
      "Time elasped: 0.3389244079589844\n",
      "Mean Error: 0.00026317447191104293% \n",
      "--------------------------\n",
      "Epoch [1851/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03794\n",
      "128\n",
      "Time elasped: 0.3491501808166504\n",
      "Epoch [1852/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03649\n",
      "128\n",
      "Time elasped: 0.3292567729949951\n",
      "Epoch [1853/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03402\n",
      "128\n",
      "Time elasped: 0.3296198844909668\n",
      "Epoch [1854/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03617\n",
      "128\n",
      "Time elasped: 0.3415985107421875\n",
      "Epoch [1855/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03684\n",
      "128\n",
      "Time elasped: 0.35323286056518555\n",
      "Mean Error: 0.0002792200830299407% \n",
      "--------------------------\n",
      "Epoch [1856/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03708\n",
      "128\n",
      "Time elasped: 0.33719444274902344\n",
      "Epoch [1857/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04070\n",
      "128\n",
      "Time elasped: 0.3408043384552002\n",
      "Epoch [1858/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04036\n",
      "128\n",
      "Time elasped: 0.3297896385192871\n",
      "Epoch [1859/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03695\n",
      "128\n",
      "Time elasped: 0.3387629985809326\n",
      "Epoch [1860/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03694\n",
      "128\n",
      "Time elasped: 0.3629171848297119\n",
      "Mean Error: 0.00029121257830411196% \n",
      "--------------------------\n",
      "Epoch [1861/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03455\n",
      "128\n",
      "Time elasped: 0.33164453506469727\n",
      "Epoch [1862/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03570\n",
      "128\n",
      "Time elasped: 0.32116079330444336\n",
      "Epoch [1863/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03979\n",
      "128\n",
      "Time elasped: 0.3557553291320801\n",
      "Epoch [1864/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03822\n",
      "128\n",
      "Time elasped: 0.33748650550842285\n",
      "Epoch [1865/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03915\n",
      "128\n",
      "Time elasped: 0.3428659439086914\n",
      "Mean Error: 0.00028848182410001755% \n",
      "--------------------------\n",
      "Epoch [1866/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03511\n",
      "128\n",
      "Time elasped: 0.3625373840332031\n",
      "Epoch [1867/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03760\n",
      "128\n",
      "Time elasped: 0.3391761779785156\n",
      "Epoch [1868/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03598\n",
      "128\n",
      "Time elasped: 0.328916072845459\n",
      "Epoch [1869/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03793\n",
      "128\n",
      "Time elasped: 0.3505682945251465\n",
      "Epoch [1870/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03842\n",
      "128\n",
      "Time elasped: 0.35722851753234863\n",
      "Mean Error: 0.0002905285800807178% \n",
      "--------------------------\n",
      "Epoch [1871/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03657\n",
      "128\n",
      "Time elasped: 0.32511091232299805\n",
      "Epoch [1872/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04059\n",
      "128\n",
      "Time elasped: 0.3181281089782715\n",
      "Epoch [1873/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03587\n",
      "128\n",
      "Time elasped: 0.33940815925598145\n",
      "Epoch [1874/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04147\n",
      "128\n",
      "Time elasped: 0.36276865005493164\n",
      "Epoch [1875/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03701\n",
      "128\n",
      "Time elasped: 0.32076311111450195\n",
      "Mean Error: 0.00030854655778966844% \n",
      "--------------------------\n",
      "Epoch [1876/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03930\n",
      "128\n",
      "Time elasped: 0.34111499786376953\n",
      "Epoch [1877/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03800\n",
      "128\n",
      "Time elasped: 0.353487491607666\n",
      "Epoch [1878/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04226\n",
      "128\n",
      "Time elasped: 0.3373265266418457\n",
      "Epoch [1879/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03739\n",
      "128\n",
      "Time elasped: 0.3469722270965576\n",
      "Epoch [1880/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04051\n",
      "128\n",
      "Time elasped: 0.3329894542694092\n",
      "Mean Error: 0.00029663959867320955% \n",
      "--------------------------\n",
      "Epoch [1881/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03956\n",
      "128\n",
      "Time elasped: 0.326570987701416\n",
      "Epoch [1882/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03598\n",
      "128\n",
      "Time elasped: 0.35674500465393066\n",
      "Epoch [1883/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03737\n",
      "128\n",
      "Time elasped: 0.3471415042877197\n",
      "Epoch [1884/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04026\n",
      "128\n",
      "Time elasped: 0.33804821968078613\n",
      "Epoch [1885/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03695\n",
      "128\n",
      "Time elasped: 0.32590222358703613\n",
      "Mean Error: 0.00032415895839221776% \n",
      "--------------------------\n",
      "Epoch [1886/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.04379\n",
      "128\n",
      "Time elasped: 0.34493255615234375\n",
      "Epoch [1887/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03824\n",
      "128\n",
      "Time elasped: 0.341597318649292\n",
      "Epoch [1888/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03698\n",
      "128\n",
      "Time elasped: 0.3508439064025879\n",
      "Epoch [1889/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03662\n",
      "128\n",
      "Time elasped: 0.33318161964416504\n",
      "Epoch [1890/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03608\n",
      "128\n",
      "Time elasped: 0.4222831726074219\n",
      "Mean Error: 0.0002996334806084633% \n",
      "--------------------------\n",
      "Epoch [1891/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03626\n",
      "128\n",
      "Time elasped: 0.34035277366638184\n",
      "Epoch [1892/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03781\n",
      "128\n",
      "Time elasped: 0.3706974983215332\n",
      "Epoch [1893/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03804\n",
      "128\n",
      "Time elasped: 0.35044264793395996\n",
      "Epoch [1894/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03532\n",
      "128\n",
      "Time elasped: 0.3572680950164795\n",
      "Epoch [1895/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03680\n",
      "128\n",
      "Time elasped: 0.3499274253845215\n",
      "Mean Error: 0.00028686801670119166% \n",
      "--------------------------\n",
      "Epoch [1896/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03403\n",
      "128\n",
      "Time elasped: 0.3291432857513428\n",
      "Epoch [1897/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03415\n",
      "128\n",
      "Time elasped: 0.350985050201416\n",
      "Epoch [1898/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03874\n",
      "128\n",
      "Time elasped: 0.3408651351928711\n",
      "Epoch [1899/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03555\n",
      "128\n",
      "Time elasped: 0.3229696750640869\n",
      "Epoch [1900/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03490\n",
      "128\n",
      "Time elasped: 0.3365299701690674\n",
      "Mean Error: 0.00029361865017563105% \n",
      "--------------------------\n",
      "Epoch [1901/3000], learning_rates 0.000397, 0.397214\n",
      "Step [1/1], Loss: 0.03430\n",
      "128\n",
      "Time elasped: 0.3440976142883301\n",
      "Epoch [1902/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03839\n",
      "128\n",
      "Time elasped: 0.3242628574371338\n",
      "Epoch [1903/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03594\n",
      "128\n",
      "Time elasped: 0.34536218643188477\n",
      "Epoch [1904/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03437\n",
      "128\n",
      "Time elasped: 0.3335096836090088\n",
      "Epoch [1905/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03623\n",
      "128\n",
      "Time elasped: 0.3303639888763428\n",
      "Mean Error: 0.0002977378899231553% \n",
      "--------------------------\n",
      "Epoch [1906/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03864\n",
      "128\n",
      "Time elasped: 0.35338902473449707\n",
      "Epoch [1907/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03583\n",
      "128\n",
      "Time elasped: 0.36949706077575684\n",
      "Epoch [1908/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03763\n",
      "128\n",
      "Time elasped: 0.3403916358947754\n",
      "Epoch [1909/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03715\n",
      "128\n",
      "Time elasped: 0.3684539794921875\n",
      "Epoch [1910/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03616\n",
      "128\n",
      "Time elasped: 0.3420424461364746\n",
      "Mean Error: 0.00032553690834902227% \n",
      "--------------------------\n",
      "Epoch [1911/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03623\n",
      "128\n",
      "Time elasped: 0.3567836284637451\n",
      "Epoch [1912/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03352\n",
      "128\n",
      "Time elasped: 0.3184685707092285\n",
      "Epoch [1913/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03495\n",
      "128\n",
      "Time elasped: 0.3431863784790039\n",
      "Epoch [1914/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03636\n",
      "128\n",
      "Time elasped: 0.3299093246459961\n",
      "Epoch [1915/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03735\n",
      "128\n",
      "Time elasped: 0.33775973320007324\n",
      "Mean Error: 0.0002978576230816543% \n",
      "--------------------------\n",
      "Epoch [1916/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03577\n",
      "128\n",
      "Time elasped: 0.33779191970825195\n",
      "Epoch [1917/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03519\n",
      "128\n",
      "Time elasped: 0.3261871337890625\n",
      "Epoch [1918/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03623\n",
      "128\n",
      "Time elasped: 0.3307192325592041\n",
      "Epoch [1919/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03742\n",
      "128\n",
      "Time elasped: 0.3208496570587158\n",
      "Epoch [1920/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03880\n",
      "128\n",
      "Time elasped: 0.32784485816955566\n",
      "Mean Error: 0.000291060539893806% \n",
      "--------------------------\n",
      "Epoch [1921/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03711\n",
      "128\n",
      "Time elasped: 0.327455997467041\n",
      "Epoch [1922/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03355\n",
      "128\n",
      "Time elasped: 0.34953975677490234\n",
      "Epoch [1923/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03488\n",
      "128\n",
      "Time elasped: 0.33349013328552246\n",
      "Epoch [1924/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03628\n",
      "128\n",
      "Time elasped: 0.32511067390441895\n",
      "Epoch [1925/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03714\n",
      "128\n",
      "Time elasped: 0.3549795150756836\n",
      "Mean Error: 0.00029013596940785646% \n",
      "--------------------------\n",
      "Epoch [1926/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.04172\n",
      "128\n",
      "Time elasped: 0.35184144973754883\n",
      "Epoch [1927/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03533\n",
      "128\n",
      "Time elasped: 0.32398176193237305\n",
      "Epoch [1928/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03621\n",
      "128\n",
      "Time elasped: 0.33840370178222656\n",
      "Epoch [1929/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03931\n",
      "128\n",
      "Time elasped: 0.34267711639404297\n",
      "Epoch [1930/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03551\n",
      "128\n",
      "Time elasped: 0.3259458541870117\n",
      "Mean Error: 0.00032614602241665125% \n",
      "--------------------------\n",
      "Epoch [1931/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03832\n",
      "128\n",
      "Time elasped: 0.3385579586029053\n",
      "Epoch [1932/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03522\n",
      "128\n",
      "Time elasped: 0.3115239143371582\n",
      "Epoch [1933/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03632\n",
      "128\n",
      "Time elasped: 0.3438856601715088\n",
      "Epoch [1934/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03764\n",
      "128\n",
      "Time elasped: 0.3342595100402832\n",
      "Epoch [1935/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03518\n",
      "128\n",
      "Time elasped: 0.3567500114440918\n",
      "Mean Error: 0.0002936192031484097% \n",
      "--------------------------\n",
      "Epoch [1936/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03552\n",
      "128\n",
      "Time elasped: 0.32867932319641113\n",
      "Epoch [1937/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03760\n",
      "128\n",
      "Time elasped: 0.341902494430542\n",
      "Epoch [1938/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03259\n",
      "128\n",
      "Time elasped: 0.3595547676086426\n",
      "Epoch [1939/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03510\n",
      "128\n",
      "Time elasped: 0.34221601486206055\n",
      "Epoch [1940/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03426\n",
      "128\n",
      "Time elasped: 0.3432586193084717\n",
      "Mean Error: 0.00029190845089033246% \n",
      "--------------------------\n",
      "Epoch [1941/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03414\n",
      "128\n",
      "Time elasped: 0.37959909439086914\n",
      "Epoch [1942/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03884\n",
      "128\n",
      "Time elasped: 0.3509032726287842\n",
      "Epoch [1943/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03561\n",
      "128\n",
      "Time elasped: 0.37760257720947266\n",
      "Epoch [1944/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03602\n",
      "128\n",
      "Time elasped: 0.3317999839782715\n",
      "Epoch [1945/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03343\n",
      "128\n",
      "Time elasped: 0.34757184982299805\n",
      "Mean Error: 0.00025880694738589227% \n",
      "--------------------------\n",
      "Epoch [1946/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03594\n",
      "128\n",
      "Time elasped: 0.34673428535461426\n",
      "Epoch [1947/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03579\n",
      "128\n",
      "Time elasped: 0.3548769950866699\n",
      "Epoch [1948/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03565\n",
      "128\n",
      "Time elasped: 0.3303518295288086\n",
      "Epoch [1949/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03411\n",
      "128\n",
      "Time elasped: 0.3479785919189453\n",
      "Epoch [1950/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03604\n",
      "128\n",
      "Time elasped: 0.31171226501464844\n",
      "Mean Error: 0.000278439256362617% \n",
      "--------------------------\n",
      "Epoch [1951/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03807\n",
      "128\n",
      "Time elasped: 0.3272249698638916\n",
      "Epoch [1952/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03566\n",
      "128\n",
      "Time elasped: 0.3412039279937744\n",
      "Epoch [1953/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03532\n",
      "128\n",
      "Time elasped: 0.32485365867614746\n",
      "Epoch [1954/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03522\n",
      "128\n",
      "Time elasped: 0.35181474685668945\n",
      "Epoch [1955/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03217\n",
      "128\n",
      "Time elasped: 0.36199140548706055\n",
      "Mean Error: 0.0002862473193090409% \n",
      "--------------------------\n",
      "Epoch [1956/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03416\n",
      "128\n",
      "Time elasped: 0.3439767360687256\n",
      "Epoch [1957/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03725\n",
      "128\n",
      "Time elasped: 0.37131381034851074\n",
      "Epoch [1958/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03574\n",
      "128\n",
      "Time elasped: 0.36505651473999023\n",
      "Epoch [1959/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03838\n",
      "128\n",
      "Time elasped: 0.33252620697021484\n",
      "Epoch [1960/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03919\n",
      "128\n",
      "Time elasped: 0.3642001152038574\n",
      "Mean Error: 0.0002981126890517771% \n",
      "--------------------------\n",
      "Epoch [1961/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03800\n",
      "128\n",
      "Time elasped: 0.333355188369751\n",
      "Epoch [1962/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03278\n",
      "128\n",
      "Time elasped: 0.34751176834106445\n",
      "Epoch [1963/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03576\n",
      "128\n",
      "Time elasped: 0.34096479415893555\n",
      "Epoch [1964/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03210\n",
      "128\n",
      "Time elasped: 0.334913969039917\n",
      "Epoch [1965/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03445\n",
      "128\n",
      "Time elasped: 0.345991849899292\n",
      "Mean Error: 0.00027625876828096807% \n",
      "--------------------------\n",
      "Epoch [1966/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03946\n",
      "128\n",
      "Time elasped: 0.34717655181884766\n",
      "Epoch [1967/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03823\n",
      "128\n",
      "Time elasped: 0.34180140495300293\n",
      "Epoch [1968/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03353\n",
      "128\n",
      "Time elasped: 0.3295111656188965\n",
      "Epoch [1969/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03611\n",
      "128\n",
      "Time elasped: 0.35715556144714355\n",
      "Epoch [1970/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03489\n",
      "128\n",
      "Time elasped: 0.32692599296569824\n",
      "Mean Error: 0.00024634439614601433% \n",
      "--------------------------\n",
      "Epoch [1971/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03364\n",
      "128\n",
      "Time elasped: 0.33811140060424805\n",
      "Epoch [1972/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03667\n",
      "128\n",
      "Time elasped: 0.3715965747833252\n",
      "Epoch [1973/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03690\n",
      "128\n",
      "Time elasped: 0.3252086639404297\n",
      "Epoch [1974/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03382\n",
      "128\n",
      "Time elasped: 0.32303333282470703\n",
      "Epoch [1975/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03538\n",
      "128\n",
      "Time elasped: 0.36235499382019043\n",
      "Mean Error: 0.0002606301859486848% \n",
      "--------------------------\n",
      "Epoch [1976/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03616\n",
      "128\n",
      "Time elasped: 0.4228038787841797\n",
      "Epoch [1977/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03171\n",
      "128\n",
      "Time elasped: 0.33006787300109863\n",
      "Epoch [1978/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03482\n",
      "128\n",
      "Time elasped: 0.3460206985473633\n",
      "Epoch [1979/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03251\n",
      "128\n",
      "Time elasped: 0.3325462341308594\n",
      "Epoch [1980/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03631\n",
      "128\n",
      "Time elasped: 0.3357360363006592\n",
      "Mean Error: 0.0002904733701143414% \n",
      "--------------------------\n",
      "Epoch [1981/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03597\n",
      "128\n",
      "Time elasped: 0.31331729888916016\n",
      "Epoch [1982/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03489\n",
      "128\n",
      "Time elasped: 0.32460904121398926\n",
      "Epoch [1983/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03402\n",
      "128\n",
      "Time elasped: 0.35243749618530273\n",
      "Epoch [1984/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03539\n",
      "128\n",
      "Time elasped: 0.33124446868896484\n",
      "Epoch [1985/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03700\n",
      "128\n",
      "Time elasped: 0.32724642753601074\n",
      "Mean Error: 0.0002826886484399438% \n",
      "--------------------------\n",
      "Epoch [1986/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03608\n",
      "128\n",
      "Time elasped: 0.33766698837280273\n",
      "Epoch [1987/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03536\n",
      "128\n",
      "Time elasped: 0.3289065361022949\n",
      "Epoch [1988/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03688\n",
      "128\n",
      "Time elasped: 0.32455945014953613\n",
      "Epoch [1989/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03344\n",
      "128\n",
      "Time elasped: 0.34770774841308594\n",
      "Epoch [1990/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03598\n",
      "128\n",
      "Time elasped: 0.3251833915710449\n",
      "Mean Error: 0.0002691840927582234% \n",
      "--------------------------\n",
      "Epoch [1991/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03741\n",
      "128\n",
      "Time elasped: 0.35692763328552246\n",
      "Epoch [1992/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03552\n",
      "128\n",
      "Time elasped: 0.3551030158996582\n",
      "Epoch [1993/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03385\n",
      "128\n",
      "Time elasped: 0.35651421546936035\n",
      "Epoch [1994/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03471\n",
      "128\n",
      "Time elasped: 0.37882065773010254\n",
      "Epoch [1995/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03426\n",
      "128\n",
      "Time elasped: 0.3273797035217285\n",
      "Mean Error: 0.0002947948523797095% \n",
      "--------------------------\n",
      "Epoch [1996/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03414\n",
      "128\n",
      "Time elasped: 0.3338186740875244\n",
      "Epoch [1997/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03585\n",
      "128\n",
      "Time elasped: 0.3278696537017822\n",
      "Epoch [1998/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03529\n",
      "128\n",
      "Time elasped: 0.3368058204650879\n",
      "Epoch [1999/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03499\n",
      "128\n",
      "Time elasped: 0.3231933116912842\n",
      "Epoch [2000/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03547\n",
      "128\n",
      "Time elasped: 0.31700730323791504\n",
      "Mean Error: 0.00026961701223626733% \n",
      "--------------------------\n",
      "Epoch [2001/3000], learning_rates 0.000377, 0.377354\n",
      "Step [1/1], Loss: 0.03354\n",
      "128\n",
      "Time elasped: 0.3441603183746338\n",
      "Epoch [2002/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03362\n",
      "128\n",
      "Time elasped: 0.35787224769592285\n",
      "Epoch [2003/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03395\n",
      "128\n",
      "Time elasped: 0.3527076244354248\n",
      "Epoch [2004/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03362\n",
      "128\n",
      "Time elasped: 0.32773661613464355\n",
      "Epoch [2005/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03606\n",
      "128\n",
      "Time elasped: 0.36112046241760254\n",
      "Mean Error: 0.00025319051928818226% \n",
      "--------------------------\n",
      "Epoch [2006/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03281\n",
      "128\n",
      "Time elasped: 0.33265161514282227\n",
      "Epoch [2007/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03821\n",
      "128\n",
      "Time elasped: 0.35901594161987305\n",
      "Epoch [2008/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03198\n",
      "128\n",
      "Time elasped: 0.34123826026916504\n",
      "Epoch [2009/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03473\n",
      "128\n",
      "Time elasped: 0.3183937072753906\n",
      "Epoch [2010/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03535\n",
      "128\n",
      "Time elasped: 0.3643624782562256\n",
      "Mean Error: 0.000267975585302338% \n",
      "--------------------------\n",
      "Epoch [2011/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03405\n",
      "128\n",
      "Time elasped: 0.33001255989074707\n",
      "Epoch [2012/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03588\n",
      "128\n",
      "Time elasped: 0.3302462100982666\n",
      "Epoch [2013/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03305\n",
      "128\n",
      "Time elasped: 0.34862685203552246\n",
      "Epoch [2014/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03678\n",
      "128\n",
      "Time elasped: 0.3324909210205078\n",
      "Epoch [2015/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03279\n",
      "128\n",
      "Time elasped: 0.31987762451171875\n",
      "Mean Error: 0.00026613925001583993% \n",
      "--------------------------\n",
      "Epoch [2016/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03300\n",
      "128\n",
      "Time elasped: 0.35077953338623047\n",
      "Epoch [2017/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03695\n",
      "128\n",
      "Time elasped: 0.3429219722747803\n",
      "Epoch [2018/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03773\n",
      "128\n",
      "Time elasped: 0.32564854621887207\n",
      "Epoch [2019/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03361\n",
      "128\n",
      "Time elasped: 0.3331944942474365\n",
      "Epoch [2020/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03327\n",
      "128\n",
      "Time elasped: 0.32987308502197266\n",
      "Mean Error: 0.00028641853714361787% \n",
      "--------------------------\n",
      "Epoch [2021/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03402\n",
      "128\n",
      "Time elasped: 0.33031392097473145\n",
      "Epoch [2022/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03526\n",
      "128\n",
      "Time elasped: 0.3337082862854004\n",
      "Epoch [2023/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03609\n",
      "128\n",
      "Time elasped: 0.36429905891418457\n",
      "Epoch [2024/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03728\n",
      "128\n",
      "Time elasped: 0.3340492248535156\n",
      "Epoch [2025/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03288\n",
      "128\n",
      "Time elasped: 0.35544443130493164\n",
      "Mean Error: 0.0002706335799302906% \n",
      "--------------------------\n",
      "Epoch [2026/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03728\n",
      "128\n",
      "Time elasped: 0.34897303581237793\n",
      "Epoch [2027/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03563\n",
      "128\n",
      "Time elasped: 0.3093230724334717\n",
      "Epoch [2028/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03550\n",
      "128\n",
      "Time elasped: 0.3451402187347412\n",
      "Epoch [2029/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03407\n",
      "128\n",
      "Time elasped: 0.3394935131072998\n",
      "Epoch [2030/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03757\n",
      "128\n",
      "Time elasped: 0.34957361221313477\n",
      "Mean Error: 0.000270294287474826% \n",
      "--------------------------\n",
      "Epoch [2031/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03587\n",
      "128\n",
      "Time elasped: 0.34715700149536133\n",
      "Epoch [2032/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03413\n",
      "128\n",
      "Time elasped: 0.33176112174987793\n",
      "Epoch [2033/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03526\n",
      "128\n",
      "Time elasped: 0.3410506248474121\n",
      "Epoch [2034/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03288\n",
      "128\n",
      "Time elasped: 0.3357551097869873\n",
      "Epoch [2035/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03401\n",
      "128\n",
      "Time elasped: 0.33414220809936523\n",
      "Mean Error: 0.0002779805799946189% \n",
      "--------------------------\n",
      "Epoch [2036/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03458\n",
      "128\n",
      "Time elasped: 0.3239595890045166\n",
      "Epoch [2037/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03242\n",
      "128\n",
      "Time elasped: 0.3295779228210449\n",
      "Epoch [2038/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03195\n",
      "128\n",
      "Time elasped: 0.3385779857635498\n",
      "Epoch [2039/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03636\n",
      "128\n",
      "Time elasped: 0.33045291900634766\n",
      "Epoch [2040/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03451\n",
      "128\n",
      "Time elasped: 0.32988667488098145\n",
      "Mean Error: 0.0002505670126993209% \n",
      "--------------------------\n",
      "Epoch [2041/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03271\n",
      "128\n",
      "Time elasped: 0.3329296112060547\n",
      "Epoch [2042/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03438\n",
      "128\n",
      "Time elasped: 0.3546130657196045\n",
      "Epoch [2043/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03536\n",
      "128\n",
      "Time elasped: 0.3240516185760498\n",
      "Epoch [2044/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03051\n",
      "128\n",
      "Time elasped: 0.33678579330444336\n",
      "Epoch [2045/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03327\n",
      "128\n",
      "Time elasped: 0.346508264541626\n",
      "Mean Error: 0.00027276630862616% \n",
      "--------------------------\n",
      "Epoch [2046/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03392\n",
      "128\n",
      "Time elasped: 0.3337531089782715\n",
      "Epoch [2047/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03483\n",
      "128\n",
      "Time elasped: 0.34354424476623535\n",
      "Epoch [2048/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03264\n",
      "128\n",
      "Time elasped: 0.3507213592529297\n",
      "Epoch [2049/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03867\n",
      "128\n",
      "Time elasped: 0.34295177459716797\n",
      "Epoch [2050/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03391\n",
      "128\n",
      "Time elasped: 0.3586466312408447\n",
      "Mean Error: 0.0002821609377861023% \n",
      "--------------------------\n",
      "Epoch [2051/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03388\n",
      "128\n",
      "Time elasped: 0.32018303871154785\n",
      "Epoch [2052/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03264\n",
      "128\n",
      "Time elasped: 0.3336660861968994\n",
      "Epoch [2053/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03328\n",
      "128\n",
      "Time elasped: 0.3498263359069824\n",
      "Epoch [2054/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03450\n",
      "128\n",
      "Time elasped: 0.35654163360595703\n",
      "Epoch [2055/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03567\n",
      "128\n",
      "Time elasped: 0.3242487907409668\n",
      "Mean Error: 0.0002535968669690192% \n",
      "--------------------------\n",
      "Epoch [2056/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03236\n",
      "128\n",
      "Time elasped: 0.31180334091186523\n",
      "Epoch [2057/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03695\n",
      "128\n",
      "Time elasped: 0.33243370056152344\n",
      "Epoch [2058/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03269\n",
      "128\n",
      "Time elasped: 0.33235931396484375\n",
      "Epoch [2059/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03194\n",
      "128\n",
      "Time elasped: 0.33859777450561523\n",
      "Epoch [2060/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03352\n",
      "128\n",
      "Time elasped: 0.33641958236694336\n",
      "Mean Error: 0.0002562750887591392% \n",
      "--------------------------\n",
      "Epoch [2061/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03865\n",
      "128\n",
      "Time elasped: 0.3549995422363281\n",
      "Epoch [2062/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03308\n",
      "128\n",
      "Time elasped: 0.32950258255004883\n",
      "Epoch [2063/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03279\n",
      "128\n",
      "Time elasped: 0.3244593143463135\n",
      "Epoch [2064/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03461\n",
      "128\n",
      "Time elasped: 0.3317294120788574\n",
      "Epoch [2065/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03329\n",
      "128\n",
      "Time elasped: 0.3450641632080078\n",
      "Mean Error: 0.00026111071929335594% \n",
      "--------------------------\n",
      "Epoch [2066/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03216\n",
      "128\n",
      "Time elasped: 0.34467387199401855\n",
      "Epoch [2067/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03095\n",
      "128\n",
      "Time elasped: 0.3275113105773926\n",
      "Epoch [2068/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03412\n",
      "128\n",
      "Time elasped: 0.33266544342041016\n",
      "Epoch [2069/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03542\n",
      "128\n",
      "Time elasped: 0.32020068168640137\n",
      "Epoch [2070/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03453\n",
      "128\n",
      "Time elasped: 0.3460090160369873\n",
      "Mean Error: 0.0002777618356049061% \n",
      "--------------------------\n",
      "Epoch [2071/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03443\n",
      "128\n",
      "Time elasped: 0.34920287132263184\n",
      "Epoch [2072/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03494\n",
      "128\n",
      "Time elasped: 0.3588578701019287\n",
      "Epoch [2073/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03242\n",
      "128\n",
      "Time elasped: 0.3320279121398926\n",
      "Epoch [2074/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03383\n",
      "128\n",
      "Time elasped: 0.32686638832092285\n",
      "Epoch [2075/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03508\n",
      "128\n",
      "Time elasped: 0.3301718235015869\n",
      "Mean Error: 0.0002735932939685881% \n",
      "--------------------------\n",
      "Epoch [2076/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03621\n",
      "128\n",
      "Time elasped: 0.34772443771362305\n",
      "Epoch [2077/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03487\n",
      "128\n",
      "Time elasped: 0.32500386238098145\n",
      "Epoch [2078/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03442\n",
      "128\n",
      "Time elasped: 0.3855772018432617\n",
      "Epoch [2079/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03115\n",
      "128\n",
      "Time elasped: 0.3407933712005615\n",
      "Epoch [2080/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03413\n",
      "128\n",
      "Time elasped: 0.32524609565734863\n",
      "Mean Error: 0.00027151964604854584% \n",
      "--------------------------\n",
      "Epoch [2081/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03185\n",
      "128\n",
      "Time elasped: 0.34671473503112793\n",
      "Epoch [2082/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03519\n",
      "128\n",
      "Time elasped: 0.3322291374206543\n",
      "Epoch [2083/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03228\n",
      "128\n",
      "Time elasped: 0.3325467109680176\n",
      "Epoch [2084/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03357\n",
      "128\n",
      "Time elasped: 0.3464515209197998\n",
      "Epoch [2085/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03371\n",
      "128\n",
      "Time elasped: 0.34539341926574707\n",
      "Mean Error: 0.00023401298676617444% \n",
      "--------------------------\n",
      "Epoch [2086/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03052\n",
      "128\n",
      "Time elasped: 0.3273794651031494\n",
      "Epoch [2087/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03243\n",
      "128\n",
      "Time elasped: 0.3256080150604248\n",
      "Epoch [2088/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03304\n",
      "128\n",
      "Time elasped: 0.3475651741027832\n",
      "Epoch [2089/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03382\n",
      "128\n",
      "Time elasped: 0.33750057220458984\n",
      "Epoch [2090/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03269\n",
      "128\n",
      "Time elasped: 0.3758111000061035\n",
      "Mean Error: 0.0002628282527439296% \n",
      "--------------------------\n",
      "Epoch [2091/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03559\n",
      "128\n",
      "Time elasped: 0.3629150390625\n",
      "Epoch [2092/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03371\n",
      "128\n",
      "Time elasped: 0.3304014205932617\n",
      "Epoch [2093/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03428\n",
      "128\n",
      "Time elasped: 0.34343814849853516\n",
      "Epoch [2094/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03306\n",
      "128\n",
      "Time elasped: 0.34691810607910156\n",
      "Epoch [2095/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03564\n",
      "128\n",
      "Time elasped: 0.3376631736755371\n",
      "Mean Error: 0.00027647241950035095% \n",
      "--------------------------\n",
      "Epoch [2096/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03727\n",
      "128\n",
      "Time elasped: 0.3410351276397705\n",
      "Epoch [2097/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03620\n",
      "128\n",
      "Time elasped: 0.36817288398742676\n",
      "Epoch [2098/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03502\n",
      "128\n",
      "Time elasped: 0.3355598449707031\n",
      "Epoch [2099/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03105\n",
      "128\n",
      "Time elasped: 0.3492462635040283\n",
      "Epoch [2100/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03408\n",
      "128\n",
      "Time elasped: 0.34194231033325195\n",
      "Mean Error: 0.0002582011220511049% \n",
      "--------------------------\n",
      "Epoch [2101/3000], learning_rates 0.000358, 0.358486\n",
      "Step [1/1], Loss: 0.03072\n",
      "128\n",
      "Time elasped: 0.3383781909942627\n",
      "Epoch [2102/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03206\n",
      "128\n",
      "Time elasped: 0.33873677253723145\n",
      "Epoch [2103/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03596\n",
      "128\n",
      "Time elasped: 0.3435797691345215\n",
      "Epoch [2104/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03159\n",
      "128\n",
      "Time elasped: 0.3741309642791748\n",
      "Epoch [2105/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03517\n",
      "128\n",
      "Time elasped: 0.34507107734680176\n",
      "Mean Error: 0.0002699823526199907% \n",
      "--------------------------\n",
      "Epoch [2106/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03490\n",
      "128\n",
      "Time elasped: 0.33648085594177246\n",
      "Epoch [2107/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03210\n",
      "128\n",
      "Time elasped: 0.3297560214996338\n",
      "Epoch [2108/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03218\n",
      "128\n",
      "Time elasped: 0.3285548686981201\n",
      "Epoch [2109/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03409\n",
      "128\n",
      "Time elasped: 0.3492591381072998\n",
      "Epoch [2110/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03177\n",
      "128\n",
      "Time elasped: 0.3284754753112793\n",
      "Mean Error: 0.0002731506247073412% \n",
      "--------------------------\n",
      "Epoch [2111/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03030\n",
      "128\n",
      "Time elasped: 0.3262746334075928\n",
      "Epoch [2112/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03447\n",
      "128\n",
      "Time elasped: 0.34296464920043945\n",
      "Epoch [2113/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03309\n",
      "128\n",
      "Time elasped: 0.3428211212158203\n",
      "Epoch [2114/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03490\n",
      "128\n",
      "Time elasped: 0.3496077060699463\n",
      "Epoch [2115/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03796\n",
      "128\n",
      "Time elasped: 0.32868361473083496\n",
      "Mean Error: 0.0002599629224278033% \n",
      "--------------------------\n",
      "Epoch [2116/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03143\n",
      "128\n",
      "Time elasped: 0.3209514617919922\n",
      "Epoch [2117/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03213\n",
      "128\n",
      "Time elasped: 0.333834171295166\n",
      "Epoch [2118/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03329\n",
      "128\n",
      "Time elasped: 0.3185293674468994\n",
      "Epoch [2119/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03199\n",
      "128\n",
      "Time elasped: 0.35193538665771484\n",
      "Epoch [2120/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03327\n",
      "128\n",
      "Time elasped: 0.33963537216186523\n",
      "Mean Error: 0.0002488082100171596% \n",
      "--------------------------\n",
      "Epoch [2121/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03164\n",
      "128\n",
      "Time elasped: 0.33590197563171387\n",
      "Epoch [2122/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03565\n",
      "128\n",
      "Time elasped: 0.3536701202392578\n",
      "Epoch [2123/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03316\n",
      "128\n",
      "Time elasped: 0.3293287754058838\n",
      "Epoch [2124/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03491\n",
      "128\n",
      "Time elasped: 0.32164692878723145\n",
      "Epoch [2125/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03073\n",
      "128\n",
      "Time elasped: 0.34105610847473145\n",
      "Mean Error: 0.000254743208643049% \n",
      "--------------------------\n",
      "Epoch [2126/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02964\n",
      "128\n",
      "Time elasped: 0.33630847930908203\n",
      "Epoch [2127/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03517\n",
      "128\n",
      "Time elasped: 0.3416764736175537\n",
      "Epoch [2128/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03479\n",
      "128\n",
      "Time elasped: 0.31904125213623047\n",
      "Epoch [2129/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03272\n",
      "128\n",
      "Time elasped: 0.32830071449279785\n",
      "Epoch [2130/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03119\n",
      "128\n",
      "Time elasped: 0.35403943061828613\n",
      "Mean Error: 0.0002595086698420346% \n",
      "--------------------------\n",
      "Epoch [2131/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03114\n",
      "128\n",
      "Time elasped: 0.32285594940185547\n",
      "Epoch [2132/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03164\n",
      "128\n",
      "Time elasped: 0.33571743965148926\n",
      "Epoch [2133/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03606\n",
      "128\n",
      "Time elasped: 0.35779786109924316\n",
      "Epoch [2134/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02947\n",
      "128\n",
      "Time elasped: 0.36011338233947754\n",
      "Epoch [2135/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03254\n",
      "128\n",
      "Time elasped: 0.3444337844848633\n",
      "Mean Error: 0.00026002447702921927% \n",
      "--------------------------\n",
      "Epoch [2136/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03226\n",
      "128\n",
      "Time elasped: 0.36152005195617676\n",
      "Epoch [2137/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03210\n",
      "128\n",
      "Time elasped: 0.33637332916259766\n",
      "Epoch [2138/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03444\n",
      "128\n",
      "Time elasped: 0.3326234817504883\n",
      "Epoch [2139/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03397\n",
      "128\n",
      "Time elasped: 0.3440666198730469\n",
      "Epoch [2140/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03657\n",
      "128\n",
      "Time elasped: 0.34137415885925293\n",
      "Mean Error: 0.00024043291341513395% \n",
      "--------------------------\n",
      "Epoch [2141/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03022\n",
      "128\n",
      "Time elasped: 0.3870704174041748\n",
      "Epoch [2142/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03430\n",
      "128\n",
      "Time elasped: 0.34735918045043945\n",
      "Epoch [2143/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03487\n",
      "128\n",
      "Time elasped: 0.3224833011627197\n",
      "Epoch [2144/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02965\n",
      "128\n",
      "Time elasped: 0.3521230220794678\n",
      "Epoch [2145/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03160\n",
      "128\n",
      "Time elasped: 0.3328227996826172\n",
      "Mean Error: 0.0002589977812021971% \n",
      "--------------------------\n",
      "Epoch [2146/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03266\n",
      "128\n",
      "Time elasped: 0.3398706912994385\n",
      "Epoch [2147/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03443\n",
      "128\n",
      "Time elasped: 0.3499782085418701\n",
      "Epoch [2148/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03160\n",
      "128\n",
      "Time elasped: 0.32131075859069824\n",
      "Epoch [2149/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03467\n",
      "128\n",
      "Time elasped: 0.3598909378051758\n",
      "Epoch [2150/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02949\n",
      "128\n",
      "Time elasped: 0.33635711669921875\n",
      "Mean Error: 0.0002783711242955178% \n",
      "--------------------------\n",
      "Epoch [2151/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03260\n",
      "128\n",
      "Time elasped: 0.3359375\n",
      "Epoch [2152/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03425\n",
      "128\n",
      "Time elasped: 0.38827037811279297\n",
      "Epoch [2153/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03364\n",
      "128\n",
      "Time elasped: 0.32149791717529297\n",
      "Epoch [2154/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03484\n",
      "128\n",
      "Time elasped: 0.3182964324951172\n",
      "Epoch [2155/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03099\n",
      "128\n",
      "Time elasped: 0.34095001220703125\n",
      "Mean Error: 0.0002662917540874332% \n",
      "--------------------------\n",
      "Epoch [2156/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03259\n",
      "128\n",
      "Time elasped: 0.3351936340332031\n",
      "Epoch [2157/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03115\n",
      "128\n",
      "Time elasped: 0.3369760513305664\n",
      "Epoch [2158/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03231\n",
      "128\n",
      "Time elasped: 0.33150625228881836\n",
      "Epoch [2159/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03317\n",
      "128\n",
      "Time elasped: 0.3550868034362793\n",
      "Epoch [2160/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03409\n",
      "128\n",
      "Time elasped: 0.35806798934936523\n",
      "Mean Error: 0.0002612703538034111% \n",
      "--------------------------\n",
      "Epoch [2161/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03169\n",
      "128\n",
      "Time elasped: 0.3320786952972412\n",
      "Epoch [2162/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02959\n",
      "128\n",
      "Time elasped: 0.33133721351623535\n",
      "Epoch [2163/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03557\n",
      "128\n",
      "Time elasped: 0.33937525749206543\n",
      "Epoch [2164/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03217\n",
      "128\n",
      "Time elasped: 0.33466553688049316\n",
      "Epoch [2165/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03283\n",
      "128\n",
      "Time elasped: 0.34669041633605957\n",
      "Mean Error: 0.0002373716124566272% \n",
      "--------------------------\n",
      "Epoch [2166/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03308\n",
      "128\n",
      "Time elasped: 0.3409430980682373\n",
      "Epoch [2167/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03269\n",
      "128\n",
      "Time elasped: 0.35485124588012695\n",
      "Epoch [2168/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03248\n",
      "128\n",
      "Time elasped: 0.36233949661254883\n",
      "Epoch [2169/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02958\n",
      "128\n",
      "Time elasped: 0.34277963638305664\n",
      "Epoch [2170/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03135\n",
      "128\n",
      "Time elasped: 0.3452622890472412\n",
      "Mean Error: 0.0002578182320576161% \n",
      "--------------------------\n",
      "Epoch [2171/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03071\n",
      "128\n",
      "Time elasped: 0.3273470401763916\n",
      "Epoch [2172/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03201\n",
      "128\n",
      "Time elasped: 0.332627534866333\n",
      "Epoch [2173/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03241\n",
      "128\n",
      "Time elasped: 0.3513529300689697\n",
      "Epoch [2174/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03104\n",
      "128\n",
      "Time elasped: 0.3438246250152588\n",
      "Epoch [2175/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03150\n",
      "128\n",
      "Time elasped: 0.37744688987731934\n",
      "Mean Error: 0.00025820438168011606% \n",
      "--------------------------\n",
      "Epoch [2176/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03332\n",
      "128\n",
      "Time elasped: 0.33017873764038086\n",
      "Epoch [2177/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03177\n",
      "128\n",
      "Time elasped: 0.34738898277282715\n",
      "Epoch [2178/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03041\n",
      "128\n",
      "Time elasped: 0.3249330520629883\n",
      "Epoch [2179/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03232\n",
      "128\n",
      "Time elasped: 0.37115931510925293\n",
      "Epoch [2180/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03286\n",
      "128\n",
      "Time elasped: 0.338700532913208\n",
      "Mean Error: 0.00024851851048879325% \n",
      "--------------------------\n",
      "Epoch [2181/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03115\n",
      "128\n",
      "Time elasped: 0.3291635513305664\n",
      "Epoch [2182/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03369\n",
      "128\n",
      "Time elasped: 0.35063600540161133\n",
      "Epoch [2183/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03338\n",
      "128\n",
      "Time elasped: 0.34583091735839844\n",
      "Epoch [2184/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.02845\n",
      "128\n",
      "Time elasped: 0.33768463134765625\n",
      "Epoch [2185/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03236\n",
      "128\n",
      "Time elasped: 0.36307430267333984\n",
      "Mean Error: 0.00026642324519343674% \n",
      "--------------------------\n",
      "Epoch [2186/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03226\n",
      "128\n",
      "Time elasped: 0.36444592475891113\n",
      "Epoch [2187/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03361\n",
      "128\n",
      "Time elasped: 0.3482401371002197\n",
      "Epoch [2188/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03295\n",
      "128\n",
      "Time elasped: 0.3553962707519531\n",
      "Epoch [2189/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03234\n",
      "128\n",
      "Time elasped: 0.3351116180419922\n",
      "Epoch [2190/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03009\n",
      "128\n",
      "Time elasped: 0.33288097381591797\n",
      "Mean Error: 0.00025046433438546956% \n",
      "--------------------------\n",
      "Epoch [2191/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03437\n",
      "128\n",
      "Time elasped: 0.33632850646972656\n",
      "Epoch [2192/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03143\n",
      "128\n",
      "Time elasped: 0.34425830841064453\n",
      "Epoch [2193/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03438\n",
      "128\n",
      "Time elasped: 0.3256564140319824\n",
      "Epoch [2194/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03084\n",
      "128\n",
      "Time elasped: 0.3336000442504883\n",
      "Epoch [2195/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03172\n",
      "128\n",
      "Time elasped: 0.35807251930236816\n",
      "Mean Error: 0.00026577230892144144% \n",
      "--------------------------\n",
      "Epoch [2196/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03095\n",
      "128\n",
      "Time elasped: 0.33095717430114746\n",
      "Epoch [2197/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03491\n",
      "128\n",
      "Time elasped: 0.34503674507141113\n",
      "Epoch [2198/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03210\n",
      "128\n",
      "Time elasped: 0.34188365936279297\n",
      "Epoch [2199/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03266\n",
      "128\n",
      "Time elasped: 0.3417165279388428\n",
      "Epoch [2200/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03309\n",
      "128\n",
      "Time elasped: 0.3288614749908447\n",
      "Mean Error: 0.00026443207752890885% \n",
      "--------------------------\n",
      "Epoch [2201/3000], learning_rates 0.000341, 0.340562\n",
      "Step [1/1], Loss: 0.03586\n",
      "128\n",
      "Time elasped: 0.34381961822509766\n",
      "Epoch [2202/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03291\n",
      "128\n",
      "Time elasped: 0.34038400650024414\n",
      "Epoch [2203/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03589\n",
      "128\n",
      "Time elasped: 0.3384416103363037\n",
      "Epoch [2204/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03215\n",
      "128\n",
      "Time elasped: 0.35228824615478516\n",
      "Epoch [2205/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03186\n",
      "128\n",
      "Time elasped: 0.34385180473327637\n",
      "Mean Error: 0.0002550526987761259% \n",
      "--------------------------\n",
      "Epoch [2206/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03415\n",
      "128\n",
      "Time elasped: 0.3321712017059326\n",
      "Epoch [2207/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03353\n",
      "128\n",
      "Time elasped: 0.35509538650512695\n",
      "Epoch [2208/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03138\n",
      "128\n",
      "Time elasped: 0.3265106678009033\n",
      "Epoch [2209/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03531\n",
      "128\n",
      "Time elasped: 0.3341972827911377\n",
      "Epoch [2210/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03668\n",
      "128\n",
      "Time elasped: 0.343813419342041\n",
      "Mean Error: 0.000268676521955058% \n",
      "--------------------------\n",
      "Epoch [2211/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03397\n",
      "128\n",
      "Time elasped: 0.35247349739074707\n",
      "Epoch [2212/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03236\n",
      "128\n",
      "Time elasped: 0.34363889694213867\n",
      "Epoch [2213/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02910\n",
      "128\n",
      "Time elasped: 0.3534688949584961\n",
      "Epoch [2214/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03235\n",
      "128\n",
      "Time elasped: 0.36205148696899414\n",
      "Epoch [2215/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03401\n",
      "128\n",
      "Time elasped: 0.34579944610595703\n",
      "Mean Error: 0.0002512661158107221% \n",
      "--------------------------\n",
      "Epoch [2216/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03455\n",
      "128\n",
      "Time elasped: 0.3368237018585205\n",
      "Epoch [2217/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03244\n",
      "128\n",
      "Time elasped: 0.34061264991760254\n",
      "Epoch [2218/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03185\n",
      "128\n",
      "Time elasped: 0.3328406810760498\n",
      "Epoch [2219/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03409\n",
      "128\n",
      "Time elasped: 0.3380467891693115\n",
      "Epoch [2220/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03549\n",
      "128\n",
      "Time elasped: 0.3203158378601074\n",
      "Mean Error: 0.00023578900436405092% \n",
      "--------------------------\n",
      "Epoch [2221/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03200\n",
      "128\n",
      "Time elasped: 0.3681938648223877\n",
      "Epoch [2222/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03195\n",
      "128\n",
      "Time elasped: 0.3478422164916992\n",
      "Epoch [2223/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03550\n",
      "128\n",
      "Time elasped: 0.35413575172424316\n",
      "Epoch [2224/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03211\n",
      "128\n",
      "Time elasped: 0.4562537670135498\n",
      "Epoch [2225/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03283\n",
      "128\n",
      "Time elasped: 0.3782327175140381\n",
      "Mean Error: 0.0002602995955385268% \n",
      "--------------------------\n",
      "Epoch [2226/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03489\n",
      "128\n",
      "Time elasped: 0.3666224479675293\n",
      "Epoch [2227/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03682\n",
      "128\n",
      "Time elasped: 0.3513908386230469\n",
      "Epoch [2228/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03285\n",
      "128\n",
      "Time elasped: 0.37425756454467773\n",
      "Epoch [2229/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03206\n",
      "128\n",
      "Time elasped: 0.3929469585418701\n",
      "Epoch [2230/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03362\n",
      "128\n",
      "Time elasped: 0.38541364669799805\n",
      "Mean Error: 0.00026950021856464446% \n",
      "--------------------------\n",
      "Epoch [2231/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03330\n",
      "128\n",
      "Time elasped: 0.3968970775604248\n",
      "Epoch [2232/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03470\n",
      "128\n",
      "Time elasped: 0.36726856231689453\n",
      "Epoch [2233/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02865\n",
      "128\n",
      "Time elasped: 0.36108946800231934\n",
      "Epoch [2234/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03013\n",
      "128\n",
      "Time elasped: 0.35595154762268066\n",
      "Epoch [2235/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03151\n",
      "128\n",
      "Time elasped: 0.3486812114715576\n",
      "Mean Error: 0.000269861426204443% \n",
      "--------------------------\n",
      "Epoch [2236/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03138\n",
      "128\n",
      "Time elasped: 0.34403276443481445\n",
      "Epoch [2237/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03279\n",
      "128\n",
      "Time elasped: 0.33771753311157227\n",
      "Epoch [2238/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03171\n",
      "128\n",
      "Time elasped: 0.3493804931640625\n",
      "Epoch [2239/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02961\n",
      "128\n",
      "Time elasped: 0.39532899856567383\n",
      "Epoch [2240/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03537\n",
      "128\n",
      "Time elasped: 0.3410167694091797\n",
      "Mean Error: 0.00023979348770808429% \n",
      "--------------------------\n",
      "Epoch [2241/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03134\n",
      "128\n",
      "Time elasped: 0.3359076976776123\n",
      "Epoch [2242/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03385\n",
      "128\n",
      "Time elasped: 0.339963436126709\n",
      "Epoch [2243/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03167\n",
      "128\n",
      "Time elasped: 0.3520834445953369\n",
      "Epoch [2244/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02973\n",
      "128\n",
      "Time elasped: 0.34498167037963867\n",
      "Epoch [2245/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03242\n",
      "128\n",
      "Time elasped: 0.352278470993042\n",
      "Mean Error: 0.00023715752467978746% \n",
      "--------------------------\n",
      "Epoch [2246/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03139\n",
      "128\n",
      "Time elasped: 0.34932422637939453\n",
      "Epoch [2247/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03089\n",
      "128\n",
      "Time elasped: 0.3460679054260254\n",
      "Epoch [2248/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02823\n",
      "128\n",
      "Time elasped: 0.3495931625366211\n",
      "Epoch [2249/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03076\n",
      "128\n",
      "Time elasped: 0.3667125701904297\n",
      "Epoch [2250/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03034\n",
      "128\n",
      "Time elasped: 0.3447725772857666\n",
      "Mean Error: 0.0002585675101727247% \n",
      "--------------------------\n",
      "Epoch [2251/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03286\n",
      "128\n",
      "Time elasped: 0.3446168899536133\n",
      "Epoch [2252/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03046\n",
      "128\n",
      "Time elasped: 0.32700490951538086\n",
      "Epoch [2253/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03166\n",
      "128\n",
      "Time elasped: 0.34592771530151367\n",
      "Epoch [2254/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02926\n",
      "128\n",
      "Time elasped: 0.3663468360900879\n",
      "Epoch [2255/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03189\n",
      "128\n",
      "Time elasped: 0.3998260498046875\n",
      "Mean Error: 0.0002749167615547776% \n",
      "--------------------------\n",
      "Epoch [2256/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03036\n",
      "128\n",
      "Time elasped: 0.3378922939300537\n",
      "Epoch [2257/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03146\n",
      "128\n",
      "Time elasped: 0.3614175319671631\n",
      "Epoch [2258/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03185\n",
      "128\n",
      "Time elasped: 0.3446941375732422\n",
      "Epoch [2259/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03284\n",
      "128\n",
      "Time elasped: 0.3728313446044922\n",
      "Epoch [2260/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03092\n",
      "128\n",
      "Time elasped: 0.3536193370819092\n",
      "Mean Error: 0.00027160203899256885% \n",
      "--------------------------\n",
      "Epoch [2261/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03123\n",
      "128\n",
      "Time elasped: 0.36335253715515137\n",
      "Epoch [2262/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03334\n",
      "128\n",
      "Time elasped: 0.3589653968811035\n",
      "Epoch [2263/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03280\n",
      "128\n",
      "Time elasped: 0.33717870712280273\n",
      "Epoch [2264/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02996\n",
      "128\n",
      "Time elasped: 0.34418535232543945\n",
      "Epoch [2265/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03028\n",
      "128\n",
      "Time elasped: 0.32907938957214355\n",
      "Mean Error: 0.00023788392718415707% \n",
      "--------------------------\n",
      "Epoch [2266/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02845\n",
      "128\n",
      "Time elasped: 0.3279998302459717\n",
      "Epoch [2267/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03186\n",
      "128\n",
      "Time elasped: 0.35828280448913574\n",
      "Epoch [2268/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03127\n",
      "128\n",
      "Time elasped: 0.3405895233154297\n",
      "Epoch [2269/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03405\n",
      "128\n",
      "Time elasped: 0.3561744689941406\n",
      "Epoch [2270/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03193\n",
      "128\n",
      "Time elasped: 0.3572685718536377\n",
      "Mean Error: 0.0002470742620062083% \n",
      "--------------------------\n",
      "Epoch [2271/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03351\n",
      "128\n",
      "Time elasped: 0.35216760635375977\n",
      "Epoch [2272/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03409\n",
      "128\n",
      "Time elasped: 0.3355293273925781\n",
      "Epoch [2273/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03418\n",
      "128\n",
      "Time elasped: 0.3422534465789795\n",
      "Epoch [2274/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02818\n",
      "128\n",
      "Time elasped: 0.34755682945251465\n",
      "Epoch [2275/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03029\n",
      "128\n",
      "Time elasped: 0.3210289478302002\n",
      "Mean Error: 0.0002426819410175085% \n",
      "--------------------------\n",
      "Epoch [2276/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03256\n",
      "128\n",
      "Time elasped: 0.33611106872558594\n",
      "Epoch [2277/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03157\n",
      "128\n",
      "Time elasped: 0.31542253494262695\n",
      "Epoch [2278/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03027\n",
      "128\n",
      "Time elasped: 0.3462984561920166\n",
      "Epoch [2279/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03215\n",
      "128\n",
      "Time elasped: 0.3467133045196533\n",
      "Epoch [2280/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03335\n",
      "128\n",
      "Time elasped: 0.3447539806365967\n",
      "Mean Error: 0.00024180496984627098% \n",
      "--------------------------\n",
      "Epoch [2281/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03144\n",
      "128\n",
      "Time elasped: 0.3440725803375244\n",
      "Epoch [2282/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03144\n",
      "128\n",
      "Time elasped: 0.3180079460144043\n",
      "Epoch [2283/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03101\n",
      "128\n",
      "Time elasped: 0.35661911964416504\n",
      "Epoch [2284/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03082\n",
      "128\n",
      "Time elasped: 0.3360440731048584\n",
      "Epoch [2285/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03287\n",
      "128\n",
      "Time elasped: 0.3648991584777832\n",
      "Mean Error: 0.00024669512640684843% \n",
      "--------------------------\n",
      "Epoch [2286/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03077\n",
      "128\n",
      "Time elasped: 0.3371584415435791\n",
      "Epoch [2287/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03119\n",
      "128\n",
      "Time elasped: 0.32419896125793457\n",
      "Epoch [2288/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02945\n",
      "128\n",
      "Time elasped: 0.3318297863006592\n",
      "Epoch [2289/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03020\n",
      "128\n",
      "Time elasped: 0.37192511558532715\n",
      "Epoch [2290/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03225\n",
      "128\n",
      "Time elasped: 0.3237433433532715\n",
      "Mean Error: 0.00023506283469032496% \n",
      "--------------------------\n",
      "Epoch [2291/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03250\n",
      "128\n",
      "Time elasped: 0.35866332054138184\n",
      "Epoch [2292/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03048\n",
      "128\n",
      "Time elasped: 0.33144450187683105\n",
      "Epoch [2293/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03098\n",
      "128\n",
      "Time elasped: 0.3443264961242676\n",
      "Epoch [2294/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03403\n",
      "128\n",
      "Time elasped: 0.3690204620361328\n",
      "Epoch [2295/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03232\n",
      "128\n",
      "Time elasped: 0.3643531799316406\n",
      "Mean Error: 0.00024286608095280826% \n",
      "--------------------------\n",
      "Epoch [2296/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02919\n",
      "128\n",
      "Time elasped: 0.35190916061401367\n",
      "Epoch [2297/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03154\n",
      "128\n",
      "Time elasped: 0.3275909423828125\n",
      "Epoch [2298/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03440\n",
      "128\n",
      "Time elasped: 0.3401477336883545\n",
      "Epoch [2299/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03362\n",
      "128\n",
      "Time elasped: 0.3454751968383789\n",
      "Epoch [2300/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.02929\n",
      "128\n",
      "Time elasped: 0.3532698154449463\n",
      "Mean Error: 0.0002628933871164918% \n",
      "--------------------------\n",
      "Epoch [2301/3000], learning_rates 0.000324, 0.323534\n",
      "Step [1/1], Loss: 0.03372\n",
      "128\n",
      "Time elasped: 0.3313932418823242\n",
      "Epoch [2302/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03387\n",
      "128\n",
      "Time elasped: 0.34394288063049316\n",
      "Epoch [2303/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03379\n",
      "128\n",
      "Time elasped: 0.3175783157348633\n",
      "Epoch [2304/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03159\n",
      "128\n",
      "Time elasped: 0.3727121353149414\n",
      "Epoch [2305/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03293\n",
      "128\n",
      "Time elasped: 0.3356359004974365\n",
      "Mean Error: 0.0002473727217875421% \n",
      "--------------------------\n",
      "Epoch [2306/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03194\n",
      "128\n",
      "Time elasped: 0.33199119567871094\n",
      "Epoch [2307/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03180\n",
      "128\n",
      "Time elasped: 0.3345158100128174\n",
      "Epoch [2308/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02854\n",
      "128\n",
      "Time elasped: 0.33878445625305176\n",
      "Epoch [2309/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02913\n",
      "128\n",
      "Time elasped: 0.34827160835266113\n",
      "Epoch [2310/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03429\n",
      "128\n",
      "Time elasped: 0.3502197265625\n",
      "Mean Error: 0.0002686399093363434% \n",
      "--------------------------\n",
      "Epoch [2311/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03129\n",
      "128\n",
      "Time elasped: 0.3280768394470215\n",
      "Epoch [2312/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03175\n",
      "128\n",
      "Time elasped: 0.34023308753967285\n",
      "Epoch [2313/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03008\n",
      "128\n",
      "Time elasped: 0.3331642150878906\n",
      "Epoch [2314/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02911\n",
      "128\n",
      "Time elasped: 0.3305680751800537\n",
      "Epoch [2315/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03143\n",
      "128\n",
      "Time elasped: 0.33286571502685547\n",
      "Mean Error: 0.0002269232354592532% \n",
      "--------------------------\n",
      "Epoch [2316/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03102\n",
      "128\n",
      "Time elasped: 0.32766079902648926\n",
      "Epoch [2317/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03138\n",
      "128\n",
      "Time elasped: 0.3613886833190918\n",
      "Epoch [2318/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03483\n",
      "128\n",
      "Time elasped: 0.3416931629180908\n",
      "Epoch [2319/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03189\n",
      "128\n",
      "Time elasped: 0.34371519088745117\n",
      "Epoch [2320/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03132\n",
      "128\n",
      "Time elasped: 0.35210657119750977\n",
      "Mean Error: 0.0002610574592836201% \n",
      "--------------------------\n",
      "Epoch [2321/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03257\n",
      "128\n",
      "Time elasped: 0.3374600410461426\n",
      "Epoch [2322/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03284\n",
      "128\n",
      "Time elasped: 0.34060025215148926\n",
      "Epoch [2323/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03219\n",
      "128\n",
      "Time elasped: 0.33229994773864746\n",
      "Epoch [2324/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03195\n",
      "128\n",
      "Time elasped: 0.35636305809020996\n",
      "Epoch [2325/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02917\n",
      "128\n",
      "Time elasped: 0.32505345344543457\n",
      "Mean Error: 0.0002489681646693498% \n",
      "--------------------------\n",
      "Epoch [2326/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03000\n",
      "128\n",
      "Time elasped: 0.32945942878723145\n",
      "Epoch [2327/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02836\n",
      "128\n",
      "Time elasped: 0.33203125\n",
      "Epoch [2328/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02819\n",
      "128\n",
      "Time elasped: 0.33182406425476074\n",
      "Epoch [2329/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02937\n",
      "128\n",
      "Time elasped: 0.33296871185302734\n",
      "Epoch [2330/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02972\n",
      "128\n",
      "Time elasped: 0.3233466148376465\n",
      "Mean Error: 0.0002467148005962372% \n",
      "--------------------------\n",
      "Epoch [2331/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02826\n",
      "128\n",
      "Time elasped: 0.3327631950378418\n",
      "Epoch [2332/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03126\n",
      "128\n",
      "Time elasped: 0.3474104404449463\n",
      "Epoch [2333/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03169\n",
      "128\n",
      "Time elasped: 0.4042539596557617\n",
      "Epoch [2334/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03120\n",
      "128\n",
      "Time elasped: 0.3530881404876709\n",
      "Epoch [2335/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03366\n",
      "128\n",
      "Time elasped: 0.33928465843200684\n",
      "Mean Error: 0.0002485256700310856% \n",
      "--------------------------\n",
      "Epoch [2336/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03229\n",
      "128\n",
      "Time elasped: 0.3745439052581787\n",
      "Epoch [2337/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02907\n",
      "128\n",
      "Time elasped: 0.4044308662414551\n",
      "Epoch [2338/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03400\n",
      "128\n",
      "Time elasped: 0.3640766143798828\n",
      "Epoch [2339/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03130\n",
      "128\n",
      "Time elasped: 0.37030720710754395\n",
      "Epoch [2340/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03039\n",
      "128\n",
      "Time elasped: 0.33292508125305176\n",
      "Mean Error: 0.00024454633239656687% \n",
      "--------------------------\n",
      "Epoch [2341/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03129\n",
      "128\n",
      "Time elasped: 0.3236370086669922\n",
      "Epoch [2342/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03037\n",
      "128\n",
      "Time elasped: 0.34275102615356445\n",
      "Epoch [2343/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02932\n",
      "128\n",
      "Time elasped: 0.3376655578613281\n",
      "Epoch [2344/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03235\n",
      "128\n",
      "Time elasped: 0.36842942237854004\n",
      "Epoch [2345/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03319\n",
      "128\n",
      "Time elasped: 0.3639211654663086\n",
      "Mean Error: 0.0002425133716315031% \n",
      "--------------------------\n",
      "Epoch [2346/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03455\n",
      "128\n",
      "Time elasped: 0.3474159240722656\n",
      "Epoch [2347/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03060\n",
      "128\n",
      "Time elasped: 0.33790016174316406\n",
      "Epoch [2348/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03119\n",
      "128\n",
      "Time elasped: 0.32398438453674316\n",
      "Epoch [2349/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03225\n",
      "128\n",
      "Time elasped: 0.36020994186401367\n",
      "Epoch [2350/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03041\n",
      "128\n",
      "Time elasped: 0.36105799674987793\n",
      "Mean Error: 0.00024223428044933826% \n",
      "--------------------------\n",
      "Epoch [2351/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03077\n",
      "128\n",
      "Time elasped: 0.3512389659881592\n",
      "Epoch [2352/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03377\n",
      "128\n",
      "Time elasped: 0.35494518280029297\n",
      "Epoch [2353/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03237\n",
      "128\n",
      "Time elasped: 0.3436136245727539\n",
      "Epoch [2354/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03290\n",
      "128\n",
      "Time elasped: 0.3661346435546875\n",
      "Epoch [2355/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03225\n",
      "128\n",
      "Time elasped: 0.32883644104003906\n",
      "Mean Error: 0.00024280585057567805% \n",
      "--------------------------\n",
      "Epoch [2356/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03437\n",
      "128\n",
      "Time elasped: 0.35375523567199707\n",
      "Epoch [2357/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03279\n",
      "128\n",
      "Time elasped: 0.35225462913513184\n",
      "Epoch [2358/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03089\n",
      "128\n",
      "Time elasped: 0.31750941276550293\n",
      "Epoch [2359/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03448\n",
      "128\n",
      "Time elasped: 0.3290271759033203\n",
      "Epoch [2360/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02979\n",
      "128\n",
      "Time elasped: 0.3448474407196045\n",
      "Mean Error: 0.0002542107831686735% \n",
      "--------------------------\n",
      "Epoch [2361/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02956\n",
      "128\n",
      "Time elasped: 0.3428926467895508\n",
      "Epoch [2362/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03202\n",
      "128\n",
      "Time elasped: 0.3280518054962158\n",
      "Epoch [2363/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03119\n",
      "128\n",
      "Time elasped: 0.33768391609191895\n",
      "Epoch [2364/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03243\n",
      "128\n",
      "Time elasped: 0.3371922969818115\n",
      "Epoch [2365/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02897\n",
      "128\n",
      "Time elasped: 0.3308732509613037\n",
      "Mean Error: 0.0002459163952153176% \n",
      "--------------------------\n",
      "Epoch [2366/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03038\n",
      "128\n",
      "Time elasped: 0.35251402854919434\n",
      "Epoch [2367/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03373\n",
      "128\n",
      "Time elasped: 0.3248567581176758\n",
      "Epoch [2368/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03236\n",
      "128\n",
      "Time elasped: 0.33342933654785156\n",
      "Epoch [2369/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03178\n",
      "128\n",
      "Time elasped: 0.3612961769104004\n",
      "Epoch [2370/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03126\n",
      "128\n",
      "Time elasped: 0.3607785701751709\n",
      "Mean Error: 0.0002251916448585689% \n",
      "--------------------------\n",
      "Epoch [2371/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03090\n",
      "128\n",
      "Time elasped: 0.322005033493042\n",
      "Epoch [2372/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03070\n",
      "128\n",
      "Time elasped: 0.3288404941558838\n",
      "Epoch [2373/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03015\n",
      "128\n",
      "Time elasped: 0.33156847953796387\n",
      "Epoch [2374/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03247\n",
      "128\n",
      "Time elasped: 0.3627142906188965\n",
      "Epoch [2375/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02905\n",
      "128\n",
      "Time elasped: 0.345564603805542\n",
      "Mean Error: 0.0002457720402162522% \n",
      "--------------------------\n",
      "Epoch [2376/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03113\n",
      "128\n",
      "Time elasped: 0.3450205326080322\n",
      "Epoch [2377/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03296\n",
      "128\n",
      "Time elasped: 0.3412013053894043\n",
      "Epoch [2378/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03253\n",
      "128\n",
      "Time elasped: 0.34223175048828125\n",
      "Epoch [2379/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03183\n",
      "128\n",
      "Time elasped: 0.3649616241455078\n",
      "Epoch [2380/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03080\n",
      "128\n",
      "Time elasped: 0.3358159065246582\n",
      "Mean Error: 0.0002415264316368848% \n",
      "--------------------------\n",
      "Epoch [2381/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02929\n",
      "128\n",
      "Time elasped: 0.34459686279296875\n",
      "Epoch [2382/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03257\n",
      "128\n",
      "Time elasped: 0.3282957077026367\n",
      "Epoch [2383/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.02960\n",
      "128\n",
      "Time elasped: 0.33988523483276367\n",
      "Epoch [2384/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03083\n",
      "128\n",
      "Time elasped: 0.35787224769592285\n",
      "Epoch [2385/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03117\n",
      "128\n",
      "Time elasped: 0.33769822120666504\n",
      "Mean Error: 0.00024857191601768136% \n",
      "--------------------------\n",
      "Epoch [2386/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03477\n",
      "128\n",
      "Time elasped: 0.33260178565979004\n",
      "Epoch [2387/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03206\n",
      "128\n",
      "Time elasped: 0.33895254135131836\n",
      "Epoch [2388/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03149\n",
      "128\n",
      "Time elasped: 0.3404512405395508\n",
      "Epoch [2389/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03103\n",
      "128\n",
      "Time elasped: 0.34853029251098633\n",
      "Epoch [2390/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03492\n",
      "128\n",
      "Time elasped: 0.3498547077178955\n",
      "Mean Error: 0.00024698194465599954% \n",
      "--------------------------\n",
      "Epoch [2391/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03184\n",
      "128\n",
      "Time elasped: 0.3318765163421631\n",
      "Epoch [2392/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03307\n",
      "128\n",
      "Time elasped: 0.4018988609313965\n",
      "Epoch [2393/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03239\n",
      "128\n",
      "Time elasped: 0.3441305160522461\n",
      "Epoch [2394/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03385\n",
      "128\n",
      "Time elasped: 0.34482288360595703\n",
      "Epoch [2395/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03001\n",
      "128\n",
      "Time elasped: 0.34642934799194336\n",
      "Mean Error: 0.0002496125816833228% \n",
      "--------------------------\n",
      "Epoch [2396/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03082\n",
      "128\n",
      "Time elasped: 0.3640937805175781\n",
      "Epoch [2397/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03093\n",
      "128\n",
      "Time elasped: 0.3575000762939453\n",
      "Epoch [2398/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03125\n",
      "128\n",
      "Time elasped: 0.3441660404205322\n",
      "Epoch [2399/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03178\n",
      "128\n",
      "Time elasped: 0.42107105255126953\n",
      "Epoch [2400/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03140\n",
      "128\n",
      "Time elasped: 0.32596898078918457\n",
      "Mean Error: 0.00023275110288523138% \n",
      "--------------------------\n",
      "Epoch [2401/3000], learning_rates 0.000307, 0.307357\n",
      "Step [1/1], Loss: 0.03112\n",
      "128\n",
      "Time elasped: 0.3317251205444336\n",
      "Epoch [2402/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03074\n",
      "128\n",
      "Time elasped: 0.3403940200805664\n",
      "Epoch [2403/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02953\n",
      "128\n",
      "Time elasped: 0.33510828018188477\n",
      "Epoch [2404/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03278\n",
      "128\n",
      "Time elasped: 0.3580172061920166\n",
      "Epoch [2405/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03082\n",
      "128\n",
      "Time elasped: 0.32713842391967773\n",
      "Mean Error: 0.00022983772214502096% \n",
      "--------------------------\n",
      "Epoch [2406/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03138\n",
      "128\n",
      "Time elasped: 0.3286168575286865\n",
      "Epoch [2407/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03118\n",
      "128\n",
      "Time elasped: 0.3218536376953125\n",
      "Epoch [2408/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03115\n",
      "128\n",
      "Time elasped: 0.32564568519592285\n",
      "Epoch [2409/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03072\n",
      "128\n",
      "Time elasped: 0.3733024597167969\n",
      "Epoch [2410/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03537\n",
      "128\n",
      "Time elasped: 0.3303861618041992\n",
      "Mean Error: 0.00023588455223944038% \n",
      "--------------------------\n",
      "Epoch [2411/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03280\n",
      "128\n",
      "Time elasped: 0.34302425384521484\n",
      "Epoch [2412/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03513\n",
      "128\n",
      "Time elasped: 0.360166072845459\n",
      "Epoch [2413/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03329\n",
      "128\n",
      "Time elasped: 0.33698153495788574\n",
      "Epoch [2414/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03186\n",
      "128\n",
      "Time elasped: 0.3331940174102783\n",
      "Epoch [2415/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02908\n",
      "128\n",
      "Time elasped: 0.32426905632019043\n",
      "Mean Error: 0.00025182831450365484% \n",
      "--------------------------\n",
      "Epoch [2416/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03016\n",
      "128\n",
      "Time elasped: 0.3327972888946533\n",
      "Epoch [2417/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03449\n",
      "128\n",
      "Time elasped: 0.34673619270324707\n",
      "Epoch [2418/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03170\n",
      "128\n",
      "Time elasped: 0.3326680660247803\n",
      "Epoch [2419/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02965\n",
      "128\n",
      "Time elasped: 0.3281111717224121\n",
      "Epoch [2420/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03200\n",
      "128\n",
      "Time elasped: 0.364896297454834\n",
      "Mean Error: 0.00022752679069526494% \n",
      "--------------------------\n",
      "Epoch [2421/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02989\n",
      "128\n",
      "Time elasped: 0.35575413703918457\n",
      "Epoch [2422/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03260\n",
      "128\n",
      "Time elasped: 0.3283369541168213\n",
      "Epoch [2423/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03295\n",
      "128\n",
      "Time elasped: 0.3550698757171631\n",
      "Epoch [2424/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03245\n",
      "128\n",
      "Time elasped: 0.3418135643005371\n",
      "Epoch [2425/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02832\n",
      "128\n",
      "Time elasped: 0.322418212890625\n",
      "Mean Error: 0.0002455890062265098% \n",
      "--------------------------\n",
      "Epoch [2426/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02967\n",
      "128\n",
      "Time elasped: 0.35090160369873047\n",
      "Epoch [2427/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03287\n",
      "128\n",
      "Time elasped: 0.32569074630737305\n",
      "Epoch [2428/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02944\n",
      "128\n",
      "Time elasped: 0.3373289108276367\n",
      "Epoch [2429/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03002\n",
      "128\n",
      "Time elasped: 0.3214836120605469\n",
      "Epoch [2430/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03238\n",
      "128\n",
      "Time elasped: 0.35182833671569824\n",
      "Mean Error: 0.0002694425929803401% \n",
      "--------------------------\n",
      "Epoch [2431/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02694\n",
      "128\n",
      "Time elasped: 0.373638391494751\n",
      "Epoch [2432/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03040\n",
      "128\n",
      "Time elasped: 0.3435969352722168\n",
      "Epoch [2433/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03180\n",
      "128\n",
      "Time elasped: 0.33719825744628906\n",
      "Epoch [2434/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02864\n",
      "128\n",
      "Time elasped: 0.3293263912200928\n",
      "Epoch [2435/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03272\n",
      "128\n",
      "Time elasped: 0.35003137588500977\n",
      "Mean Error: 0.00024559462326578796% \n",
      "--------------------------\n",
      "Epoch [2436/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03267\n",
      "128\n",
      "Time elasped: 0.34197378158569336\n",
      "Epoch [2437/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03316\n",
      "128\n",
      "Time elasped: 0.3318312168121338\n",
      "Epoch [2438/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03087\n",
      "128\n",
      "Time elasped: 0.3378438949584961\n",
      "Epoch [2439/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03244\n",
      "128\n",
      "Time elasped: 0.3306286334991455\n",
      "Epoch [2440/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03096\n",
      "128\n",
      "Time elasped: 0.35767459869384766\n",
      "Mean Error: 0.00028521911008283496% \n",
      "--------------------------\n",
      "Epoch [2441/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03368\n",
      "128\n",
      "Time elasped: 0.34476161003112793\n",
      "Epoch [2442/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03124\n",
      "128\n",
      "Time elasped: 0.3548707962036133\n",
      "Epoch [2443/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03270\n",
      "128\n",
      "Time elasped: 0.343519926071167\n",
      "Epoch [2444/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02956\n",
      "128\n",
      "Time elasped: 0.31621527671813965\n",
      "Epoch [2445/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03260\n",
      "128\n",
      "Time elasped: 0.3446812629699707\n",
      "Mean Error: 0.00023656085249967873% \n",
      "--------------------------\n",
      "Epoch [2446/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03204\n",
      "128\n",
      "Time elasped: 0.33095812797546387\n",
      "Epoch [2447/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03003\n",
      "128\n",
      "Time elasped: 0.34831881523132324\n",
      "Epoch [2448/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03181\n",
      "128\n",
      "Time elasped: 0.34952807426452637\n",
      "Epoch [2449/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02905\n",
      "128\n",
      "Time elasped: 0.34233927726745605\n",
      "Epoch [2450/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03310\n",
      "128\n",
      "Time elasped: 0.35431337356567383\n",
      "Mean Error: 0.0002481632982380688% \n",
      "--------------------------\n",
      "Epoch [2451/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03009\n",
      "128\n",
      "Time elasped: 0.3263278007507324\n",
      "Epoch [2452/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03187\n",
      "128\n",
      "Time elasped: 0.3516373634338379\n",
      "Epoch [2453/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02958\n",
      "128\n",
      "Time elasped: 0.35767245292663574\n",
      "Epoch [2454/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02743\n",
      "128\n",
      "Time elasped: 0.34747982025146484\n",
      "Epoch [2455/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03150\n",
      "128\n",
      "Time elasped: 0.34435558319091797\n",
      "Mean Error: 0.00022301131684798747% \n",
      "--------------------------\n",
      "Epoch [2456/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03106\n",
      "128\n",
      "Time elasped: 0.32555127143859863\n",
      "Epoch [2457/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02896\n",
      "128\n",
      "Time elasped: 0.34853363037109375\n",
      "Epoch [2458/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02945\n",
      "128\n",
      "Time elasped: 0.33476972579956055\n",
      "Epoch [2459/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03221\n",
      "128\n",
      "Time elasped: 0.3148627281188965\n",
      "Epoch [2460/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03325\n",
      "128\n",
      "Time elasped: 0.3254873752593994\n",
      "Mean Error: 0.0002490657498128712% \n",
      "--------------------------\n",
      "Epoch [2461/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03002\n",
      "128\n",
      "Time elasped: 0.3586385250091553\n",
      "Epoch [2462/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02677\n",
      "128\n",
      "Time elasped: 0.34137892723083496\n",
      "Epoch [2463/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03168\n",
      "128\n",
      "Time elasped: 0.3274378776550293\n",
      "Epoch [2464/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03141\n",
      "128\n",
      "Time elasped: 0.32579469680786133\n",
      "Epoch [2465/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03039\n",
      "128\n",
      "Time elasped: 0.3327927589416504\n",
      "Mean Error: 0.00022819306468591094% \n",
      "--------------------------\n",
      "Epoch [2466/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03008\n",
      "128\n",
      "Time elasped: 0.33492612838745117\n",
      "Epoch [2467/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03070\n",
      "128\n",
      "Time elasped: 0.3587357997894287\n",
      "Epoch [2468/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03135\n",
      "128\n",
      "Time elasped: 0.3276228904724121\n",
      "Epoch [2469/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02805\n",
      "128\n",
      "Time elasped: 0.3348531723022461\n",
      "Epoch [2470/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03071\n",
      "128\n",
      "Time elasped: 0.35357093811035156\n",
      "Mean Error: 0.00024404369469266385% \n",
      "--------------------------\n",
      "Epoch [2471/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02955\n",
      "128\n",
      "Time elasped: 0.33263206481933594\n",
      "Epoch [2472/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02948\n",
      "128\n",
      "Time elasped: 0.34288978576660156\n",
      "Epoch [2473/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02934\n",
      "128\n",
      "Time elasped: 0.32813572883605957\n",
      "Epoch [2474/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03097\n",
      "128\n",
      "Time elasped: 0.361774206161499\n",
      "Epoch [2475/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03377\n",
      "128\n",
      "Time elasped: 0.3406839370727539\n",
      "Mean Error: 0.0002401479141553864% \n",
      "--------------------------\n",
      "Epoch [2476/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02775\n",
      "128\n",
      "Time elasped: 0.3349931240081787\n",
      "Epoch [2477/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02882\n",
      "128\n",
      "Time elasped: 0.3967154026031494\n",
      "Epoch [2478/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03149\n",
      "128\n",
      "Time elasped: 0.4355294704437256\n",
      "Epoch [2479/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03135\n",
      "128\n",
      "Time elasped: 0.40985774993896484\n",
      "Epoch [2480/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03209\n",
      "128\n",
      "Time elasped: 0.3645312786102295\n",
      "Mean Error: 0.0002515304659027606% \n",
      "--------------------------\n",
      "Epoch [2481/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03032\n",
      "128\n",
      "Time elasped: 0.3694462776184082\n",
      "Epoch [2482/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03190\n",
      "128\n",
      "Time elasped: 0.35649943351745605\n",
      "Epoch [2483/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03242\n",
      "128\n",
      "Time elasped: 0.3561418056488037\n",
      "Epoch [2484/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03253\n",
      "128\n",
      "Time elasped: 0.3643507957458496\n",
      "Epoch [2485/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02899\n",
      "128\n",
      "Time elasped: 0.3471248149871826\n",
      "Mean Error: 0.00022254047507885844% \n",
      "--------------------------\n",
      "Epoch [2486/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03171\n",
      "128\n",
      "Time elasped: 0.33469438552856445\n",
      "Epoch [2487/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03065\n",
      "128\n",
      "Time elasped: 0.34819483757019043\n",
      "Epoch [2488/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02979\n",
      "128\n",
      "Time elasped: 0.39380383491516113\n",
      "Epoch [2489/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03013\n",
      "128\n",
      "Time elasped: 0.34395551681518555\n",
      "Epoch [2490/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03063\n",
      "128\n",
      "Time elasped: 0.346071720123291\n",
      "Mean Error: 0.0002258371823700145% \n",
      "--------------------------\n",
      "Epoch [2491/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03160\n",
      "128\n",
      "Time elasped: 0.3749997615814209\n",
      "Epoch [2492/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03179\n",
      "128\n",
      "Time elasped: 0.38573241233825684\n",
      "Epoch [2493/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.02878\n",
      "128\n",
      "Time elasped: 0.3401963710784912\n",
      "Epoch [2494/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03109\n",
      "128\n",
      "Time elasped: 0.3503296375274658\n",
      "Epoch [2495/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03225\n",
      "128\n",
      "Time elasped: 0.37398624420166016\n",
      "Mean Error: 0.00023383177176583558% \n",
      "--------------------------\n",
      "Epoch [2496/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03149\n",
      "128\n",
      "Time elasped: 0.33620285987854004\n",
      "Epoch [2497/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03147\n",
      "128\n",
      "Time elasped: 0.34279513359069824\n",
      "Epoch [2498/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03197\n",
      "128\n",
      "Time elasped: 0.3297269344329834\n",
      "Epoch [2499/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03284\n",
      "128\n",
      "Time elasped: 0.38622593879699707\n",
      "Epoch [2500/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03081\n",
      "128\n",
      "Time elasped: 0.388988733291626\n",
      "Mean Error: 0.00023621607397217304% \n",
      "--------------------------\n",
      "Epoch [2501/3000], learning_rates 0.000292, 0.291989\n",
      "Step [1/1], Loss: 0.03122\n",
      "128\n",
      "Time elasped: 0.36064982414245605\n",
      "Epoch [2502/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03125\n",
      "128\n",
      "Time elasped: 0.3510112762451172\n",
      "Epoch [2503/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03221\n",
      "128\n",
      "Time elasped: 0.3472003936767578\n",
      "Epoch [2504/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02885\n",
      "128\n",
      "Time elasped: 0.33750247955322266\n",
      "Epoch [2505/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02994\n",
      "128\n",
      "Time elasped: 0.36762285232543945\n",
      "Mean Error: 0.00020941600087098777% \n",
      "--------------------------\n",
      "Epoch [2506/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03047\n",
      "128\n",
      "Time elasped: 0.3602597713470459\n",
      "Epoch [2507/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03059\n",
      "128\n",
      "Time elasped: 0.39163947105407715\n",
      "Epoch [2508/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03175\n",
      "128\n",
      "Time elasped: 0.3380699157714844\n",
      "Epoch [2509/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02948\n",
      "128\n",
      "Time elasped: 0.35468530654907227\n",
      "Epoch [2510/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02940\n",
      "128\n",
      "Time elasped: 0.3694267272949219\n",
      "Mean Error: 0.00023034869809634984% \n",
      "--------------------------\n",
      "Epoch [2511/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02676\n",
      "128\n",
      "Time elasped: 0.34487223625183105\n",
      "Epoch [2512/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02757\n",
      "128\n",
      "Time elasped: 0.35413360595703125\n",
      "Epoch [2513/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03109\n",
      "128\n",
      "Time elasped: 0.3549659252166748\n",
      "Epoch [2514/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03291\n",
      "128\n",
      "Time elasped: 0.3523216247558594\n",
      "Epoch [2515/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03338\n",
      "128\n",
      "Time elasped: 0.39571094512939453\n",
      "Mean Error: 0.0002612208772916347% \n",
      "--------------------------\n",
      "Epoch [2516/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02974\n",
      "128\n",
      "Time elasped: 0.35630226135253906\n",
      "Epoch [2517/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02924\n",
      "128\n",
      "Time elasped: 0.34122252464294434\n",
      "Epoch [2518/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03352\n",
      "128\n",
      "Time elasped: 0.3693981170654297\n",
      "Epoch [2519/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03077\n",
      "128\n",
      "Time elasped: 0.3475778102874756\n",
      "Epoch [2520/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03300\n",
      "128\n",
      "Time elasped: 0.35529518127441406\n",
      "Mean Error: 0.0002305835805600509% \n",
      "--------------------------\n",
      "Epoch [2521/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03266\n",
      "128\n",
      "Time elasped: 0.36129212379455566\n",
      "Epoch [2522/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03260\n",
      "128\n",
      "Time elasped: 0.365903377532959\n",
      "Epoch [2523/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02991\n",
      "128\n",
      "Time elasped: 0.35290098190307617\n",
      "Epoch [2524/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02936\n",
      "128\n",
      "Time elasped: 0.37735843658447266\n",
      "Epoch [2525/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02941\n",
      "128\n",
      "Time elasped: 0.35459280014038086\n",
      "Mean Error: 0.00021309888688847423% \n",
      "--------------------------\n",
      "Epoch [2526/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03243\n",
      "128\n",
      "Time elasped: 0.3708381652832031\n",
      "Epoch [2527/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03031\n",
      "128\n",
      "Time elasped: 0.37128233909606934\n",
      "Epoch [2528/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03063\n",
      "128\n",
      "Time elasped: 0.3493165969848633\n",
      "Epoch [2529/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02843\n",
      "128\n",
      "Time elasped: 0.35705113410949707\n",
      "Epoch [2530/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03007\n",
      "128\n",
      "Time elasped: 0.367016077041626\n",
      "Mean Error: 0.00023383714142255485% \n",
      "--------------------------\n",
      "Epoch [2531/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03185\n",
      "128\n",
      "Time elasped: 0.35015058517456055\n",
      "Epoch [2532/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02779\n",
      "128\n",
      "Time elasped: 0.36270999908447266\n",
      "Epoch [2533/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03269\n",
      "128\n",
      "Time elasped: 0.40779542922973633\n",
      "Epoch [2534/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02894\n",
      "128\n",
      "Time elasped: 0.3377203941345215\n",
      "Epoch [2535/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03202\n",
      "128\n",
      "Time elasped: 0.3804922103881836\n",
      "Mean Error: 0.00021490897051990032% \n",
      "--------------------------\n",
      "Epoch [2536/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02705\n",
      "128\n",
      "Time elasped: 0.36117053031921387\n",
      "Epoch [2537/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03203\n",
      "128\n",
      "Time elasped: 0.3612048625946045\n",
      "Epoch [2538/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02833\n",
      "128\n",
      "Time elasped: 0.36291027069091797\n",
      "Epoch [2539/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03055\n",
      "128\n",
      "Time elasped: 0.388378381729126\n",
      "Epoch [2540/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03272\n",
      "128\n",
      "Time elasped: 0.35645174980163574\n",
      "Mean Error: 0.00024515847326256335% \n",
      "--------------------------\n",
      "Epoch [2541/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03133\n",
      "128\n",
      "Time elasped: 0.34427833557128906\n",
      "Epoch [2542/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03096\n",
      "128\n",
      "Time elasped: 0.35858654975891113\n",
      "Epoch [2543/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03012\n",
      "128\n",
      "Time elasped: 0.34691619873046875\n",
      "Epoch [2544/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03132\n",
      "128\n",
      "Time elasped: 0.37714719772338867\n",
      "Epoch [2545/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02721\n",
      "128\n",
      "Time elasped: 0.3736605644226074\n",
      "Mean Error: 0.0002394172188360244% \n",
      "--------------------------\n",
      "Epoch [2546/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02943\n",
      "128\n",
      "Time elasped: 0.4043140411376953\n",
      "Epoch [2547/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03201\n",
      "128\n",
      "Time elasped: 0.3638434410095215\n",
      "Epoch [2548/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03016\n",
      "128\n",
      "Time elasped: 0.35659027099609375\n",
      "Epoch [2549/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03190\n",
      "128\n",
      "Time elasped: 0.3643205165863037\n",
      "Epoch [2550/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02953\n",
      "128\n",
      "Time elasped: 0.37934422492980957\n",
      "Mean Error: 0.00023222046729642898% \n",
      "--------------------------\n",
      "Epoch [2551/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02787\n",
      "128\n",
      "Time elasped: 0.41205930709838867\n",
      "Epoch [2552/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02993\n",
      "128\n",
      "Time elasped: 0.41322755813598633\n",
      "Epoch [2553/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02966\n",
      "128\n",
      "Time elasped: 0.35744619369506836\n",
      "Epoch [2554/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03286\n",
      "128\n",
      "Time elasped: 0.3474550247192383\n",
      "Epoch [2555/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03003\n",
      "128\n",
      "Time elasped: 0.3555929660797119\n",
      "Mean Error: 0.00023222567688208073% \n",
      "--------------------------\n",
      "Epoch [2556/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02788\n",
      "128\n",
      "Time elasped: 0.37599825859069824\n",
      "Epoch [2557/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02854\n",
      "128\n",
      "Time elasped: 0.4168722629547119\n",
      "Epoch [2558/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02863\n",
      "128\n",
      "Time elasped: 0.4129493236541748\n",
      "Epoch [2559/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03032\n",
      "128\n",
      "Time elasped: 0.35605478286743164\n",
      "Epoch [2560/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03073\n",
      "128\n",
      "Time elasped: 0.3562769889831543\n",
      "Mean Error: 0.00025680242106318474% \n",
      "--------------------------\n",
      "Epoch [2561/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02831\n",
      "128\n",
      "Time elasped: 0.36373376846313477\n",
      "Epoch [2562/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02903\n",
      "128\n",
      "Time elasped: 0.40755367279052734\n",
      "Epoch [2563/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02885\n",
      "128\n",
      "Time elasped: 0.39575743675231934\n",
      "Epoch [2564/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02678\n",
      "128\n",
      "Time elasped: 0.41190576553344727\n",
      "Epoch [2565/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03116\n",
      "128\n",
      "Time elasped: 0.4294590950012207\n",
      "Mean Error: 0.00023440798395313323% \n",
      "--------------------------\n",
      "Epoch [2566/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02987\n",
      "128\n",
      "Time elasped: 0.4637925624847412\n",
      "Epoch [2567/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02905\n",
      "128\n",
      "Time elasped: 0.4461405277252197\n",
      "Epoch [2568/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02860\n",
      "128\n",
      "Time elasped: 0.46968674659729004\n",
      "Epoch [2569/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02931\n",
      "128\n",
      "Time elasped: 0.44341087341308594\n",
      "Epoch [2570/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02848\n",
      "128\n",
      "Time elasped: 0.4109981060028076\n",
      "Mean Error: 0.0002321673819096759% \n",
      "--------------------------\n",
      "Epoch [2571/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02992\n",
      "128\n",
      "Time elasped: 0.4380156993865967\n",
      "Epoch [2572/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03254\n",
      "128\n",
      "Time elasped: 0.4162774085998535\n",
      "Epoch [2573/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02817\n",
      "128\n",
      "Time elasped: 0.3761880397796631\n",
      "Epoch [2574/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03051\n",
      "128\n",
      "Time elasped: 0.38289546966552734\n",
      "Epoch [2575/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02810\n",
      "128\n",
      "Time elasped: 0.4050254821777344\n",
      "Mean Error: 0.0002428316220175475% \n",
      "--------------------------\n",
      "Epoch [2576/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03229\n",
      "128\n",
      "Time elasped: 0.4403707981109619\n",
      "Epoch [2577/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02882\n",
      "128\n",
      "Time elasped: 0.4117264747619629\n",
      "Epoch [2578/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02772\n",
      "128\n",
      "Time elasped: 0.39302587509155273\n",
      "Epoch [2579/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02872\n",
      "128\n",
      "Time elasped: 0.3851959705352783\n",
      "Epoch [2580/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02879\n",
      "128\n",
      "Time elasped: 0.4016282558441162\n",
      "Mean Error: 0.00023742017219774425% \n",
      "--------------------------\n",
      "Epoch [2581/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03187\n",
      "128\n",
      "Time elasped: 0.4221491813659668\n",
      "Epoch [2582/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02864\n",
      "128\n",
      "Time elasped: 0.39944887161254883\n",
      "Epoch [2583/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02927\n",
      "128\n",
      "Time elasped: 0.40421128273010254\n",
      "Epoch [2584/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02853\n",
      "128\n",
      "Time elasped: 0.40659546852111816\n",
      "Epoch [2585/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03088\n",
      "128\n",
      "Time elasped: 0.3889942169189453\n",
      "Mean Error: 0.0002398402866674587% \n",
      "--------------------------\n",
      "Epoch [2586/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02988\n",
      "128\n",
      "Time elasped: 0.42026519775390625\n",
      "Epoch [2587/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02997\n",
      "128\n",
      "Time elasped: 0.4091324806213379\n",
      "Epoch [2588/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03311\n",
      "128\n",
      "Time elasped: 0.4037513732910156\n",
      "Epoch [2589/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03166\n",
      "128\n",
      "Time elasped: 0.40380358695983887\n",
      "Epoch [2590/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03012\n",
      "128\n",
      "Time elasped: 0.39090538024902344\n",
      "Mean Error: 0.00024125774507410824% \n",
      "--------------------------\n",
      "Epoch [2591/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03198\n",
      "128\n",
      "Time elasped: 0.4544346332550049\n",
      "Epoch [2592/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03379\n",
      "128\n",
      "Time elasped: 0.39403772354125977\n",
      "Epoch [2593/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03026\n",
      "128\n",
      "Time elasped: 0.3968937397003174\n",
      "Epoch [2594/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02866\n",
      "128\n",
      "Time elasped: 0.3673593997955322\n",
      "Epoch [2595/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02990\n",
      "128\n",
      "Time elasped: 0.35198044776916504\n",
      "Mean Error: 0.00023820705246180296% \n",
      "--------------------------\n",
      "Epoch [2596/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03356\n",
      "128\n",
      "Time elasped: 0.36890149116516113\n",
      "Epoch [2597/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02961\n",
      "128\n",
      "Time elasped: 0.36185669898986816\n",
      "Epoch [2598/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03056\n",
      "128\n",
      "Time elasped: 0.3756217956542969\n",
      "Epoch [2599/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03265\n",
      "128\n",
      "Time elasped: 0.3670628070831299\n",
      "Epoch [2600/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.03059\n",
      "128\n",
      "Time elasped: 0.3538978099822998\n",
      "Mean Error: 0.00024500771542079747% \n",
      "--------------------------\n",
      "Epoch [2601/3000], learning_rates 0.000277, 0.277390\n",
      "Step [1/1], Loss: 0.02967\n",
      "128\n",
      "Time elasped: 0.373856782913208\n",
      "Epoch [2602/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03046\n",
      "128\n",
      "Time elasped: 0.35886502265930176\n",
      "Epoch [2603/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03027\n",
      "128\n",
      "Time elasped: 0.35713839530944824\n",
      "Epoch [2604/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02864\n",
      "128\n",
      "Time elasped: 0.37242817878723145\n",
      "Epoch [2605/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03176\n",
      "128\n",
      "Time elasped: 0.34744691848754883\n",
      "Mean Error: 0.00023322719789575785% \n",
      "--------------------------\n",
      "Epoch [2606/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02796\n",
      "128\n",
      "Time elasped: 0.384448766708374\n",
      "Epoch [2607/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02781\n",
      "128\n",
      "Time elasped: 0.3630204200744629\n",
      "Epoch [2608/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03020\n",
      "128\n",
      "Time elasped: 0.3449361324310303\n",
      "Epoch [2609/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03034\n",
      "128\n",
      "Time elasped: 0.3339972496032715\n",
      "Epoch [2610/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03115\n",
      "128\n",
      "Time elasped: 0.37231016159057617\n",
      "Mean Error: 0.00021461724827531725% \n",
      "--------------------------\n",
      "Epoch [2611/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03271\n",
      "128\n",
      "Time elasped: 0.35999536514282227\n",
      "Epoch [2612/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02896\n",
      "128\n",
      "Time elasped: 0.3691554069519043\n",
      "Epoch [2613/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02985\n",
      "128\n",
      "Time elasped: 0.3493494987487793\n",
      "Epoch [2614/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02717\n",
      "128\n",
      "Time elasped: 0.38037705421447754\n",
      "Epoch [2615/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02939\n",
      "128\n",
      "Time elasped: 0.35474181175231934\n",
      "Mean Error: 0.00020547643362078816% \n",
      "--------------------------\n",
      "Epoch [2616/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03026\n",
      "128\n",
      "Time elasped: 0.35744524002075195\n",
      "Epoch [2617/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02894\n",
      "128\n",
      "Time elasped: 0.3430047035217285\n",
      "Epoch [2618/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03008\n",
      "128\n",
      "Time elasped: 0.35268139839172363\n",
      "Epoch [2619/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03182\n",
      "128\n",
      "Time elasped: 0.3730185031890869\n",
      "Epoch [2620/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03162\n",
      "128\n",
      "Time elasped: 0.369243860244751\n",
      "Mean Error: 0.00022849623928777874% \n",
      "--------------------------\n",
      "Epoch [2621/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03390\n",
      "128\n",
      "Time elasped: 0.37125468254089355\n",
      "Epoch [2622/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02870\n",
      "128\n",
      "Time elasped: 0.3876819610595703\n",
      "Epoch [2623/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03384\n",
      "128\n",
      "Time elasped: 0.3332858085632324\n",
      "Epoch [2624/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03231\n",
      "128\n",
      "Time elasped: 0.3549041748046875\n",
      "Epoch [2625/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02790\n",
      "128\n",
      "Time elasped: 0.4024035930633545\n",
      "Mean Error: 0.00022965284006204456% \n",
      "--------------------------\n",
      "Epoch [2626/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03042\n",
      "128\n",
      "Time elasped: 0.375521183013916\n",
      "Epoch [2627/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02795\n",
      "128\n",
      "Time elasped: 0.38045644760131836\n",
      "Epoch [2628/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02828\n",
      "128\n",
      "Time elasped: 0.36621618270874023\n",
      "Epoch [2629/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02744\n",
      "128\n",
      "Time elasped: 0.34812498092651367\n",
      "Epoch [2630/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02899\n",
      "128\n",
      "Time elasped: 0.3509664535522461\n",
      "Mean Error: 0.00021773649496026337% \n",
      "--------------------------\n",
      "Epoch [2631/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02861\n",
      "128\n",
      "Time elasped: 0.3599116802215576\n",
      "Epoch [2632/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03330\n",
      "128\n",
      "Time elasped: 0.38751959800720215\n",
      "Epoch [2633/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02995\n",
      "128\n",
      "Time elasped: 0.3663909435272217\n",
      "Epoch [2634/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02915\n",
      "128\n",
      "Time elasped: 0.3537003993988037\n",
      "Epoch [2635/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02780\n",
      "128\n",
      "Time elasped: 0.3846712112426758\n",
      "Mean Error: 0.000238878681557253% \n",
      "--------------------------\n",
      "Epoch [2636/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02732\n",
      "128\n",
      "Time elasped: 0.3640744686126709\n",
      "Epoch [2637/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02922\n",
      "128\n",
      "Time elasped: 0.32556605339050293\n",
      "Epoch [2638/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02791\n",
      "128\n",
      "Time elasped: 0.35242247581481934\n",
      "Epoch [2639/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02748\n",
      "128\n",
      "Time elasped: 0.32996296882629395\n",
      "Epoch [2640/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02834\n",
      "128\n",
      "Time elasped: 0.3748617172241211\n",
      "Mean Error: 0.00022088598052505404% \n",
      "--------------------------\n",
      "Epoch [2641/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02736\n",
      "128\n",
      "Time elasped: 0.3656916618347168\n",
      "Epoch [2642/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02804\n",
      "128\n",
      "Time elasped: 0.3589959144592285\n",
      "Epoch [2643/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03012\n",
      "128\n",
      "Time elasped: 0.32505345344543457\n",
      "Epoch [2644/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02943\n",
      "128\n",
      "Time elasped: 0.3324708938598633\n",
      "Epoch [2645/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02790\n",
      "128\n",
      "Time elasped: 0.344463586807251\n",
      "Mean Error: 0.00024775511701591313% \n",
      "--------------------------\n",
      "Epoch [2646/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03226\n",
      "128\n",
      "Time elasped: 0.33837890625\n",
      "Epoch [2647/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03178\n",
      "128\n",
      "Time elasped: 0.32724642753601074\n",
      "Epoch [2648/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02904\n",
      "128\n",
      "Time elasped: 0.3259148597717285\n",
      "Epoch [2649/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02618\n",
      "128\n",
      "Time elasped: 0.33144640922546387\n",
      "Epoch [2650/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02874\n",
      "128\n",
      "Time elasped: 0.34328222274780273\n",
      "Mean Error: 0.00023373703879769892% \n",
      "--------------------------\n",
      "Epoch [2651/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02736\n",
      "128\n",
      "Time elasped: 0.3294038772583008\n",
      "Epoch [2652/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02959\n",
      "128\n",
      "Time elasped: 0.3292856216430664\n",
      "Epoch [2653/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02921\n",
      "128\n",
      "Time elasped: 0.3477022647857666\n",
      "Epoch [2654/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02855\n",
      "128\n",
      "Time elasped: 0.3449413776397705\n",
      "Epoch [2655/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02971\n",
      "128\n",
      "Time elasped: 0.3307797908782959\n",
      "Mean Error: 0.0002117980911862105% \n",
      "--------------------------\n",
      "Epoch [2656/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03257\n",
      "128\n",
      "Time elasped: 0.35100722312927246\n",
      "Epoch [2657/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02722\n",
      "128\n",
      "Time elasped: 0.3402099609375\n",
      "Epoch [2658/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03167\n",
      "128\n",
      "Time elasped: 0.339566707611084\n",
      "Epoch [2659/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02800\n",
      "128\n",
      "Time elasped: 0.36888861656188965\n",
      "Epoch [2660/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02860\n",
      "128\n",
      "Time elasped: 0.3365654945373535\n",
      "Mean Error: 0.00022128663840703666% \n",
      "--------------------------\n",
      "Epoch [2661/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02801\n",
      "128\n",
      "Time elasped: 0.3650083541870117\n",
      "Epoch [2662/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02869\n",
      "128\n",
      "Time elasped: 0.35280418395996094\n",
      "Epoch [2663/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03019\n",
      "128\n",
      "Time elasped: 0.36141228675842285\n",
      "Epoch [2664/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02675\n",
      "128\n",
      "Time elasped: 0.3780660629272461\n",
      "Epoch [2665/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03124\n",
      "128\n",
      "Time elasped: 0.3474581241607666\n",
      "Mean Error: 0.0002515718515496701% \n",
      "--------------------------\n",
      "Epoch [2666/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02937\n",
      "128\n",
      "Time elasped: 0.34685635566711426\n",
      "Epoch [2667/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02942\n",
      "128\n",
      "Time elasped: 0.32840585708618164\n",
      "Epoch [2668/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02651\n",
      "128\n",
      "Time elasped: 0.3488593101501465\n",
      "Epoch [2669/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02807\n",
      "128\n",
      "Time elasped: 0.3346278667449951\n",
      "Epoch [2670/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02936\n",
      "128\n",
      "Time elasped: 0.3397247791290283\n",
      "Mean Error: 0.00022031209664419293% \n",
      "--------------------------\n",
      "Epoch [2671/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02746\n",
      "128\n",
      "Time elasped: 0.3491036891937256\n",
      "Epoch [2672/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02960\n",
      "128\n",
      "Time elasped: 0.34894514083862305\n",
      "Epoch [2673/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02870\n",
      "128\n",
      "Time elasped: 0.35184288024902344\n",
      "Epoch [2674/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03044\n",
      "128\n",
      "Time elasped: 0.33117151260375977\n",
      "Epoch [2675/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03114\n",
      "128\n",
      "Time elasped: 0.33892011642456055\n",
      "Mean Error: 0.000228924720431678% \n",
      "--------------------------\n",
      "Epoch [2676/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03192\n",
      "128\n",
      "Time elasped: 0.35222482681274414\n",
      "Epoch [2677/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03086\n",
      "128\n",
      "Time elasped: 0.33898210525512695\n",
      "Epoch [2678/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03025\n",
      "128\n",
      "Time elasped: 0.32003331184387207\n",
      "Epoch [2679/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02994\n",
      "128\n",
      "Time elasped: 0.35098981857299805\n",
      "Epoch [2680/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02991\n",
      "128\n",
      "Time elasped: 0.42082858085632324\n",
      "Mean Error: 0.00024320901138707995% \n",
      "--------------------------\n",
      "Epoch [2681/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02931\n",
      "128\n",
      "Time elasped: 0.36704087257385254\n",
      "Epoch [2682/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02779\n",
      "128\n",
      "Time elasped: 0.45950818061828613\n",
      "Epoch [2683/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02846\n",
      "128\n",
      "Time elasped: 0.45235729217529297\n",
      "Epoch [2684/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02984\n",
      "128\n",
      "Time elasped: 0.39696717262268066\n",
      "Epoch [2685/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02898\n",
      "128\n",
      "Time elasped: 0.3701314926147461\n",
      "Mean Error: 0.0002043731219600886% \n",
      "--------------------------\n",
      "Epoch [2686/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02768\n",
      "128\n",
      "Time elasped: 0.3580803871154785\n",
      "Epoch [2687/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02916\n",
      "128\n",
      "Time elasped: 0.3621189594268799\n",
      "Epoch [2688/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02653\n",
      "128\n",
      "Time elasped: 0.3381779193878174\n",
      "Epoch [2689/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02933\n",
      "128\n",
      "Time elasped: 0.34165120124816895\n",
      "Epoch [2690/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03205\n",
      "128\n",
      "Time elasped: 0.3472881317138672\n",
      "Mean Error: 0.00020973272330593318% \n",
      "--------------------------\n",
      "Epoch [2691/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02764\n",
      "128\n",
      "Time elasped: 0.34567713737487793\n",
      "Epoch [2692/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02847\n",
      "128\n",
      "Time elasped: 0.3643784523010254\n",
      "Epoch [2693/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02752\n",
      "128\n",
      "Time elasped: 0.3584158420562744\n",
      "Epoch [2694/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02864\n",
      "128\n",
      "Time elasped: 0.36110472679138184\n",
      "Epoch [2695/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02937\n",
      "128\n",
      "Time elasped: 0.33583831787109375\n",
      "Mean Error: 0.00019874457211699337% \n",
      "--------------------------\n",
      "Epoch [2696/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03004\n",
      "128\n",
      "Time elasped: 0.34956908226013184\n",
      "Epoch [2697/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02610\n",
      "128\n",
      "Time elasped: 0.3737649917602539\n",
      "Epoch [2698/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02953\n",
      "128\n",
      "Time elasped: 0.3470597267150879\n",
      "Epoch [2699/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02837\n",
      "128\n",
      "Time elasped: 0.33871889114379883\n",
      "Epoch [2700/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.02762\n",
      "128\n",
      "Time elasped: 0.35846877098083496\n",
      "Mean Error: 0.0002341532235732302% \n",
      "--------------------------\n",
      "Epoch [2701/3000], learning_rates 0.000264, 0.263520\n",
      "Step [1/1], Loss: 0.03067\n",
      "128\n",
      "Time elasped: 0.3468782901763916\n",
      "Epoch [2702/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02866\n",
      "128\n",
      "Time elasped: 0.3580899238586426\n",
      "Epoch [2703/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02814\n",
      "128\n",
      "Time elasped: 0.3556692600250244\n",
      "Epoch [2704/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02771\n",
      "128\n",
      "Time elasped: 0.3272678852081299\n",
      "Epoch [2705/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03048\n",
      "128\n",
      "Time elasped: 0.34508633613586426\n",
      "Mean Error: 0.00024292075249832124% \n",
      "--------------------------\n",
      "Epoch [2706/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02891\n",
      "128\n",
      "Time elasped: 0.32517290115356445\n",
      "Epoch [2707/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02806\n",
      "128\n",
      "Time elasped: 0.35784316062927246\n",
      "Epoch [2708/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03038\n",
      "128\n",
      "Time elasped: 0.32776975631713867\n",
      "Epoch [2709/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02972\n",
      "128\n",
      "Time elasped: 0.33952999114990234\n",
      "Epoch [2710/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02724\n",
      "128\n",
      "Time elasped: 0.33327651023864746\n",
      "Mean Error: 0.0002135249669663608% \n",
      "--------------------------\n",
      "Epoch [2711/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02968\n",
      "128\n",
      "Time elasped: 0.37853288650512695\n",
      "Epoch [2712/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03111\n",
      "128\n",
      "Time elasped: 0.3499414920806885\n",
      "Epoch [2713/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02724\n",
      "128\n",
      "Time elasped: 0.3567075729370117\n",
      "Epoch [2714/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02818\n",
      "128\n",
      "Time elasped: 0.33508849143981934\n",
      "Epoch [2715/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03069\n",
      "128\n",
      "Time elasped: 0.3538787364959717\n",
      "Mean Error: 0.00023584238078910857% \n",
      "--------------------------\n",
      "Epoch [2716/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02755\n",
      "128\n",
      "Time elasped: 0.36423707008361816\n",
      "Epoch [2717/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02882\n",
      "128\n",
      "Time elasped: 0.3369863033294678\n",
      "Epoch [2718/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02741\n",
      "128\n",
      "Time elasped: 0.3384668827056885\n",
      "Epoch [2719/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02942\n",
      "128\n",
      "Time elasped: 0.33075690269470215\n",
      "Epoch [2720/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03124\n",
      "128\n",
      "Time elasped: 0.3342883586883545\n",
      "Mean Error: 0.00021052687952760607% \n",
      "--------------------------\n",
      "Epoch [2721/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02733\n",
      "128\n",
      "Time elasped: 0.34737229347229004\n",
      "Epoch [2722/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02687\n",
      "128\n",
      "Time elasped: 0.3204152584075928\n",
      "Epoch [2723/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03017\n",
      "128\n",
      "Time elasped: 0.3363182544708252\n",
      "Epoch [2724/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02860\n",
      "128\n",
      "Time elasped: 0.34304332733154297\n",
      "Epoch [2725/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02871\n",
      "128\n",
      "Time elasped: 0.3448667526245117\n",
      "Mean Error: 0.00021971292153466493% \n",
      "--------------------------\n",
      "Epoch [2726/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02936\n",
      "128\n",
      "Time elasped: 0.3379940986633301\n",
      "Epoch [2727/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02791\n",
      "128\n",
      "Time elasped: 0.3465418815612793\n",
      "Epoch [2728/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02761\n",
      "128\n",
      "Time elasped: 0.32515430450439453\n",
      "Epoch [2729/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02960\n",
      "128\n",
      "Time elasped: 0.3385930061340332\n",
      "Epoch [2730/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02695\n",
      "128\n",
      "Time elasped: 0.31127333641052246\n",
      "Mean Error: 0.00022624908888246864% \n",
      "--------------------------\n",
      "Epoch [2731/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02760\n",
      "128\n",
      "Time elasped: 0.371845006942749\n",
      "Epoch [2732/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03019\n",
      "128\n",
      "Time elasped: 0.3283226490020752\n",
      "Epoch [2733/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02923\n",
      "128\n",
      "Time elasped: 0.310086727142334\n",
      "Epoch [2734/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02685\n",
      "128\n",
      "Time elasped: 0.31740379333496094\n",
      "Epoch [2735/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03043\n",
      "128\n",
      "Time elasped: 0.3268589973449707\n",
      "Mean Error: 0.00023838052584324032% \n",
      "--------------------------\n",
      "Epoch [2736/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03107\n",
      "128\n",
      "Time elasped: 0.3311758041381836\n",
      "Epoch [2737/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02745\n",
      "128\n",
      "Time elasped: 0.34894299507141113\n",
      "Epoch [2738/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03109\n",
      "128\n",
      "Time elasped: 0.3233988285064697\n",
      "Epoch [2739/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02932\n",
      "128\n",
      "Time elasped: 0.32600903511047363\n",
      "Epoch [2740/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03027\n",
      "128\n",
      "Time elasped: 0.3387918472290039\n",
      "Mean Error: 0.0002136479306500405% \n",
      "--------------------------\n",
      "Epoch [2741/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02913\n",
      "128\n",
      "Time elasped: 0.34514570236206055\n",
      "Epoch [2742/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02709\n",
      "128\n",
      "Time elasped: 0.3315412998199463\n",
      "Epoch [2743/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03082\n",
      "128\n",
      "Time elasped: 0.35237932205200195\n",
      "Epoch [2744/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02713\n",
      "128\n",
      "Time elasped: 0.3407433032989502\n",
      "Epoch [2745/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02740\n",
      "128\n",
      "Time elasped: 0.3204360008239746\n",
      "Mean Error: 0.0002050864859484136% \n",
      "--------------------------\n",
      "Epoch [2746/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02670\n",
      "128\n",
      "Time elasped: 0.3254222869873047\n",
      "Epoch [2747/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02921\n",
      "128\n",
      "Time elasped: 0.32858753204345703\n",
      "Epoch [2748/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02763\n",
      "128\n",
      "Time elasped: 0.3264434337615967\n",
      "Epoch [2749/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02847\n",
      "128\n",
      "Time elasped: 0.32416749000549316\n",
      "Epoch [2750/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02765\n",
      "128\n",
      "Time elasped: 0.35238003730773926\n",
      "Mean Error: 0.00022935769811738282% \n",
      "--------------------------\n",
      "Epoch [2751/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02673\n",
      "128\n",
      "Time elasped: 0.3421168327331543\n",
      "Epoch [2752/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02969\n",
      "128\n",
      "Time elasped: 0.32811594009399414\n",
      "Epoch [2753/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02872\n",
      "128\n",
      "Time elasped: 0.3202226161956787\n",
      "Epoch [2754/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03155\n",
      "128\n",
      "Time elasped: 0.30959153175354004\n",
      "Epoch [2755/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02782\n",
      "128\n",
      "Time elasped: 0.3487865924835205\n",
      "Mean Error: 0.00022654682106804103% \n",
      "--------------------------\n",
      "Epoch [2756/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02781\n",
      "128\n",
      "Time elasped: 0.33056187629699707\n",
      "Epoch [2757/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02840\n",
      "128\n",
      "Time elasped: 0.32967519760131836\n",
      "Epoch [2758/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03002\n",
      "128\n",
      "Time elasped: 0.3331122398376465\n",
      "Epoch [2759/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03012\n",
      "128\n",
      "Time elasped: 0.33225393295288086\n",
      "Epoch [2760/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02770\n",
      "128\n",
      "Time elasped: 0.3294649124145508\n",
      "Mean Error: 0.00022995811013970524% \n",
      "--------------------------\n",
      "Epoch [2761/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02671\n",
      "128\n",
      "Time elasped: 0.34955716133117676\n",
      "Epoch [2762/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02948\n",
      "128\n",
      "Time elasped: 0.32191991806030273\n",
      "Epoch [2763/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02646\n",
      "128\n",
      "Time elasped: 0.34246397018432617\n",
      "Epoch [2764/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02797\n",
      "128\n",
      "Time elasped: 0.3137059211730957\n",
      "Epoch [2765/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02930\n",
      "128\n",
      "Time elasped: 0.3426530361175537\n",
      "Mean Error: 0.00024426673189736903% \n",
      "--------------------------\n",
      "Epoch [2766/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02742\n",
      "128\n",
      "Time elasped: 0.32563304901123047\n",
      "Epoch [2767/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02801\n",
      "128\n",
      "Time elasped: 0.3215956687927246\n",
      "Epoch [2768/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03075\n",
      "128\n",
      "Time elasped: 0.36496520042419434\n",
      "Epoch [2769/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03034\n",
      "128\n",
      "Time elasped: 0.34011030197143555\n",
      "Epoch [2770/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02949\n",
      "128\n",
      "Time elasped: 0.34455132484436035\n",
      "Mean Error: 0.00024931153166107833% \n",
      "--------------------------\n",
      "Epoch [2771/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02964\n",
      "128\n",
      "Time elasped: 0.319077730178833\n",
      "Epoch [2772/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03023\n",
      "128\n",
      "Time elasped: 0.3561215400695801\n",
      "Epoch [2773/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02661\n",
      "128\n",
      "Time elasped: 0.34677553176879883\n",
      "Epoch [2774/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03057\n",
      "128\n",
      "Time elasped: 0.3293588161468506\n",
      "Epoch [2775/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03139\n",
      "128\n",
      "Time elasped: 0.31328439712524414\n",
      "Mean Error: 0.00021718337666243315% \n",
      "--------------------------\n",
      "Epoch [2776/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02858\n",
      "128\n",
      "Time elasped: 0.3272373676300049\n",
      "Epoch [2777/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02982\n",
      "128\n",
      "Time elasped: 0.31279540061950684\n",
      "Epoch [2778/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02881\n",
      "128\n",
      "Time elasped: 0.3269665241241455\n",
      "Epoch [2779/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03199\n",
      "128\n",
      "Time elasped: 0.35130786895751953\n",
      "Epoch [2780/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02882\n",
      "128\n",
      "Time elasped: 0.3143177032470703\n",
      "Mean Error: 0.00021175495930947363% \n",
      "--------------------------\n",
      "Epoch [2781/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02977\n",
      "128\n",
      "Time elasped: 0.34006524085998535\n",
      "Epoch [2782/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02965\n",
      "128\n",
      "Time elasped: 0.3273186683654785\n",
      "Epoch [2783/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02943\n",
      "128\n",
      "Time elasped: 0.322246789932251\n",
      "Epoch [2784/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03086\n",
      "128\n",
      "Time elasped: 0.3278234004974365\n",
      "Epoch [2785/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03052\n",
      "128\n",
      "Time elasped: 0.3222072124481201\n",
      "Mean Error: 0.0002584609028417617% \n",
      "--------------------------\n",
      "Epoch [2786/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03084\n",
      "128\n",
      "Time elasped: 0.323178768157959\n",
      "Epoch [2787/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02884\n",
      "128\n",
      "Time elasped: 0.3482346534729004\n",
      "Epoch [2788/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02800\n",
      "128\n",
      "Time elasped: 0.31767892837524414\n",
      "Epoch [2789/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02948\n",
      "128\n",
      "Time elasped: 0.32247042655944824\n",
      "Epoch [2790/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02712\n",
      "128\n",
      "Time elasped: 0.34978771209716797\n",
      "Mean Error: 0.00022067829559091479% \n",
      "--------------------------\n",
      "Epoch [2791/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02864\n",
      "128\n",
      "Time elasped: 0.32436609268188477\n",
      "Epoch [2792/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02840\n",
      "128\n",
      "Time elasped: 0.3455824851989746\n",
      "Epoch [2793/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02940\n",
      "128\n",
      "Time elasped: 0.34013795852661133\n",
      "Epoch [2794/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03043\n",
      "128\n",
      "Time elasped: 0.3301224708557129\n",
      "Epoch [2795/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02905\n",
      "128\n",
      "Time elasped: 0.32997560501098633\n",
      "Mean Error: 0.00020632374798879027% \n",
      "--------------------------\n",
      "Epoch [2796/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02945\n",
      "128\n",
      "Time elasped: 0.3261525630950928\n",
      "Epoch [2797/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02941\n",
      "128\n",
      "Time elasped: 0.3397715091705322\n",
      "Epoch [2798/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02828\n",
      "128\n",
      "Time elasped: 0.32570767402648926\n",
      "Epoch [2799/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03035\n",
      "128\n",
      "Time elasped: 0.32441163063049316\n",
      "Epoch [2800/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.03035\n",
      "128\n",
      "Time elasped: 0.35384082794189453\n",
      "Mean Error: 0.0002376844931859523% \n",
      "--------------------------\n",
      "Epoch [2801/3000], learning_rates 0.000250, 0.250344\n",
      "Step [1/1], Loss: 0.02797\n",
      "128\n",
      "Time elasped: 0.3130364418029785\n",
      "Epoch [2802/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02931\n",
      "128\n",
      "Time elasped: 0.3384079933166504\n",
      "Epoch [2803/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02943\n",
      "128\n",
      "Time elasped: 0.3498060703277588\n",
      "Epoch [2804/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02804\n",
      "128\n",
      "Time elasped: 0.32788753509521484\n",
      "Epoch [2805/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02895\n",
      "128\n",
      "Time elasped: 0.3386852741241455\n",
      "Mean Error: 0.00023978264653123915% \n",
      "--------------------------\n",
      "Epoch [2806/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02835\n",
      "128\n",
      "Time elasped: 0.3321704864501953\n",
      "Epoch [2807/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02941\n",
      "128\n",
      "Time elasped: 0.3451728820800781\n",
      "Epoch [2808/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02933\n",
      "128\n",
      "Time elasped: 0.3159902095794678\n",
      "Epoch [2809/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02853\n",
      "128\n",
      "Time elasped: 0.32364487648010254\n",
      "Epoch [2810/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02847\n",
      "128\n",
      "Time elasped: 0.3158557415008545\n",
      "Mean Error: 0.00023399620840791613% \n",
      "--------------------------\n",
      "Epoch [2811/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03147\n",
      "128\n",
      "Time elasped: 0.369767427444458\n",
      "Epoch [2812/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02531\n",
      "128\n",
      "Time elasped: 0.3428840637207031\n",
      "Epoch [2813/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02788\n",
      "128\n",
      "Time elasped: 0.3178384304046631\n",
      "Epoch [2814/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03035\n",
      "128\n",
      "Time elasped: 0.3249483108520508\n",
      "Epoch [2815/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03091\n",
      "128\n",
      "Time elasped: 0.31739211082458496\n",
      "Mean Error: 0.0002379942307015881% \n",
      "--------------------------\n",
      "Epoch [2816/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02987\n",
      "128\n",
      "Time elasped: 0.3459329605102539\n",
      "Epoch [2817/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02882\n",
      "128\n",
      "Time elasped: 0.344083309173584\n",
      "Epoch [2818/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02888\n",
      "128\n",
      "Time elasped: 0.31842756271362305\n",
      "Epoch [2819/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03054\n",
      "128\n",
      "Time elasped: 0.3233969211578369\n",
      "Epoch [2820/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02730\n",
      "128\n",
      "Time elasped: 0.34853291511535645\n",
      "Mean Error: 0.0002059707185253501% \n",
      "--------------------------\n",
      "Epoch [2821/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02848\n",
      "128\n",
      "Time elasped: 0.3391757011413574\n",
      "Epoch [2822/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02638\n",
      "128\n",
      "Time elasped: 0.3303489685058594\n",
      "Epoch [2823/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02737\n",
      "128\n",
      "Time elasped: 0.33232641220092773\n",
      "Epoch [2824/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03291\n",
      "128\n",
      "Time elasped: 0.3191390037536621\n",
      "Epoch [2825/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02816\n",
      "128\n",
      "Time elasped: 0.30992555618286133\n",
      "Mean Error: 0.00022938281472306699% \n",
      "--------------------------\n",
      "Epoch [2826/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02846\n",
      "128\n",
      "Time elasped: 0.3267655372619629\n",
      "Epoch [2827/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02818\n",
      "128\n",
      "Time elasped: 0.32311582565307617\n",
      "Epoch [2828/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02696\n",
      "128\n",
      "Time elasped: 0.3346688747406006\n",
      "Epoch [2829/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02861\n",
      "128\n",
      "Time elasped: 0.33168578147888184\n",
      "Epoch [2830/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03008\n",
      "128\n",
      "Time elasped: 0.33443117141723633\n",
      "Mean Error: 0.00020290359680075198% \n",
      "--------------------------\n",
      "Epoch [2831/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02631\n",
      "128\n",
      "Time elasped: 0.3262960910797119\n",
      "Epoch [2832/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02737\n",
      "128\n",
      "Time elasped: 0.31377077102661133\n",
      "Epoch [2833/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02972\n",
      "128\n",
      "Time elasped: 0.33014559745788574\n",
      "Epoch [2834/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02969\n",
      "128\n",
      "Time elasped: 0.34305906295776367\n",
      "Epoch [2835/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02855\n",
      "128\n",
      "Time elasped: 0.3423807621002197\n",
      "Mean Error: 0.0002271232515340671% \n",
      "--------------------------\n",
      "Epoch [2836/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02979\n",
      "128\n",
      "Time elasped: 0.3271653652191162\n",
      "Epoch [2837/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02820\n",
      "128\n",
      "Time elasped: 0.35508036613464355\n",
      "Epoch [2838/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02956\n",
      "128\n",
      "Time elasped: 0.32637691497802734\n",
      "Epoch [2839/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03023\n",
      "128\n",
      "Time elasped: 0.3311624526977539\n",
      "Epoch [2840/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02795\n",
      "128\n",
      "Time elasped: 0.32836246490478516\n",
      "Mean Error: 0.0002234838902950287% \n",
      "--------------------------\n",
      "Epoch [2841/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02976\n",
      "128\n",
      "Time elasped: 0.328838586807251\n",
      "Epoch [2842/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02793\n",
      "128\n",
      "Time elasped: 0.332150936126709\n",
      "Epoch [2843/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02686\n",
      "128\n",
      "Time elasped: 0.33176088333129883\n",
      "Epoch [2844/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03009\n",
      "128\n",
      "Time elasped: 0.31621623039245605\n",
      "Epoch [2845/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03006\n",
      "128\n",
      "Time elasped: 0.3316304683685303\n",
      "Mean Error: 0.00023481562675442547% \n",
      "--------------------------\n",
      "Epoch [2846/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02571\n",
      "128\n",
      "Time elasped: 0.31322813034057617\n",
      "Epoch [2847/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02691\n",
      "128\n",
      "Time elasped: 0.33507418632507324\n",
      "Epoch [2848/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02871\n",
      "128\n",
      "Time elasped: 0.32101964950561523\n",
      "Epoch [2849/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03006\n",
      "128\n",
      "Time elasped: 0.33492612838745117\n",
      "Epoch [2850/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02599\n",
      "128\n",
      "Time elasped: 0.33534765243530273\n",
      "Mean Error: 0.00023245300690177828% \n",
      "--------------------------\n",
      "Epoch [2851/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03045\n",
      "128\n",
      "Time elasped: 0.3326091766357422\n",
      "Epoch [2852/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02870\n",
      "128\n",
      "Time elasped: 0.339815616607666\n",
      "Epoch [2853/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02910\n",
      "128\n",
      "Time elasped: 0.32371091842651367\n",
      "Epoch [2854/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02930\n",
      "128\n",
      "Time elasped: 0.33825111389160156\n",
      "Epoch [2855/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02830\n",
      "128\n",
      "Time elasped: 0.30790114402770996\n",
      "Mean Error: 0.00022632366744801402% \n",
      "--------------------------\n",
      "Epoch [2856/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02634\n",
      "128\n",
      "Time elasped: 0.3249380588531494\n",
      "Epoch [2857/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02625\n",
      "128\n",
      "Time elasped: 0.3224334716796875\n",
      "Epoch [2858/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02589\n",
      "128\n",
      "Time elasped: 0.3238980770111084\n",
      "Epoch [2859/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02581\n",
      "128\n",
      "Time elasped: 0.3138701915740967\n",
      "Epoch [2860/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02927\n",
      "128\n",
      "Time elasped: 0.3356964588165283\n",
      "Mean Error: 0.00022150918084662408% \n",
      "--------------------------\n",
      "Epoch [2861/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02826\n",
      "128\n",
      "Time elasped: 0.3276636600494385\n",
      "Epoch [2862/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02734\n",
      "128\n",
      "Time elasped: 0.3221721649169922\n",
      "Epoch [2863/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02630\n",
      "128\n",
      "Time elasped: 0.33689165115356445\n",
      "Epoch [2864/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02941\n",
      "128\n",
      "Time elasped: 0.32114434242248535\n",
      "Epoch [2865/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02707\n",
      "128\n",
      "Time elasped: 0.3101801872253418\n",
      "Mean Error: 0.0002230375976068899% \n",
      "--------------------------\n",
      "Epoch [2866/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03028\n",
      "128\n",
      "Time elasped: 0.32873988151550293\n",
      "Epoch [2867/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02827\n",
      "128\n",
      "Time elasped: 0.32155776023864746\n",
      "Epoch [2868/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03018\n",
      "128\n",
      "Time elasped: 0.3217010498046875\n",
      "Epoch [2869/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03158\n",
      "128\n",
      "Time elasped: 0.33794140815734863\n",
      "Epoch [2870/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02881\n",
      "128\n",
      "Time elasped: 0.3359694480895996\n",
      "Mean Error: 0.0002192602405557409% \n",
      "--------------------------\n",
      "Epoch [2871/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02870\n",
      "128\n",
      "Time elasped: 0.4079906940460205\n",
      "Epoch [2872/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02996\n",
      "128\n",
      "Time elasped: 0.4616260528564453\n",
      "Epoch [2873/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03082\n",
      "128\n",
      "Time elasped: 0.37822628021240234\n",
      "Epoch [2874/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02782\n",
      "128\n",
      "Time elasped: 0.3400588035583496\n",
      "Epoch [2875/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02923\n",
      "128\n",
      "Time elasped: 0.3512454032897949\n",
      "Mean Error: 0.00021998149168211967% \n",
      "--------------------------\n",
      "Epoch [2876/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02826\n",
      "128\n",
      "Time elasped: 0.3609907627105713\n",
      "Epoch [2877/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02641\n",
      "128\n",
      "Time elasped: 0.3355724811553955\n",
      "Epoch [2878/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02743\n",
      "128\n",
      "Time elasped: 0.34368443489074707\n",
      "Epoch [2879/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02859\n",
      "128\n",
      "Time elasped: 0.3568117618560791\n",
      "Epoch [2880/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02804\n",
      "128\n",
      "Time elasped: 0.3407278060913086\n",
      "Mean Error: 0.00023861801309976727% \n",
      "--------------------------\n",
      "Epoch [2881/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02966\n",
      "128\n",
      "Time elasped: 0.3382599353790283\n",
      "Epoch [2882/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02891\n",
      "128\n",
      "Time elasped: 0.34484314918518066\n",
      "Epoch [2883/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02827\n",
      "128\n",
      "Time elasped: 0.34691762924194336\n",
      "Epoch [2884/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02999\n",
      "128\n",
      "Time elasped: 0.34386587142944336\n",
      "Epoch [2885/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02782\n",
      "128\n",
      "Time elasped: 0.35750675201416016\n",
      "Mean Error: 0.00021701891091652215% \n",
      "--------------------------\n",
      "Epoch [2886/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02911\n",
      "128\n",
      "Time elasped: 0.36559176445007324\n",
      "Epoch [2887/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02603\n",
      "128\n",
      "Time elasped: 0.330031156539917\n",
      "Epoch [2888/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02779\n",
      "128\n",
      "Time elasped: 0.33655405044555664\n",
      "Epoch [2889/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02810\n",
      "128\n",
      "Time elasped: 0.3414154052734375\n",
      "Epoch [2890/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02800\n",
      "128\n",
      "Time elasped: 0.38160085678100586\n",
      "Mean Error: 0.0002223883493570611% \n",
      "--------------------------\n",
      "Epoch [2891/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02715\n",
      "128\n",
      "Time elasped: 0.33486366271972656\n",
      "Epoch [2892/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02890\n",
      "128\n",
      "Time elasped: 0.3476285934448242\n",
      "Epoch [2893/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02887\n",
      "128\n",
      "Time elasped: 0.34014320373535156\n",
      "Epoch [2894/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02817\n",
      "128\n",
      "Time elasped: 0.3449580669403076\n",
      "Epoch [2895/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02593\n",
      "128\n",
      "Time elasped: 0.32747364044189453\n",
      "Mean Error: 0.0002147121267626062% \n",
      "--------------------------\n",
      "Epoch [2896/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02721\n",
      "128\n",
      "Time elasped: 0.3283205032348633\n",
      "Epoch [2897/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02667\n",
      "128\n",
      "Time elasped: 0.3178849220275879\n",
      "Epoch [2898/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.03078\n",
      "128\n",
      "Time elasped: 0.33591699600219727\n",
      "Epoch [2899/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02805\n",
      "128\n",
      "Time elasped: 0.3340761661529541\n",
      "Epoch [2900/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02748\n",
      "128\n",
      "Time elasped: 0.32514286041259766\n",
      "Mean Error: 0.00021322646352928132% \n",
      "--------------------------\n",
      "Epoch [2901/3000], learning_rates 0.000238, 0.237827\n",
      "Step [1/1], Loss: 0.02705\n",
      "128\n",
      "Time elasped: 0.33434462547302246\n",
      "Epoch [2902/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02634\n",
      "128\n",
      "Time elasped: 0.3415791988372803\n",
      "Epoch [2903/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02808\n",
      "128\n",
      "Time elasped: 0.34398889541625977\n",
      "Epoch [2904/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02720\n",
      "128\n",
      "Time elasped: 0.3387777805328369\n",
      "Epoch [2905/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02787\n",
      "128\n",
      "Time elasped: 0.32702088356018066\n",
      "Mean Error: 0.00021165881480555981% \n",
      "--------------------------\n",
      "Epoch [2906/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02731\n",
      "128\n",
      "Time elasped: 0.3134603500366211\n",
      "Epoch [2907/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02942\n",
      "128\n",
      "Time elasped: 0.3109433650970459\n",
      "Epoch [2908/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03014\n",
      "128\n",
      "Time elasped: 0.33811116218566895\n",
      "Epoch [2909/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02913\n",
      "128\n",
      "Time elasped: 0.34315943717956543\n",
      "Epoch [2910/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02836\n",
      "128\n",
      "Time elasped: 0.3245425224304199\n",
      "Mean Error: 0.00025166774867102504% \n",
      "--------------------------\n",
      "Epoch [2911/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03035\n",
      "128\n",
      "Time elasped: 0.3197667598724365\n",
      "Epoch [2912/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02846\n",
      "128\n",
      "Time elasped: 0.32511305809020996\n",
      "Epoch [2913/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02605\n",
      "128\n",
      "Time elasped: 0.34323859214782715\n",
      "Epoch [2914/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03042\n",
      "128\n",
      "Time elasped: 0.31790781021118164\n",
      "Epoch [2915/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02543\n",
      "128\n",
      "Time elasped: 0.332228422164917\n",
      "Mean Error: 0.00021742557873949409% \n",
      "--------------------------\n",
      "Epoch [2916/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02747\n",
      "128\n",
      "Time elasped: 0.31931471824645996\n",
      "Epoch [2917/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02734\n",
      "128\n",
      "Time elasped: 0.34677934646606445\n",
      "Epoch [2918/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02602\n",
      "128\n",
      "Time elasped: 0.33583974838256836\n",
      "Epoch [2919/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02886\n",
      "128\n",
      "Time elasped: 0.3217918872833252\n",
      "Epoch [2920/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02819\n",
      "128\n",
      "Time elasped: 0.3166229724884033\n",
      "Mean Error: 0.00022468139650300145% \n",
      "--------------------------\n",
      "Epoch [2921/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02917\n",
      "128\n",
      "Time elasped: 0.3350033760070801\n",
      "Epoch [2922/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02762\n",
      "128\n",
      "Time elasped: 0.3217141628265381\n",
      "Epoch [2923/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02792\n",
      "128\n",
      "Time elasped: 0.3499011993408203\n",
      "Epoch [2924/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02881\n",
      "128\n",
      "Time elasped: 0.3799471855163574\n",
      "Epoch [2925/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02990\n",
      "128\n",
      "Time elasped: 0.3798694610595703\n",
      "Mean Error: 0.00022018015442881733% \n",
      "--------------------------\n",
      "Epoch [2926/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02764\n",
      "128\n",
      "Time elasped: 0.3579287528991699\n",
      "Epoch [2927/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02943\n",
      "128\n",
      "Time elasped: 0.3270695209503174\n",
      "Epoch [2928/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02919\n",
      "128\n",
      "Time elasped: 0.3336818218231201\n",
      "Epoch [2929/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02710\n",
      "128\n",
      "Time elasped: 0.3179292678833008\n",
      "Epoch [2930/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02840\n",
      "128\n",
      "Time elasped: 0.33790016174316406\n",
      "Mean Error: 0.00020958911045454443% \n",
      "--------------------------\n",
      "Epoch [2931/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02852\n",
      "128\n",
      "Time elasped: 0.32559752464294434\n",
      "Epoch [2932/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02446\n",
      "128\n",
      "Time elasped: 0.3181736469268799\n",
      "Epoch [2933/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02830\n",
      "128\n",
      "Time elasped: 0.33137965202331543\n",
      "Epoch [2934/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02808\n",
      "128\n",
      "Time elasped: 0.3250091075897217\n",
      "Epoch [2935/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02740\n",
      "128\n",
      "Time elasped: 0.3139007091522217\n",
      "Mean Error: 0.00021928618662059307% \n",
      "--------------------------\n",
      "Epoch [2936/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02588\n",
      "128\n",
      "Time elasped: 0.41718173027038574\n",
      "Epoch [2937/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02612\n",
      "128\n",
      "Time elasped: 0.3189218044281006\n",
      "Epoch [2938/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02714\n",
      "128\n",
      "Time elasped: 0.37388038635253906\n",
      "Epoch [2939/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02853\n",
      "128\n",
      "Time elasped: 0.3404695987701416\n",
      "Epoch [2940/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02703\n",
      "128\n",
      "Time elasped: 0.32021355628967285\n",
      "Mean Error: 0.0002301789354532957% \n",
      "--------------------------\n",
      "Epoch [2941/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02637\n",
      "128\n",
      "Time elasped: 0.3280141353607178\n",
      "Epoch [2942/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02803\n",
      "128\n",
      "Time elasped: 0.32451438903808594\n",
      "Epoch [2943/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03040\n",
      "128\n",
      "Time elasped: 0.31868433952331543\n",
      "Epoch [2944/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02964\n",
      "128\n",
      "Time elasped: 0.326141357421875\n",
      "Epoch [2945/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02988\n",
      "128\n",
      "Time elasped: 0.3367125988006592\n",
      "Mean Error: 0.00021574135462287813% \n",
      "--------------------------\n",
      "Epoch [2946/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03132\n",
      "128\n",
      "Time elasped: 0.3168771266937256\n",
      "Epoch [2947/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02872\n",
      "128\n",
      "Time elasped: 0.37009310722351074\n",
      "Epoch [2948/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02887\n",
      "128\n",
      "Time elasped: 0.3599426746368408\n",
      "Epoch [2949/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02646\n",
      "128\n",
      "Time elasped: 0.4127616882324219\n",
      "Epoch [2950/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02796\n",
      "128\n",
      "Time elasped: 0.37515854835510254\n",
      "Mean Error: 0.00022111108410172164% \n",
      "--------------------------\n",
      "Epoch [2951/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02842\n",
      "128\n",
      "Time elasped: 0.4000124931335449\n",
      "Epoch [2952/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02735\n",
      "128\n",
      "Time elasped: 0.367295503616333\n",
      "Epoch [2953/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02506\n",
      "128\n",
      "Time elasped: 0.3324906826019287\n",
      "Epoch [2954/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02987\n",
      "128\n",
      "Time elasped: 0.360856294631958\n",
      "Epoch [2955/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02775\n",
      "128\n",
      "Time elasped: 0.33426904678344727\n",
      "Mean Error: 0.0002322960353922099% \n",
      "--------------------------\n",
      "Epoch [2956/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02630\n",
      "128\n",
      "Time elasped: 0.35897064208984375\n",
      "Epoch [2957/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02743\n",
      "128\n",
      "Time elasped: 0.32686781883239746\n",
      "Epoch [2958/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02749\n",
      "128\n",
      "Time elasped: 0.34217357635498047\n",
      "Epoch [2959/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02606\n",
      "128\n",
      "Time elasped: 0.3294374942779541\n",
      "Epoch [2960/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02712\n",
      "128\n",
      "Time elasped: 0.3404569625854492\n",
      "Mean Error: 0.00022769525821786374% \n",
      "--------------------------\n",
      "Epoch [2961/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02617\n",
      "128\n",
      "Time elasped: 0.32976841926574707\n",
      "Epoch [2962/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02601\n",
      "128\n",
      "Time elasped: 0.37021684646606445\n",
      "Epoch [2963/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02536\n",
      "128\n",
      "Time elasped: 0.3198366165161133\n",
      "Epoch [2964/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02806\n",
      "128\n",
      "Time elasped: 0.3213942050933838\n",
      "Epoch [2965/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02976\n",
      "128\n",
      "Time elasped: 0.3332540988922119\n",
      "Mean Error: 0.00023760741169098765% \n",
      "--------------------------\n",
      "Epoch [2966/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02742\n",
      "128\n",
      "Time elasped: 0.3143322467803955\n",
      "Epoch [2967/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02843\n",
      "128\n",
      "Time elasped: 0.356048583984375\n",
      "Epoch [2968/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02761\n",
      "128\n",
      "Time elasped: 0.3422257900238037\n",
      "Epoch [2969/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02680\n",
      "128\n",
      "Time elasped: 0.33473849296569824\n",
      "Epoch [2970/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02537\n",
      "128\n",
      "Time elasped: 0.3525216579437256\n",
      "Mean Error: 0.00020544443395920098% \n",
      "--------------------------\n",
      "Epoch [2971/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02785\n",
      "128\n",
      "Time elasped: 0.3387782573699951\n",
      "Epoch [2972/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03027\n",
      "128\n",
      "Time elasped: 0.32487940788269043\n",
      "Epoch [2973/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02562\n",
      "128\n",
      "Time elasped: 0.3451693058013916\n",
      "Epoch [2974/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02596\n",
      "128\n",
      "Time elasped: 0.38259220123291016\n",
      "Epoch [2975/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02749\n",
      "128\n",
      "Time elasped: 0.3434872627258301\n",
      "Mean Error: 0.0002470710314810276% \n",
      "--------------------------\n",
      "Epoch [2976/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02768\n",
      "128\n",
      "Time elasped: 0.3171215057373047\n",
      "Epoch [2977/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02894\n",
      "128\n",
      "Time elasped: 0.3297395706176758\n",
      "Epoch [2978/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02904\n",
      "128\n",
      "Time elasped: 0.3302617073059082\n",
      "Epoch [2979/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03046\n",
      "128\n",
      "Time elasped: 0.3331317901611328\n",
      "Epoch [2980/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02800\n",
      "128\n",
      "Time elasped: 0.3270578384399414\n",
      "Mean Error: 0.00019956195319537073% \n",
      "--------------------------\n",
      "Epoch [2981/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02519\n",
      "128\n",
      "Time elasped: 0.3470301628112793\n",
      "Epoch [2982/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02879\n",
      "128\n",
      "Time elasped: 0.32832932472229004\n",
      "Epoch [2983/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02964\n",
      "128\n",
      "Time elasped: 0.32656025886535645\n",
      "Epoch [2984/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02852\n",
      "128\n",
      "Time elasped: 0.3483712673187256\n",
      "Epoch [2985/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02762\n",
      "128\n",
      "Time elasped: 0.33963918685913086\n",
      "Mean Error: 0.00024133037368301302% \n",
      "--------------------------\n",
      "Epoch [2986/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03022\n",
      "128\n",
      "Time elasped: 0.3260231018066406\n",
      "Epoch [2987/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03380\n",
      "128\n",
      "Time elasped: 0.3275141716003418\n",
      "Epoch [2988/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.03031\n",
      "128\n",
      "Time elasped: 0.33882737159729004\n",
      "Epoch [2989/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02783\n",
      "128\n",
      "Time elasped: 0.3359253406524658\n",
      "Epoch [2990/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02874\n",
      "128\n",
      "Time elasped: 0.3425939083099365\n",
      "Mean Error: 0.0002232533588539809% \n",
      "--------------------------\n",
      "Epoch [2991/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02752\n",
      "128\n",
      "Time elasped: 0.30977749824523926\n",
      "Epoch [2992/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02682\n",
      "128\n",
      "Time elasped: 0.3407723903656006\n",
      "Epoch [2993/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02850\n",
      "128\n",
      "Time elasped: 0.3225550651550293\n",
      "Epoch [2994/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02881\n",
      "128\n",
      "Time elasped: 0.32803869247436523\n",
      "Epoch [2995/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02731\n",
      "128\n",
      "Time elasped: 0.3204224109649658\n",
      "Mean Error: 0.0002011229662457481% \n",
      "--------------------------\n",
      "Epoch [2996/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02998\n",
      "128\n",
      "Time elasped: 0.34154629707336426\n",
      "Epoch [2997/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02616\n",
      "128\n",
      "Time elasped: 0.31606388092041016\n",
      "Epoch [2998/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02853\n",
      "128\n",
      "Time elasped: 0.34526801109313965\n",
      "Epoch [2999/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02900\n",
      "128\n",
      "Time elasped: 0.3337228298187256\n",
      "Epoch [3000/3000], learning_rates 0.000226, 0.225936\n",
      "Step [1/1], Loss: 0.02606\n",
      "128\n",
      "Time elasped: 0.3426375389099121\n",
      "Mean Error: 0.00022682051348965615% \n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "lr_tau = 1.0\n",
    "train(snn_f, train_loader, test_loader, 1e-3, num_epochs, ckpt_dir=ckpt_dir, test_behavior=tb_addtask_refact, \n",
    "      scheduler=(100, 0.95), clear=True, lr_tau=lr_tau, printed_steps=10)\n",
    "train(snn_rnn, train_loader, test_loader, 1e-3, num_epochs, ckpt_dir=ckpt_dir, test_behavior=tb_addtask_refact, \n",
    "      scheduler=(100, 0.95), clear=True, lr_tau=lr_tau, printed_steps=10)\n",
    "train(snn_rd, train_loader, test_loader, 1e-3, num_epochs, ckpt_dir=ckpt_dir, test_behavior=tb_addtask_refact, \n",
    "      scheduler=(100, 0.95), clear=True, lr_tau=lr_tau, printed_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'train loss')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from snn_delays.utils.visualization_utils import plot_per_epoch\n",
    "import matplotlib.pyplot as plt\n",
    "loss_error = np.mean(np.array(snn_f.train_loss)[:,1][-100:])\n",
    "plot_per_epoch(snn_f.train_loss, label = 'feedforward')\n",
    "plot_per_epoch(snn_rnn.train_loss, label = 'recurrent')\n",
    "plot_per_epoch(snn_rd.train_loss, label = 'delays')\n",
    "#plt.title(f'loss error = {loss_error}')\n",
    "\n",
    "plt.ylim(0, 0.3)\n",
    "plt.title('train loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_f.debug = True\n",
    "snn_rnn.debug = True\n",
    "snn_rd.debug = True\n",
    "snn_f.init_state_logger()\n",
    "snn_rnn.init_state_logger()\n",
    "snn_rd.init_state_logger()\n",
    "\n",
    "from snn_delays.utils.train_utils_refact_minimal import propagate_batch_simple\n",
    "images, labels_mf = propagate_batch_simple(snn_f, test_loader)\n",
    "_, labels_rnn = propagate_batch_simple(snn_rnn, test_loader)\n",
    "_, labels_rd = propagate_batch_simple(snn_rd, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_mf = labels_mf[:,:int(0.1*time_window), :].permute(1, 0, 2).cpu().numpy()\n",
    "out_mf = snn_f.mem_state['output'][int(0.9*time_window):].detach().cpu().numpy()\n",
    "\n",
    "ref_rnn = labels_rnn[:,:int(0.1*time_window), :].permute(1, 0, 2).cpu().numpy()\n",
    "out_rnn = snn_rnn.mem_state['output'][int(0.9*time_window):].detach().cpu().numpy()\n",
    "\n",
    "ref_rd = labels_rd[:,:int(0.1*time_window),:].permute(1, 0, 2).cpu().numpy()\n",
    "out_rd = snn_rd.mem_state['output'][int(0.9*time_window):].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661.55994\n",
      "720.9131\n",
      "416.53506\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(ref_mf - out_mf).sum())\n",
    "print(np.abs(ref_rnn - out_rnn).sum())\n",
    "print(np.abs(ref_rd - out_rd).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snn_delays.utils.visualization_utils import plot_add_task, plot_add_task2, plot_add_task3, plot_add_task4\n",
    "# Create a figure with 3 rows and N columns (for stacking results horizontally)\n",
    "\n",
    "def plot_add_task(output, reference, N, axes=None, name=''):\n",
    "    \n",
    "\n",
    "    ns = 50 # num samples to display for better viz\n",
    "\n",
    "    ref = reference[:, :ns , N]\n",
    "    out = output[:, :ns , N]\n",
    "    diff = abs(ref-out)\n",
    "\n",
    "    print(np.mean(diff))\n",
    "\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(5, 10))  # Create a new figure if axes not provided\n",
    "\n",
    "    axes[0].imshow(ref, vmin=0, vmax=0.9)\n",
    "    axes[0].set_title('Reference '+name)\n",
    "    axes[0].set_ylabel('Time')\n",
    "\n",
    "    axes[1].imshow(out, vmin=0, vmax=0.9)\n",
    "    axes[1].set_title('Output')\n",
    "    axes[1].set_ylabel('Time')\n",
    "\n",
    "    axes[2].imshow(diff, vmin=0.0, vmax=1.0)\n",
    "    axes[2].set_title('Difference')\n",
    "    axes[2].set_ylabel('Time')\n",
    "    axes[2].set_xlabel('Training Sample')\n",
    "\n",
    "    return axes  # Return the axes to be used in an external figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2087092\n",
      "0.2499079\n",
      "0.13652068\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 3))  # 3 rows, 2 columns\n",
    "\n",
    "plot_add_task(out_mf, ref_mf, 0, axes[:, 0])  \n",
    "plot_add_task(out_rnn, ref_rnn, 0, axes[:, 1])  \n",
    "plot_add_task(out_rd, ref_rd, 0, axes[:, 2])  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2150098\n",
      "0.22811036\n",
      "0.15741517\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 3))  # 3 rows, 2 columns\n",
    "\n",
    "plot_add_task(out_mf, ref_mf, 1, axes[:, 0])  \n",
    "plot_add_task(out_rnn, ref_rnn, 1, axes[:, 1])  \n",
    "plot_add_task(out_rd, ref_rd, 1, axes[:, 2])  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21644232\n",
      "0.21513623\n",
      "0.14535967\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 3))  # 3 rows, 2 columns\n",
    "\n",
    "plot_add_task(out_mf, ref_mf, 2, axes[:, 0])  \n",
    "plot_add_task(out_rnn, ref_rnn, 2, axes[:, 1])  \n",
    "plot_add_task(out_rd, ref_rd, 2, axes[:, 2])  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21923643\n",
      "0.20317028\n",
      "0.1449566\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 3))  # 3 rows, 2 columns\n",
    "\n",
    "plot_add_task(out_mf, ref_mf, 3, axes[:, 0])  \n",
    "plot_add_task(out_rnn, ref_rnn, 3, axes[:, 1])  \n",
    "plot_add_task(out_rd, ref_rd, 3, axes[:, 2])  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2132532\n",
      "0.20638739\n",
      "0.05431114\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 3))  # 3 rows, 2 columns\n",
    "\n",
    "plot_add_task(out_mf, ref_mf, 4, axes[:, 0])  \n",
    "plot_add_task(out_rnn, ref_rnn, 4, axes[:, 1])  \n",
    "plot_add_task(out_rd, ref_rd, 4, axes[:, 2])  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
