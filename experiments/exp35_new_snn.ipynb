{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n",
      "[CropTime(min=0, max=1000000.0), ToFrame(sensor_size=(700, 1, 1), time_window=None, event_count=None, n_time_bins=50, n_event_bins=None, overlap=0, include_incomplete=False)]\n",
      "\n",
      "[INFO] Delays: tensor([ 0, 16, 32])\n",
      "\n",
      "[INFO] Delays i: tensor([0])\n",
      "\n",
      "[INFO] Delays h: tensor([ 0, 16, 32])\n",
      "\n",
      "[INFO] Delays o: tensor([0])\n",
      "1000.0\n",
      "Delta t: 20.0 ms\n",
      "mean of normal: -0.541324854612918\n",
      "1000.0\n",
      "Delta t: 20.0 ms\n",
      "mean of normal: -0.541324854612918\n",
      "1000.0\n",
      "Delta t: 20.0 ms\n",
      "mean of normal: -0.541324854612918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (layers): ModuleList(\n",
       "    (0): FeedforwardSNNLayer(\n",
       "      (linear): Linear(in_features=700, out_features=64, bias=False)\n",
       "    )\n",
       "    (1): FeedforwardSNNLayer(\n",
       "      (linear): Linear(in_features=64, out_features=64, bias=False)\n",
       "    )\n",
       "    (2): FeedforwardSNNLayer(\n",
       "      (linear): Linear(in_features=64, out_features=20, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from snn_delays.snn_refactoring import SNN\n",
    "from snn_delays.utils.dataset_loader import DatasetLoader\n",
    "from snn_delays.utils.train_utils_refact import train, get_device, propagate_batch\n",
    "from snn_delays.utils.test_behavior import tb_save_max_last_acc\n",
    "\n",
    "'''\n",
    "SHD dataset as in ablation study\n",
    "'''\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(10)\n",
    "\n",
    "dataset = 'shd'\n",
    "total_time = 50\n",
    "batch_size = 1024\n",
    "\n",
    "# DATASET\n",
    "DL = DatasetLoader(dataset=dataset,\n",
    "                  caching='memory',\n",
    "                  num_workers=0,\n",
    "                  batch_size=batch_size,\n",
    "                  total_time=total_time,\n",
    "                  crop_to=1e6)\n",
    "train_loader, test_loader, dataset_dict = DL.get_dataloaders()\n",
    "          \n",
    "num_epochs = 50\n",
    "\n",
    "lr = 1e-3\n",
    "# SNN CON DELAYS\n",
    "taimu1 = time.time()\n",
    "\n",
    "tau_m = 'normal'\n",
    "delay = (48,16)\n",
    "ckpt_dir = 'exp3_shd50_rnn' \n",
    "\n",
    "snn = SNN(dataset_dict=dataset_dict, structure=(64, 2), connection_type='f',\n",
    "    delay=delay, delay_type='h', tau_m = tau_m,\n",
    "    win=total_time, loss_fn='mem_sum', batch_size=batch_size, device=device,\n",
    "    debug=False)\n",
    "\n",
    "snn.use_amp = False\n",
    "\n",
    "snn.set_layers()\n",
    "snn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shd50_l2_48d16.t7 for 50 epochs...\n",
      "Epoch [1/50], learning_rates 0.001000, 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\autograd\\__init__.py:266: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\asyncio\\base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\asyncio\\base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Alberto\\AppData\\Local\\Temp\\ipykernel_29244\\3084186708.py\", line 2, in <module>\n",
      "    train(snn, train_loader, test_loader, lr, num_epochs, dropout=0.0,\n",
      "  File \"C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\utils\\train_utils_refact.py\", line 97, in train\n",
      "    snn.train_step(train_loader,\n",
      "  File \"C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\snn_refactoring.py\", line 219, in train_step\n",
      "    outputs, reference = self.propagate(images, labels)\n",
      "  File \"C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\snn_refactoring.py\", line 146, in propagate\n",
      "    all_o_mems, all_o_spikes = self(images)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\snn_refactoring.py\", line 880, in forward\n",
      "    spikes = layer(spikes).clone()  # Each layer updates its own activations\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\snn_refactoring.py\", line 501, in forward\n",
      "    self.mems, self.spikes = self.update_mem(\n",
      "  File \"C:\\Users\\Alberto\\Python\\SNNdelays\\snn_delays\\snn_refactoring.py\", line 482, in update_mem\n",
      "    mem = mem * alpha * (1 - o_spike) + self.linear(i_spike)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 116, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:118.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 20]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m train(snn, train_loader, test_loader, lr, num_epochs, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \n\u001b[0;32m      3\u001b[0m     test_behavior\u001b[38;5;241m=\u001b[39mtb_save_max_last_acc, ckpt_dir\u001b[38;5;241m=\u001b[39mckpt_dir, scheduler\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.95\u001b[39m), test_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\utils\\train_utils_refact.py:97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(snn, train_loader, test_loader, learning_rate, num_epochs, spk_reg, l1_reg, dropout, lr_tau, scheduler, ckpt_dir, test_behavior, test_every, delay_pruning, weight_pruning, lsm, random_delay_pruning, weight_quantization, k, depth, freeze_taus, verbose, streamlit, clear)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m], learning_rates \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_epochs,\n\u001b[0;32m     93\u001b[0m                                                  current_lr, current_lr_tau), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     snn\u001b[38;5;241m.\u001b[39mtrain_step(train_loader,\n\u001b[0;32m     98\u001b[0m                 optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     99\u001b[0m                 scheduler \u001b[38;5;241m=\u001b[39m scheduler,\n\u001b[0;32m    100\u001b[0m                 spk_reg\u001b[38;5;241m=\u001b[39mspk_reg,\n\u001b[0;32m    101\u001b[0m                 l1_reg\u001b[38;5;241m=\u001b[39ml1_reg,\n\u001b[0;32m    102\u001b[0m                 dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m    103\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39mverbose)        \n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     snn\u001b[38;5;241m.\u001b[39mtrain_step_tr(train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m    106\u001b[0m                     criterion\u001b[38;5;241m=\u001b[39msnn\u001b[38;5;241m.\u001b[39mcriterion, spk_reg\u001b[38;5;241m=\u001b[39mspk_reg,\n\u001b[0;32m    107\u001b[0m                     depth\u001b[38;5;241m=\u001b[39mdepth, k\u001b[38;5;241m=\u001b[39mk, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\snn_refactoring.py:248\u001b[0m, in \u001b[0;36mTraining.train_step\u001b[1;34m(self, train_loader, optimizer, scheduler, spk_reg, l1_reg, dropout, verbose)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 248\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    249\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 20]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "train(snn, train_loader, test_loader, lr, num_epochs, dropout=0.0, \n",
    "    test_behavior=tb_save_max_last_acc, ckpt_dir=ckpt_dir, scheduler=(100, 0.95), test_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
