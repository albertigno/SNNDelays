{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from snn_delays.snn import SNN\n",
    "from snn_delays.utils.dataset_loader import DatasetLoader\n",
    "from snn_delays.utils.train_utils import train, get_device, propagate_batch, to_plot, set_seed\n",
    "from snn_delays.utils.test_behavior import tb_addtask\n",
    "from snn_delays.utils.visualization_utils import plot_taus\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have unique batches in \"episodic\" sampling or procedural sampling, we must disable caching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_seed(42)\n",
    "\n",
    "time_window = 50\n",
    "batch_size = 128 # 128: anil kag\n",
    "\n",
    "ckpt_dir = 'sequential_tests_24_2_25'\n",
    "\n",
    "dataset = 'addtask_episodic'\n",
    "#dataset = 'addtask' \n",
    "\n",
    "DL = DatasetLoader(dataset=dataset, caching='', num_workers=0, batch_size=batch_size, total_time=time_window)\n",
    "train_loader, test_loader, dataset_dict = DL.get_dataloaders()\n",
    "dataset_dict[\"time_ms\"] = 2e3\n",
    "#dataset_dict[\"time_ms\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Delays: tensor([0])\n",
      "\n",
      "[INFO] Delays i: tensor([0])\n",
      "\n",
      "[INFO] Delays h: tensor([0])\n",
      "\n",
      "[INFO] Delays o: tensor([0])\n",
      "2000.0\n",
      "Delta t: 40.0 ms\n",
      "mean of normal: -1.8545865421311407\n"
     ]
    }
   ],
   "source": [
    "structure = (10, 2)\n",
    "snn_mf = SNN(dataset_dict, structure=structure, connection_type='mf', delay=None, delay_type='',\n",
    "           reset_to_zero=False, win=time_window, loss_fn='mem_prediction', batch_size=batch_size, device=device, debug=True)\n",
    "\n",
    "snn_mf.multi_proj = 50\n",
    "\n",
    "snn_mf.set_network()\n",
    "snn_mf.use_amp = False\n",
    "snn_mf.model_name = f'addask_d_50_50'\n",
    "snn_mf.input2spike_th = None\n",
    "snn_mf.num_train_samples = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training addask_d_50_50 for 10 epochs...\n",
      "Epoch [1/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[1.1900, 1.1900, 1.1900,  ..., 1.1900, 1.1900, 1.1900],\n",
      "        [1.4651, 1.4651, 1.4651,  ..., 1.4651, 1.4651, 1.4651],\n",
      "        [0.5976, 0.5976, 0.5976,  ..., 0.5976, 0.5976, 0.5976],\n",
      "        ...,\n",
      "        [0.9045, 0.9045, 0.9045,  ..., 0.9045, 0.9045, 0.9045],\n",
      "        [0.8546, 0.8546, 0.8546,  ..., 0.8546, 0.8546, 0.8546],\n",
      "        [1.2614, 1.2614, 1.2614,  ..., 1.2614, 1.2614, 1.2614]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.75193\n",
      "Time elasped: 0.37712907791137695\n",
      "Epoch [2/10], learning_rates 0.001000, 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([128, 10])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1978, 1.1978, 1.1978,  ..., 1.1978, 1.1978, 1.1978],\n",
      "        [0.7947, 0.7947, 0.7947,  ..., 0.7947, 0.7947, 0.7947],\n",
      "        [1.6192, 1.6192, 1.6192,  ..., 1.6192, 1.6192, 1.6192],\n",
      "        ...,\n",
      "        [0.6750, 0.6750, 0.6750,  ..., 0.6750, 0.6750, 0.6750],\n",
      "        [1.5603, 1.5603, 1.5603,  ..., 1.5603, 1.5603, 1.5603],\n",
      "        [1.2609, 1.2609, 1.2609,  ..., 1.2609, 1.2609, 1.2609]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.77050\n",
      "Time elasped: 0.18027019500732422\n",
      "Epoch [3/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[1.5743, 1.5743, 1.5743,  ..., 1.5743, 1.5743, 1.5743],\n",
      "        [0.8114, 0.8114, 0.8114,  ..., 0.8114, 0.8114, 0.8114],\n",
      "        [0.7592, 0.7592, 0.7592,  ..., 0.7592, 0.7592, 0.7592],\n",
      "        ...,\n",
      "        [0.3869, 0.3869, 0.3869,  ..., 0.3869, 0.3869, 0.3869],\n",
      "        [0.6577, 0.6577, 0.6577,  ..., 0.6577, 0.6577, 0.6577],\n",
      "        [1.1904, 1.1904, 1.1904,  ..., 1.1904, 1.1904, 1.1904]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.51145\n",
      "Time elasped: 0.17543411254882812\n",
      "Epoch [4/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[0.9582, 0.9582, 0.9582,  ..., 0.9582, 0.9582, 0.9582],\n",
      "        [1.0341, 1.0341, 1.0341,  ..., 1.0341, 1.0341, 1.0341],\n",
      "        [1.5394, 1.5394, 1.5394,  ..., 1.5394, 1.5394, 1.5394],\n",
      "        ...,\n",
      "        [1.4706, 1.4706, 1.4706,  ..., 1.4706, 1.4706, 1.4706],\n",
      "        [1.1568, 1.1568, 1.1568,  ..., 1.1568, 1.1568, 1.1568],\n",
      "        [0.9349, 0.9349, 0.9349,  ..., 0.9349, 0.9349, 0.9349]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.50751\n",
      "Time elasped: 0.17103123664855957\n",
      "Epoch [5/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[1.3415, 1.3415, 1.3415,  ..., 1.3415, 1.3415, 1.3415],\n",
      "        [1.0323, 1.0323, 1.0323,  ..., 1.0323, 1.0323, 1.0323],\n",
      "        [1.7522, 1.7522, 1.7522,  ..., 1.7522, 1.7522, 1.7522],\n",
      "        ...,\n",
      "        [0.6948, 0.6948, 0.6948,  ..., 0.6948, 0.6948, 0.6948],\n",
      "        [1.2013, 1.2013, 1.2013,  ..., 1.2013, 1.2013, 1.2013],\n",
      "        [1.4945, 1.4945, 1.4945,  ..., 1.4945, 1.4945, 1.4945]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.44298\n",
      "Time elasped: 0.1685347557067871\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\sequential_tests_24_2_25\n",
      "Mean Error: 0.0029220941942185163% \n",
      "--------------------------\n",
      "Epoch [6/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[0.7932, 0.7932, 0.7932,  ..., 0.7932, 0.7932, 0.7932],\n",
      "        [1.9000, 1.9000, 1.9000,  ..., 1.9000, 1.9000, 1.9000],\n",
      "        [1.3885, 1.3885, 1.3885,  ..., 1.3885, 1.3885, 1.3885],\n",
      "        ...,\n",
      "        [1.0955, 1.0955, 1.0955,  ..., 1.0955, 1.0955, 1.0955],\n",
      "        [0.8990, 0.8990, 0.8990,  ..., 0.8990, 0.8990, 0.8990],\n",
      "        [1.0573, 1.0573, 1.0573,  ..., 1.0573, 1.0573, 1.0573]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.39704\n",
      "Time elasped: 0.1590104103088379\n",
      "Epoch [7/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000],\n",
      "        [1.0848, 1.0848, 1.0848,  ..., 1.0848, 1.0848, 1.0848],\n",
      "        [0.9888, 0.9888, 0.9888,  ..., 0.9888, 0.9888, 0.9888],\n",
      "        ...,\n",
      "        [0.8986, 0.8986, 0.8986,  ..., 0.8986, 0.8986, 0.8986],\n",
      "        [1.6178, 1.6178, 1.6178,  ..., 1.6178, 1.6178, 1.6178],\n",
      "        [1.7755, 1.7755, 1.7755,  ..., 1.7755, 1.7755, 1.7755]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.37604\n",
      "Time elasped: 0.15304946899414062\n",
      "Epoch [8/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[1.4356, 1.4356, 1.4356,  ..., 1.4356, 1.4356, 1.4356],\n",
      "        [0.5525, 0.5525, 0.5525,  ..., 0.5525, 0.5525, 0.5525],\n",
      "        [0.6391, 0.6391, 0.6391,  ..., 0.6391, 0.6391, 0.6391],\n",
      "        ...,\n",
      "        [1.3214, 1.3214, 1.3214,  ..., 1.3214, 1.3214, 1.3214],\n",
      "        [1.0653, 1.0653, 1.0653,  ..., 1.0653, 1.0653, 1.0653],\n",
      "        [0.7342, 0.7342, 0.7342,  ..., 0.7342, 0.7342, 0.7342]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.27886\n",
      "Time elasped: 0.17054963111877441\n",
      "Epoch [9/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[0.8896, 0.8896, 0.8896,  ..., 0.8896, 0.8896, 0.8896],\n",
      "        [0.6248, 0.6248, 0.6248,  ..., 0.6248, 0.6248, 0.6248],\n",
      "        [1.0366, 1.0366, 1.0366,  ..., 1.0366, 1.0366, 1.0366],\n",
      "        ...,\n",
      "        [1.1417, 1.1417, 1.1417,  ..., 1.1417, 1.1417, 1.1417],\n",
      "        [1.3423, 1.3423, 1.3423,  ..., 1.3423, 1.3423, 1.3423],\n",
      "        [1.6014, 1.6014, 1.6014,  ..., 1.6014, 1.6014, 1.6014]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.34022\n",
      "Time elasped: 0.15436935424804688\n",
      "Epoch [10/10], learning_rates 0.001000, 0.100000\n",
      "tensor([[0.5110, 0.5110, 0.5110,  ..., 0.5110, 0.5110, 0.5110],\n",
      "        [1.0661, 1.0661, 1.0661,  ..., 1.0661, 1.0661, 1.0661],\n",
      "        [1.4916, 1.4916, 1.4916,  ..., 1.4916, 1.4916, 1.4916],\n",
      "        ...,\n",
      "        [1.5216, 1.5216, 1.5216,  ..., 1.5216, 1.5216, 1.5216],\n",
      "        [1.1805, 1.1805, 1.1805,  ..., 1.1805, 1.1805, 1.1805],\n",
      "        [1.5603, 1.5603, 1.5603,  ..., 1.5603, 1.5603, 1.5603]],\n",
      "       device='cuda:0')\n",
      "Step [1/1], Loss: 0.35613\n",
      "Time elasped: 0.1679370403289795\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\sequential_tests_24_2_25\n",
      "Mean Error: 0.003272701520472765% \n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "train(snn_mf, train_loader, test_loader, 1e-3, 10, ckpt_dir=ckpt_dir, test_behavior=tb_addtask, scheduler=(100, 0.95), freeze_taus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7136078\n",
      "--------------\n",
      "1.5091248\n",
      "--------------\n",
      "1.021637\n",
      "--------------\n",
      "1.4748383\n",
      "--------------\n",
      "1.2240906\n",
      "--------------\n",
      "1.5293121\n",
      "--------------\n",
      "1.0679016\n",
      "--------------\n",
      "1.3389587\n",
      "--------------\n",
      "1.1376495\n",
      "--------------\n",
      "1.0477448\n",
      "--------------\n",
      "1.5675507\n",
      "--------------\n",
      "0.06889343\n",
      "--------------\n",
      "0.70092773\n",
      "--------------\n",
      "0.9018402\n",
      "--------------\n",
      "0.8581848\n",
      "--------------\n",
      "0.20864868\n",
      "--------------\n",
      "1.5592041\n",
      "--------------\n",
      "0.21469116\n",
      "--------------\n",
      "0.44506836\n",
      "--------------\n",
      "0.33380127\n",
      "--------------\n",
      "0.708786\n",
      "--------------\n",
      "0.36499023\n",
      "--------------\n",
      "1.3300629\n",
      "--------------\n",
      "0.87161255\n",
      "--------------\n",
      "0.23083496\n",
      "--------------\n",
      "0.7825012\n",
      "--------------\n",
      "1.4906464\n",
      "--------------\n",
      "1.4910431\n",
      "--------------\n",
      "1.0254669\n",
      "--------------\n",
      "1.6647186\n",
      "--------------\n",
      "1.4189453\n",
      "--------------\n",
      "0.90501404\n",
      "--------------\n",
      "0.30986023\n",
      "--------------\n",
      "1.3889923\n",
      "--------------\n",
      "0.87342834\n",
      "--------------\n",
      "1.0052185\n",
      "--------------\n",
      "0.6659088\n",
      "--------------\n",
      "0.6404114\n",
      "--------------\n",
      "1.1413574\n",
      "--------------\n",
      "1.1138306\n",
      "--------------\n",
      "1.22995\n",
      "--------------\n",
      "1.4955902\n",
      "--------------\n",
      "0.93652344\n",
      "--------------\n",
      "1.0824127\n",
      "--------------\n",
      "1.5701904\n",
      "--------------\n",
      "1.7877808\n",
      "--------------\n",
      "0.9257965\n",
      "--------------\n",
      "1.3299866\n",
      "--------------\n",
      "1.147644\n",
      "--------------\n",
      "0.91319275\n",
      "--------------\n",
      "1.2007141\n",
      "--------------\n",
      "1.0640717\n",
      "--------------\n",
      "1.2700806\n",
      "--------------\n",
      "0.47747803\n",
      "--------------\n",
      "1.434906\n",
      "--------------\n",
      "0.7400055\n",
      "--------------\n",
      "0.30439758\n",
      "--------------\n",
      "1.1838226\n",
      "--------------\n",
      "1.0370636\n",
      "--------------\n",
      "0.6811218\n",
      "--------------\n",
      "1.418396\n",
      "--------------\n",
      "1.2046051\n",
      "--------------\n",
      "1.3871613\n",
      "--------------\n",
      "0.8860016\n",
      "--------------\n",
      "0.32406616\n",
      "--------------\n",
      "1.2507324\n",
      "--------------\n",
      "1.1486816\n",
      "--------------\n",
      "0.1824646\n",
      "--------------\n",
      "1.3708496\n",
      "--------------\n",
      "0.8203125\n",
      "--------------\n",
      "0.22921753\n",
      "--------------\n",
      "0.5249634\n",
      "--------------\n",
      "0.55558777\n",
      "--------------\n",
      "1.3697052\n",
      "--------------\n",
      "0.98243713\n",
      "--------------\n",
      "0.8461609\n",
      "--------------\n",
      "0.51275635\n",
      "--------------\n",
      "1.063797\n",
      "--------------\n",
      "1.7457123\n",
      "--------------\n",
      "0.9443512\n",
      "--------------\n",
      "0.45887756\n",
      "--------------\n",
      "1.3804474\n",
      "--------------\n",
      "1.3280487\n",
      "--------------\n",
      "0.64993286\n",
      "--------------\n",
      "1.7127533\n",
      "--------------\n",
      "0.9259033\n",
      "--------------\n",
      "0.5401764\n",
      "--------------\n",
      "1.5547485\n",
      "--------------\n",
      "1.2389679\n",
      "--------------\n",
      "1.5999146\n",
      "--------------\n",
      "1.0716858\n",
      "--------------\n",
      "0.9796448\n",
      "--------------\n",
      "0.78022766\n",
      "--------------\n",
      "1.1643982\n",
      "--------------\n",
      "1.0199127\n",
      "--------------\n",
      "1.0699921\n",
      "--------------\n",
      "0.43855286\n",
      "--------------\n",
      "1.1727905\n",
      "--------------\n",
      "1.0546875\n",
      "--------------\n",
      "0.3052063\n",
      "--------------\n",
      "0.8952942\n",
      "--------------\n",
      "1.0752716\n",
      "--------------\n",
      "0.5552826\n",
      "--------------\n",
      "0.5215454\n",
      "--------------\n",
      "0.87976074\n",
      "--------------\n",
      "1.8935394\n",
      "--------------\n",
      "0.9916382\n",
      "--------------\n",
      "0.39112854\n",
      "--------------\n",
      "0.53189087\n",
      "--------------\n",
      "1.1076965\n",
      "--------------\n",
      "1.5594482\n",
      "--------------\n",
      "1.4376984\n",
      "--------------\n",
      "1.1033325\n",
      "--------------\n",
      "1.342041\n",
      "--------------\n",
      "0.6923981\n",
      "--------------\n",
      "1.0254059\n",
      "--------------\n",
      "0.96347046\n",
      "--------------\n",
      "0.52033997\n",
      "--------------\n",
      "1.6781616\n",
      "--------------\n",
      "1.30896\n",
      "--------------\n",
      "0.5012512\n",
      "--------------\n",
      "1.3389282\n",
      "--------------\n",
      "1.1759033\n",
      "--------------\n",
      "1.3315735\n",
      "--------------\n",
      "1.3682861\n",
      "--------------\n",
      "1.2175903\n",
      "--------------\n",
      "0.4677887\n",
      "--------------\n",
      "0.7483978\n",
      "--------------\n",
      "0.9350281\n",
      "--------------\n",
      "1.0074768\n",
      "--------------\n",
      "0.18400574\n",
      "--------------\n",
      "0.5855408\n",
      "--------------\n",
      "0.6530762\n",
      "--------------\n",
      "1.5908966\n",
      "--------------\n",
      "0.8038025\n",
      "--------------\n",
      "0.8560028\n",
      "--------------\n",
      "0.947525\n",
      "--------------\n",
      "0.70214844\n",
      "--------------\n",
      "1.2045441\n",
      "--------------\n",
      "1.3924103\n",
      "--------------\n",
      "1.0171661\n",
      "--------------\n",
      "0.48526\n",
      "--------------\n",
      "0.6407776\n",
      "--------------\n",
      "1.0481415\n",
      "--------------\n",
      "1.0936432\n",
      "--------------\n",
      "0.891449\n",
      "--------------\n",
      "1.9038696\n",
      "--------------\n",
      "0.1920929\n",
      "--------------\n",
      "1.0009613\n",
      "--------------\n",
      "0.6765442\n",
      "--------------\n",
      "1.1486206\n",
      "--------------\n",
      "0.6537628\n",
      "--------------\n",
      "1.0710602\n",
      "--------------\n",
      "0.8760681\n",
      "--------------\n",
      "1.0241241\n",
      "--------------\n",
      "1.0220184\n",
      "--------------\n",
      "1.0058594\n",
      "--------------\n",
      "0.81539917\n",
      "--------------\n",
      "1.6450043\n",
      "--------------\n",
      "1.4345093\n",
      "--------------\n",
      "1.2453766\n",
      "--------------\n",
      "1.241745\n",
      "--------------\n",
      "1.1808014\n",
      "--------------\n",
      "1.0214081\n",
      "--------------\n",
      "0.73109436\n",
      "--------------\n",
      "0.9599304\n",
      "--------------\n",
      "1.3400879\n",
      "--------------\n",
      "1.1902466\n",
      "--------------\n",
      "1.4824677\n",
      "--------------\n",
      "0.60961914\n",
      "--------------\n",
      "0.95010376\n",
      "--------------\n",
      "1.4006348\n",
      "--------------\n",
      "0.90470886\n",
      "--------------\n",
      "0.5357971\n",
      "--------------\n",
      "1.4749756\n",
      "--------------\n",
      "0.93803406\n",
      "--------------\n",
      "1.4699402\n",
      "--------------\n",
      "0.48002625\n",
      "--------------\n",
      "0.649704\n",
      "--------------\n",
      "0.6294403\n",
      "--------------\n",
      "1.1791992\n",
      "--------------\n",
      "0.7433319\n",
      "--------------\n",
      "0.9728241\n",
      "--------------\n",
      "0.9702301\n",
      "--------------\n",
      "0.21388245\n",
      "--------------\n",
      "1.0553741\n",
      "--------------\n",
      "1.0658417\n",
      "--------------\n",
      "1.8353424\n",
      "--------------\n",
      "0.9445343\n",
      "--------------\n",
      "0.7517395\n",
      "--------------\n",
      "1.5484467\n",
      "--------------\n",
      "0.8682251\n",
      "--------------\n",
      "1.0522766\n",
      "--------------\n",
      "1.0069275\n",
      "--------------\n",
      "1.2612762\n",
      "--------------\n",
      "0.8488922\n",
      "--------------\n",
      "0.33106995\n",
      "--------------\n",
      "1.7741241\n",
      "--------------\n",
      "0.8766022\n",
      "--------------\n",
      "0.18952942\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "refs = []\n",
    "for x in range(200):\n",
    "        _, labels = propagate_batch(snn_mf, train_loader)\n",
    "        ref = labels[:,5:,0].T.cpu().numpy()\n",
    "        out = snn_mf.mem_state['output'][int(0.9*time_window):].detach().cpu().numpy()\n",
    "\n",
    "        #print(labels)\n",
    "        print(ref[0][0])\n",
    "        print('--------------')\n",
    "        refs.append(ref[0][0])\n",
    "        #print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [float(ref) for ref in refs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7136077880859375,\n",
       " 1.509124755859375,\n",
       " 1.021636962890625,\n",
       " 1.4748382568359375,\n",
       " 1.224090576171875,\n",
       " 1.5293121337890625,\n",
       " 1.067901611328125,\n",
       " 1.338958740234375,\n",
       " 1.1376495361328125,\n",
       " 1.0477447509765625,\n",
       " 1.5675506591796875,\n",
       " 0.0688934326171875,\n",
       " 0.700927734375,\n",
       " 0.9018402099609375,\n",
       " 0.858184814453125,\n",
       " 0.208648681640625,\n",
       " 1.5592041015625,\n",
       " 0.214691162109375,\n",
       " 0.445068359375,\n",
       " 0.33380126953125,\n",
       " 0.7087860107421875,\n",
       " 0.364990234375,\n",
       " 1.3300628662109375,\n",
       " 0.871612548828125,\n",
       " 0.2308349609375,\n",
       " 0.782501220703125,\n",
       " 1.4906463623046875,\n",
       " 1.4910430908203125,\n",
       " 1.0254669189453125,\n",
       " 1.6647186279296875,\n",
       " 1.4189453125,\n",
       " 0.9050140380859375,\n",
       " 0.3098602294921875,\n",
       " 1.3889923095703125,\n",
       " 0.8734283447265625,\n",
       " 1.005218505859375,\n",
       " 0.6659088134765625,\n",
       " 0.640411376953125,\n",
       " 1.141357421875,\n",
       " 1.11383056640625,\n",
       " 1.229949951171875,\n",
       " 1.4955902099609375,\n",
       " 0.9365234375,\n",
       " 1.0824127197265625,\n",
       " 1.5701904296875,\n",
       " 1.78778076171875,\n",
       " 0.9257965087890625,\n",
       " 1.329986572265625,\n",
       " 1.14764404296875,\n",
       " 0.9131927490234375,\n",
       " 1.200714111328125,\n",
       " 1.0640716552734375,\n",
       " 1.27008056640625,\n",
       " 0.47747802734375,\n",
       " 1.434906005859375,\n",
       " 0.7400054931640625,\n",
       " 0.3043975830078125,\n",
       " 1.1838226318359375,\n",
       " 1.0370635986328125,\n",
       " 0.681121826171875,\n",
       " 1.41839599609375,\n",
       " 1.2046051025390625,\n",
       " 1.3871612548828125,\n",
       " 0.8860015869140625,\n",
       " 0.324066162109375,\n",
       " 1.250732421875,\n",
       " 1.148681640625,\n",
       " 0.182464599609375,\n",
       " 1.370849609375,\n",
       " 0.8203125,\n",
       " 0.229217529296875,\n",
       " 0.52496337890625,\n",
       " 0.5555877685546875,\n",
       " 1.3697052001953125,\n",
       " 0.9824371337890625,\n",
       " 0.846160888671875,\n",
       " 0.51275634765625,\n",
       " 1.0637969970703125,\n",
       " 1.7457122802734375,\n",
       " 0.9443511962890625,\n",
       " 0.4588775634765625,\n",
       " 1.3804473876953125,\n",
       " 1.3280487060546875,\n",
       " 0.649932861328125,\n",
       " 1.7127532958984375,\n",
       " 0.9259033203125,\n",
       " 0.5401763916015625,\n",
       " 1.55474853515625,\n",
       " 1.2389678955078125,\n",
       " 1.59991455078125,\n",
       " 1.071685791015625,\n",
       " 0.979644775390625,\n",
       " 0.7802276611328125,\n",
       " 1.164398193359375,\n",
       " 1.0199127197265625,\n",
       " 1.0699920654296875,\n",
       " 0.4385528564453125,\n",
       " 1.17279052734375,\n",
       " 1.0546875,\n",
       " 0.305206298828125,\n",
       " 0.895294189453125,\n",
       " 1.0752716064453125,\n",
       " 0.5552825927734375,\n",
       " 0.52154541015625,\n",
       " 0.8797607421875,\n",
       " 1.8935394287109375,\n",
       " 0.99163818359375,\n",
       " 0.3911285400390625,\n",
       " 0.531890869140625,\n",
       " 1.107696533203125,\n",
       " 1.5594482421875,\n",
       " 1.4376983642578125,\n",
       " 1.10333251953125,\n",
       " 1.342041015625,\n",
       " 0.6923980712890625,\n",
       " 1.0254058837890625,\n",
       " 0.963470458984375,\n",
       " 0.5203399658203125,\n",
       " 1.67816162109375,\n",
       " 1.3089599609375,\n",
       " 0.501251220703125,\n",
       " 1.33892822265625,\n",
       " 1.1759033203125,\n",
       " 1.331573486328125,\n",
       " 1.3682861328125,\n",
       " 1.21759033203125,\n",
       " 0.4677886962890625,\n",
       " 0.7483978271484375,\n",
       " 0.935028076171875,\n",
       " 1.007476806640625,\n",
       " 0.1840057373046875,\n",
       " 0.585540771484375,\n",
       " 0.653076171875,\n",
       " 1.5908966064453125,\n",
       " 0.803802490234375,\n",
       " 0.8560028076171875,\n",
       " 0.9475250244140625,\n",
       " 0.7021484375,\n",
       " 1.2045440673828125,\n",
       " 1.3924102783203125,\n",
       " 1.0171661376953125,\n",
       " 0.485260009765625,\n",
       " 0.640777587890625,\n",
       " 1.0481414794921875,\n",
       " 1.0936431884765625,\n",
       " 0.891448974609375,\n",
       " 1.90386962890625,\n",
       " 0.1920928955078125,\n",
       " 1.0009613037109375,\n",
       " 0.676544189453125,\n",
       " 1.14862060546875,\n",
       " 0.6537628173828125,\n",
       " 1.0710601806640625,\n",
       " 0.876068115234375,\n",
       " 1.0241241455078125,\n",
       " 1.0220184326171875,\n",
       " 1.005859375,\n",
       " 0.815399169921875,\n",
       " 1.6450042724609375,\n",
       " 1.43450927734375,\n",
       " 1.2453765869140625,\n",
       " 1.2417449951171875,\n",
       " 1.1808013916015625,\n",
       " 1.0214080810546875,\n",
       " 0.7310943603515625,\n",
       " 0.959930419921875,\n",
       " 1.340087890625,\n",
       " 1.19024658203125,\n",
       " 1.4824676513671875,\n",
       " 0.609619140625,\n",
       " 0.950103759765625,\n",
       " 1.400634765625,\n",
       " 0.9047088623046875,\n",
       " 0.535797119140625,\n",
       " 1.4749755859375,\n",
       " 0.9380340576171875,\n",
       " 1.469940185546875,\n",
       " 0.4800262451171875,\n",
       " 0.6497039794921875,\n",
       " 0.6294403076171875,\n",
       " 1.17919921875,\n",
       " 0.7433319091796875,\n",
       " 0.9728240966796875,\n",
       " 0.9702301025390625,\n",
       " 0.2138824462890625,\n",
       " 1.0553741455078125,\n",
       " 1.0658416748046875,\n",
       " 1.8353424072265625,\n",
       " 0.9445343017578125,\n",
       " 0.751739501953125,\n",
       " 1.5484466552734375,\n",
       " 0.86822509765625,\n",
       " 1.052276611328125,\n",
       " 1.006927490234375,\n",
       " 1.2612762451171875,\n",
       " 0.8488922119140625,\n",
       " 0.3310699462890625,\n",
       " 1.7741241455078125,\n",
       " 0.8766021728515625,\n",
       " 0.1895294189453125]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique samples must match number of calls to propagate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n",
      "[100]\n",
      "[101]\n",
      "[102]\n",
      "[103]\n",
      "[104]\n",
      "[105]\n",
      "[106]\n",
      "[107]\n",
      "[108]\n",
      "[109]\n",
      "[110]\n",
      "[111]\n",
      "[112]\n",
      "[113]\n",
      "[114]\n",
      "[115]\n",
      "[116]\n",
      "[117]\n",
      "[118]\n",
      "[119]\n",
      "[120]\n",
      "[121]\n",
      "[122]\n",
      "[123]\n",
      "[124]\n",
      "[125]\n",
      "[126]\n",
      "[127]\n",
      "[128]\n",
      "[129]\n",
      "[130]\n",
      "[131]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[135]\n",
      "[136]\n",
      "[137]\n",
      "[138]\n",
      "[139]\n",
      "[140]\n",
      "[141]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[146]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[150]\n",
      "[151]\n",
      "[152]\n",
      "[153]\n",
      "[154]\n",
      "[155]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[162]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[168]\n",
      "[169]\n",
      "[170]\n",
      "[171]\n",
      "[172]\n",
      "[173]\n",
      "[174]\n",
      "[175]\n",
      "[176]\n",
      "[177]\n",
      "[178]\n",
      "[179]\n",
      "[180]\n",
      "[181]\n",
      "[182]\n",
      "[183]\n",
      "[184]\n",
      "[185]\n",
      "[186]\n",
      "[187]\n",
      "[188]\n",
      "[189]\n",
      "[190]\n",
      "[191]\n",
      "[192]\n",
      "[193]\n",
      "[194]\n",
      "[195]\n",
      "[196]\n",
      "[197]\n",
      "[198]\n",
      "[199]\n"
     ]
    }
   ],
   "source": [
    "for ref in refs:\n",
    "    print([i for i, x in enumerate(refs) if x == ref])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
