{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num timesteps per sample: 48\n",
      "{'num_input': 25, 'num_training_samples': 8000, 'num_output': 20, 'time_ms': 1000.0, 'dataset_name': 'letters'}\n"
     ]
    }
   ],
   "source": [
    "from snn_delays.datasets.custom_datasets import CustomDataset\n",
    "from snn_delays.config import DATASET_PATH\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "from tonic import MemoryCachedDataset\n",
    "import numpy as np\n",
    "\n",
    "#data = np.load(os.path.join(DATASET_PATH, 'raw_datasets', 'Letters', 'letter_classification_dataset.npz'))\n",
    "data = np.load(os.path.join(DATASET_PATH, 'raw_datasets', 'Letters', 'three_letter_classification_dataset.npz'))\n",
    "\n",
    "train_data = data['train_data']\n",
    "test_data = data['test_data']\n",
    "train_labels= data['train_labels']\n",
    "test_labels = data['test_labels']\n",
    "\n",
    "num_samples = len(train_labels)\n",
    "\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "test_dataset = CustomDataset(test_data, test_labels)\n",
    "\n",
    "dataset_dict = train_dataset.get_train_attributes()\n",
    "\n",
    "cached_train_dataset = MemoryCachedDataset(train_dataset)\n",
    "cached_test_dataset = MemoryCachedDataset(test_dataset)\n",
    "\n",
    "total_time = train_data.shape[1]\n",
    "print(f'num timesteps per sample: {total_time}')\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(cached_train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            drop_last=False,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=0)\n",
    "\n",
    "test_loader = DataLoader(cached_test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            drop_last=False,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=0)\n",
    "\n",
    "dataset_dict[\"time_ms\"] = 1e3\n",
    "dataset_dict[\"dataset_name\"] = \"letters\"\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 48, 1, 25])\n"
     ]
    }
   ],
   "source": [
    "for i, l in test_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delays(all):50\\\n",
    "Delays(3coarse):47%\\\n",
    "Delays(3fine): 47%\n",
    "Recurrent: 43, 47% after 26 epochs\\\n",
    "Feedforward:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n",
      "\n",
      "[INFO] Delays: tensor([0])\n",
      "\n",
      "[INFO] Delays i: tensor([0])\n",
      "\n",
      "[INFO] Delays h: tensor([0])\n",
      "\n",
      "[INFO] Delays o: tensor([0])\n",
      "1000.0\n",
      "Delta t: 20.833333333333332 ms\n",
      "mean of normal: -0.6064646446020864\n",
      "training lettersNoneletters48_l2_1d1.t7 for 100 epochs...\n",
      "Epoch [1/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 2.95485\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 2.96107\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 2.86583\n",
      "l1_score: 0\n",
      "Time elasped: 19.287636756896973\n",
      "Test Loss: 3.0935159623622894\n",
      "Avg spk_count per neuron for all 48 time-steps 3.086205244064331\n",
      "Avg spk per neuron per layer [6.0916328125, 6.2531875]\n",
      "Test Accuracy of the model on the test samples: 8.100\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 8.1\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [2/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 2.79531\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 2.63588\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 2.47370\n",
      "l1_score: 0\n",
      "Time elasped: 21.920053243637085\n",
      "Test Loss: 3.2259145379066467\n",
      "Avg spk_count per neuron for all 48 time-steps 3.428582191467285\n",
      "Avg spk per neuron per layer [6.2696953125, 7.4446328125]\n",
      "Test Accuracy of the model on the test samples: 11.450\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 11.45\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [3/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 2.34228\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 2.23978\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 2.12354\n",
      "l1_score: 0\n",
      "Time elasped: 21.510473012924194\n",
      "Test Loss: 3.5300942063331604\n",
      "Avg spk_count per neuron for all 48 time-steps 3.1861817836761475\n",
      "Avg spk per neuron per layer [5.8746796875, 6.870046875]\n",
      "Test Accuracy of the model on the test samples: 15.400\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 15.4\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [4/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 2.27238\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 2.04038\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 1.88909\n",
      "l1_score: 0\n",
      "Time elasped: 21.702472686767578\n",
      "Test Loss: 4.049001529812813\n",
      "Avg spk_count per neuron for all 48 time-steps 3.13261342048645\n",
      "Avg spk per neuron per layer [6.077984375, 6.45246875]\n",
      "Test Accuracy of the model on the test samples: 19.800\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 19.8\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [5/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 1.79719\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 1.83206\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 1.77165\n",
      "l1_score: 0\n",
      "Time elasped: 21.490784883499146\n",
      "Test Loss: 4.27845473587513\n",
      "Avg spk_count per neuron for all 48 time-steps 3.2855119705200195\n",
      "Avg spk per neuron per layer [6.401375, 6.740671875]\n",
      "Test Accuracy of the model on the test samples: 23.200\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 23.2\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [6/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 1.45414\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 1.51124\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 1.34568\n",
      "l1_score: 0\n",
      "Time elasped: 25.209328174591064\n",
      "Test Loss: 4.785350516438484\n",
      "Avg spk_count per neuron for all 48 time-steps 3.293445587158203\n",
      "Avg spk per neuron per layer [6.321171875, 6.852609375]\n",
      "Test Accuracy of the model on the test samples: 25.400\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 25.4\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [7/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 1.38650\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 1.30108\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 1.48776\n",
      "l1_score: 0\n",
      "Time elasped: 21.773993730545044\n",
      "Test Loss: 4.310869589447975\n",
      "Avg spk_count per neuron for all 48 time-steps 3.505021572113037\n",
      "Avg spk per neuron per layer [6.762734375, 7.2573515625]\n",
      "Test Accuracy of the model on the test samples: 28.600\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 28.6\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [8/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 1.39254\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 1.03302\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 1.40047\n",
      "l1_score: 0\n",
      "Time elasped: 21.709770679473877\n",
      "Test Loss: 4.2889136373996735\n",
      "Avg spk_count per neuron for all 48 time-steps 3.4896798133850098\n",
      "Avg spk per neuron per layer [6.9190625, 7.03965625]\n",
      "Test Accuracy of the model on the test samples: 30.700\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 30.7\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [9/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 1.07011\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.94426\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.96525\n",
      "l1_score: 0\n",
      "Time elasped: 21.582334280014038\n",
      "Test Loss: 3.9970558434724808\n",
      "Avg spk_count per neuron for all 48 time-steps 3.4466564655303955\n",
      "Avg spk per neuron per layer [6.79896875, 6.98765625]\n",
      "Test Accuracy of the model on the test samples: 30.350\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [10/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.90401\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.98229\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.89044\n",
      "l1_score: 0\n",
      "Time elasped: 21.63046669960022\n",
      "Test Loss: 4.101054817438126\n",
      "Avg spk_count per neuron for all 48 time-steps 3.5091681480407715\n",
      "Avg spk per neuron per layer [7.026921875, 7.00975]\n",
      "Test Accuracy of the model on the test samples: 35.650\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 35.65\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [11/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.96553\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.85894\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.77895\n",
      "l1_score: 0\n",
      "Time elasped: 21.843234539031982\n",
      "Test Loss: 4.867082953453064\n",
      "Avg spk_count per neuron for all 48 time-steps 3.6722521781921387\n",
      "Avg spk per neuron per layer [7.2434609375, 7.445546875]\n",
      "Test Accuracy of the model on the test samples: 34.800\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [12/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.83744\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.90254\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.88285\n",
      "l1_score: 0\n",
      "Time elasped: 21.972071409225464\n",
      "Test Loss: 4.010448008775711\n",
      "Avg spk_count per neuron for all 48 time-steps 3.644587993621826\n",
      "Avg spk per neuron per layer [7.1382890625, 7.4400625]\n",
      "Test Accuracy of the model on the test samples: 35.800\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 35.8\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [13/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.66357\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 1.18554\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.60313\n",
      "l1_score: 0\n",
      "Time elasped: 21.632210731506348\n",
      "Test Loss: 4.2072057873010635\n",
      "Avg spk_count per neuron for all 48 time-steps 3.6898694038391113\n",
      "Avg spk per neuron per layer [7.24371875, 7.5157578125]\n",
      "Test Accuracy of the model on the test samples: 37.900\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 37.9\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [14/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.96031\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.60944\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.81018\n",
      "l1_score: 0\n",
      "Time elasped: 21.814532041549683\n",
      "Test Loss: 4.448437511920929\n",
      "Avg spk_count per neuron for all 48 time-steps 3.6437442302703857\n",
      "Avg spk per neuron per layer [7.0368671875, 7.538109375]\n",
      "Test Accuracy of the model on the test samples: 38.350\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 38.35\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [15/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.71368\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.73353\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.68999\n",
      "l1_score: 0\n",
      "Time elasped: 21.460992097854614\n",
      "Test Loss: 4.617271289229393\n",
      "Avg spk_count per neuron for all 48 time-steps 3.723551034927368\n",
      "Avg spk per neuron per layer [7.2292890625, 7.6649140625]\n",
      "Test Accuracy of the model on the test samples: 39.400\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 39.4\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [16/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.65710\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.50961\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.74777\n",
      "l1_score: 0\n",
      "Time elasped: 21.71922516822815\n",
      "Test Loss: 4.341803282499313\n",
      "Avg spk_count per neuron for all 48 time-steps 3.7977190017700195\n",
      "Avg spk per neuron per layer [7.5828125, 7.6080625]\n",
      "Test Accuracy of the model on the test samples: 40.650\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 40.65\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [17/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.43045\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.53033\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.61902\n",
      "l1_score: 0\n",
      "Time elasped: 21.744234085083008\n",
      "Test Loss: 3.662678971886635\n",
      "Avg spk_count per neuron for all 48 time-steps 3.768693447113037\n",
      "Avg spk per neuron per layer [7.4473984375, 7.627375]\n",
      "Test Accuracy of the model on the test samples: 43.450\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 43.45\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [18/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.46524\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.67674\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.72974\n",
      "l1_score: 0\n",
      "Time elasped: 21.529612064361572\n",
      "Test Loss: 4.158970668911934\n",
      "Avg spk_count per neuron for all 48 time-steps 3.734642744064331\n",
      "Avg spk per neuron per layer [7.1606015625, 7.77796875]\n",
      "Test Accuracy of the model on the test samples: 39.300\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [19/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.45650\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.34923\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.47530\n",
      "l1_score: 0\n",
      "Time elasped: 21.796267986297607\n",
      "Test Loss: 4.194621279835701\n",
      "Avg spk_count per neuron for all 48 time-steps 3.8873302936553955\n",
      "Avg spk per neuron per layer [7.6666015625, 7.88271875]\n",
      "Test Accuracy of the model on the test samples: 41.200\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [20/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.49158\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.36472\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.79327\n",
      "l1_score: 0\n",
      "Time elasped: 21.800021409988403\n",
      "Test Loss: 3.694410800933838\n",
      "Avg spk_count per neuron for all 48 time-steps 3.864222764968872\n",
      "Avg spk per neuron per layer [7.528484375, 7.92840625]\n",
      "Test Accuracy of the model on the test samples: 42.700\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [21/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.49684\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.39495\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.35605\n",
      "l1_score: 0\n",
      "Time elasped: 21.84677529335022\n",
      "Test Loss: 3.4116118252277374\n",
      "Avg spk_count per neuron for all 48 time-steps 3.826709270477295\n",
      "Avg spk per neuron per layer [7.616953125, 7.6898828125]\n",
      "Test Accuracy of the model on the test samples: 45.750\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 45.75\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [22/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.49795\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.51330\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.49294\n",
      "l1_score: 0\n",
      "Time elasped: 21.709447860717773\n",
      "Test Loss: 4.383152142167091\n",
      "Avg spk_count per neuron for all 48 time-steps 3.7515430450439453\n",
      "Avg spk per neuron per layer [7.256203125, 7.74996875]\n",
      "Test Accuracy of the model on the test samples: 42.250\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [23/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.38396\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.48558\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.59596\n",
      "l1_score: 0\n",
      "Time elasped: 21.633890628814697\n",
      "Test Loss: 4.988205000758171\n",
      "Avg spk_count per neuron for all 48 time-steps 3.7813282012939453\n",
      "Avg spk per neuron per layer [7.33065625, 7.79465625]\n",
      "Test Accuracy of the model on the test samples: 44.050\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [24/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.39306\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.36541\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.42386\n",
      "l1_score: 0\n",
      "Time elasped: 21.76962637901306\n",
      "Test Loss: 3.5056212693452835\n",
      "Avg spk_count per neuron for all 48 time-steps 3.923191547393799\n",
      "Avg spk per neuron per layer [7.7001015625, 7.9926640625]\n",
      "Test Accuracy of the model on the test samples: 46.350\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 46.35\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [25/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.30286\n",
      "l1_score: 0\n",
      "Step [40/62], Loss: 0.18587\n",
      "l1_score: 0\n",
      "Step [60/62], Loss: 0.19421\n",
      "l1_score: 0\n",
      "Time elasped: 22.516576766967773\n",
      "Test Loss: 4.150279372930527\n",
      "Avg spk_count per neuron for all 48 time-steps 3.8006486892700195\n",
      "Avg spk per neuron per layer [7.4394453125, 7.7631484375]\n",
      "Test Accuracy of the model on the test samples: 47.700\n",
      "\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "saving max acc: 47.7\n",
      "Model saved in  C:\\Users\\Alberto\\OneDrive - UNIVERSIDAD DE SEVILLA\\PythonData\\Checkpoints\\exp23_letters_memory\n",
      "Epoch [26/100], learning_rates 0.001000, 0.100000\n",
      "Step [20/62], Loss: 0.22039\n",
      "l1_score: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m snn\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletters\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m snn\u001b[38;5;241m.\u001b[39mmodel_name\n\u001b[0;32m     30\u001b[0m snn\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m train(snn, train_loader, test_loader, lr, num_epochs, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \n\u001b[0;32m     32\u001b[0m     test_behavior\u001b[38;5;241m=\u001b[39mtb_save_max_last_acc, ckpt_dir\u001b[38;5;241m=\u001b[39mckpt_dir, scheduler\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m0.95\u001b[39m), test_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[INFO] TIEMPO: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtaimu1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\utils\\train_utils.py:83\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(snn, train_loader, test_loader, learning_rate, num_epochs, spk_reg, l1_reg, dropout, lr_tau, scheduler, ckpt_dir, test_behavior, test_every, delay_pruning, weight_pruning, lsm, random_delay_pruning, weight_quantization, k, depth, freeze_taus, verbose)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m], learning_rates \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_epochs,\n\u001b[0;32m     79\u001b[0m                                                  current_lr, current_lr_tau), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     snn\u001b[38;5;241m.\u001b[39mtrain_step(train_loader,\n\u001b[0;32m     84\u001b[0m                 optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     85\u001b[0m                 scheduler \u001b[38;5;241m=\u001b[39m scheduler,\n\u001b[0;32m     86\u001b[0m                 spk_reg\u001b[38;5;241m=\u001b[39mspk_reg,\n\u001b[0;32m     87\u001b[0m                 l1_reg\u001b[38;5;241m=\u001b[39ml1_reg,\n\u001b[0;32m     88\u001b[0m                 dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m     89\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39mverbose)        \n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     snn\u001b[38;5;241m.\u001b[39mtrain_step_tr(train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     92\u001b[0m                     criterion\u001b[38;5;241m=\u001b[39msnn\u001b[38;5;241m.\u001b[39mcriterion, spk_reg\u001b[38;5;241m=\u001b[39mspk_reg,\n\u001b[0;32m     93\u001b[0m                     depth\u001b[38;5;241m=\u001b[39mdepth, k\u001b[38;5;241m=\u001b[39mk, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\snn.py:187\u001b[0m, in \u001b[0;36mTraining.train_step\u001b[1;34m(self, train_loader, optimizer, scheduler, spk_reg, l1_reg, dropout, verbose)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# # Dropout [REVIEW]\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# images = self.dropout(images.float())\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_amp):\n\u001b[0;32m    185\u001b[0m \n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# Propagate data\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     outputs, reference \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(images, labels)\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m#total spike count (for spike regularization)\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     spk_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_sum_spike \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_neurons_list) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwin)\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\snn.py:123\u001b[0m, in \u001b[0;36mTraining.propagate\u001b[1;34m(self, images, labels)\u001b[0m\n\u001b[0;32m    119\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    121\u001b[0m l_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn\n\u001b[1;32m--> 123\u001b[0m all_o_mems, all_o_spikes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(images)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l_f \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmem_last\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    126\u001b[0m     _, labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(labels\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\snn.py:1000\u001b[0m, in \u001b[0;36mSNN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     r_ext_spk \u001b[38;5;241m=\u001b[39m spikes[layer]\n\u001b[1;32m-> 1000\u001b[0m mems[layer], spikes[layer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_mem_fn(\n\u001b[0;32m   1001\u001b[0m     prev_spikes\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), spikes[layer], mems[layer], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresh, r_ext_spk)\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay_type:\n\u001b[0;32m   1004\u001b[0m     extended_spikes[layer][:, step\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_d,\n\u001b[0;32m   1005\u001b[0m                         :] \u001b[38;5;241m=\u001b[39m spikes[layer]\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# possibly detach()\u001b[39;00m\n",
      "File \u001b[1;32m~\\Python\\SNNdelays\\snn_delays\\snn.py:929\u001b[0m, in \u001b[0;36mSNN.update_mem_rnn\u001b[1;34m(self, i_spike, o_spike, mem, thresh, extended_o_spikes)\u001b[0m\n\u001b[0;32m    923\u001b[0m mem \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m b \u001b[38;5;241m+\u001b[39m c\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# o_spike = self.act_fun(mem-thresh)\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# mem = mem*(mem < self.th_reset)   \u001b[39;00m\n\u001b[0;32m    927\u001b[0m \n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# Update attributes\u001b[39;00m\n\u001b[1;32m--> 929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(mem, thresh)\n",
      "File \u001b[1;32mc:\\Users\\Alberto\\anaconda3\\envs\\deepsnn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1690\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1687\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_from\u001b[39m(\u001b[38;5;241m*\u001b[39mdicts_or_sets):\n\u001b[0;32m   1692\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dicts_or_sets:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from snn_delays.snn import SNN\n",
    "from snn_delays.utils.train_utils import train, get_device\n",
    "from snn_delays.utils.test_behavior import tb_save_max_last_acc\n",
    "import time\n",
    "\n",
    "device = get_device()\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "taimu1 = time.time()\n",
    "\n",
    "tau_m = 'normal'\n",
    "#delay = (48*2,16*2)\n",
    "#delay = (96,32)\n",
    "delay = None\n",
    "#delay = (total_time, 1)\n",
    "#delay = (total_time, total_time//3)\n",
    "#delay = (3, 1)\n",
    "ckpt_dir = 'exp23_letters_memory' \n",
    "\n",
    "snn = SNN(dataset_dict=dataset_dict, structure=(64, 2), connection_type='r',\n",
    "    delay=delay, delay_type='h', tau_m = tau_m,\n",
    "    win=total_time, loss_fn='mem_sum', batch_size=batch_size, device=device,\n",
    "    debug=True)\n",
    "\n",
    "snn.set_network()\n",
    "\n",
    "snn.model_name = f'letters{delay}'+ snn.model_name\n",
    "\n",
    "snn.to(device)\n",
    "train(snn, train_loader, test_loader, lr, num_epochs, dropout=0.0, \n",
    "    test_behavior=tb_save_max_last_acc, ckpt_dir=ckpt_dir, scheduler=(100, 0.95), test_every=1)\n",
    "\n",
    "print(f'[INFO] TIEMPO: {time.time() - taimu1}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.307549476623535\n",
      "Avg spk_count per neuron for all 16 time-steps 0.819427490234375\n",
      "Avg spk per neuron per layer [2.0072021484375, 1.2705078125]\n",
      "Test Accuracy of the model on the test samples: 7.812\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Time (ms)', ylabel='Neuron'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from snn_delays.utils.visualization_utils import plot_raster\n",
    "snn.test(test_loader, only_one_batch=True)\n",
    "plot_raster(snn, 'input', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
